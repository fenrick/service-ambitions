# Pydantic Logfire Documentation

> Pydantic Logfire Documentation

From the team behind Pydantic, Logfire is an observability platform built on the same belief as our
open source library — that the most powerful tools can be easy to use.

# General

# Getting Started

## About Pydantic Logfire

From the team behind **Pydantic Validation**, **Pydantic Logfire** is a new type of observability platform built on the same belief as our open source library — that the most powerful tools can be easy to use.

**Logfire** is built on OpenTelemetry, and supports monitoring your application from **any language**, with particularly great support for Python! [Read more](why/).

## Overview

This page is a quick walk-through for setting up a Python app:

1. [Set up Logfire](#logfire)
1. [Install the SDK](#sdk)
1. [Instrument your project](#instrument)

## Set up Logfire

1. [Log into Logfire](https://logfire.pydantic.dev/login)

1. Follow the prompts to create your account

1. Once logged in, you'll see the **Welcome to Logfire** prompt. Click **Let's go!** to go to the **starter-project** Setup page.

1. You will find how to send data to your **starter-project** there. Also, there are some code snippets to help you get started.

A **Logfire** project is a namespace for organizing your data. All data sent to **Logfire** must be associated with a project.

Ready to create your own projects in UI or CLI?

- In the UI, create projects by navigating to the Organization > Projects page, and click **New project**.
- For CLI check the [SDK CLI documentation](reference/cli/#create-projects-new).

## Install the SDK

1. In the terminal, install the **Logfire** SDK (Software Developer Kit):

```bash
pip install logfire
```

```bash
uv add logfire
```

```bash
poetry add logfire
```

```bash
conda install -c conda-forge logfire
```

2. Once installed, try it out!

```bash
logfire -h
```

3. Next, authenticate your local environment:

```bash
logfire auth
```

Upon successful authentication, credentials are stored in `~/.logfire/default.toml`.

## Instrument your project

Development setup

During development, we recommend using the CLI to configure Logfire. You can also use a [write token](how-to-guides/create-write-tokens/).

1. Set your project

in the terminal:

```bash
logfire projects use <first-project>
```

Run this command from the root directory of your app, e.g. `~/projects/first-project`

2. Write some basic logs in your Python app

hello_world.py

```py
import logfire

logfire.configure()  # (1)!
logfire.info('Hello, {name}!', name='world')  # (2)!
```

3. The `configure()` method should be called once before logging to initialize **Logfire**.
1. This will log `Hello world!` with `info` level.

Other log levels are also available to use, including `trace`, `debug`, `notice`, `warn`, `error`, and `fatal`.

3. See your logs in the **Live** view

Production setup

In production, we recommend you provide your write token to the Logfire SDK via environment variables.

1. Generate a new write token in the **Logfire** platform

   - Go to Project Settings Write Tokens
   - Follow the prompts to create a new token

1. Configure your **Logfire** environment

In the terminal:

```bash
export LOGFIRE_TOKEN=<your-write-token>
```

Running this command stores a Write Token used by the SDK to send data to a file in the current directory, at `.logfire/logfire_credentials.json`

3. Write some basic logs in your Python app

hello_world.py

```py
import logfire

logfire.configure()  # (1)!
logfire.info('Hello, {name}!', name='world')  # (2)!
```

1. The `configure()` method should be called once before logging to initialize **Logfire**.
1. This will log `Hello world!` with `info` level.

Other log levels are also available to use, including `trace`, `debug`, `notice`, `warn`, `error`, and `fatal`.

4. See your logs in the **Live** view

______________________________________________________________________

## Next steps

Ready to keep going?

- Read about [Concepts](concepts/)
- Complete the [Onboarding Checklist](guides/onboarding-checklist/)

More topics to explore...

- Logfire's real power comes from [integrations with many popular libraries](integrations/)
- As well as spans, you can [use Logfire to record metrics](guides/onboarding-checklist/add-metrics/)
- Logfire doesn't just work with Python, [read more about Language support](https://opentelemetry.io/docs/languages/)
- Compliance requirements (e.g. SOC2)? [See Logfire's certifications](compliance/)

# Introducing Pydantic Logfire

From the team behind Pydantic Validation, **Pydantic Logfire** is an observability platform built on the same belief as our open source library — that the most powerful tools can be easy to use.

## What sets Logfire apart

- **Simplicity and Power**

______________________________________________________________________

Logfire's dashboard is simple relative to the power it provides, ensuring your entire engineering team will actually use it. Time-to-first-log should be less than 5 minutes.

[Read more](#simplicity-and-power)

- **Python-centric Insights**

______________________________________________________________________

From rich display of **Python objects**, to **event-loop telemetry**, to **profiling Python code & database queries**, Logfire gives you unparalleled visibility into your Python application's behavior.

[Read more](#python-centric-insights)

- **Pydantic Integration**

______________________________________________________________________

Understand the data flowing through your Pydantic Validation models and get built-in analytics on validations.

Pydantic Logfire helps you instrument your applications with less code, less time, and better understanding.

[Read more](#pydantic-integration)

- **OpenTelemetry**

______________________________________________________________________

Logfire is an opinionated wrapper around OpenTelemetry, allowing you to leverage existing tooling, infrastructure, and instrumentation for many common Python packages, and enabling support for virtually any language.

[Read more](#opentelemetry-under-the-hood)

- **Structured Data**

______________________________________________________________________

Include your Python objects in Logfire calls (lists, dict, dataclasses, Pydantic models, DataFrames, and more), and it'll end up as structured data in our platform ready to be queried.

[Read more](#sql)

- **SQL**

______________________________________________________________________

Query your data using standard SQL — all the control and (for many) nothing new to learn. Using SQL also means you can query your data with existing BI tools and database querying libraries.

[Read more](#sql)

## Find the needle in a *stack trace*

We understand Python and its peculiarities. Pydantic Logfire was crafted by Python developers, for Python developers, addressing the unique challenges and opportunities of the Python environment. It's not just about having data; it's about having the *right* data, presented in ways that make sense for Python applications.

## Simplicity and Power

Emulating the Pydantic library's philosophy, Pydantic Logfire offers an intuitive start for beginners while providing the depth experts desire. It's the same balance of ease, sophistication, and productivity, reimagined for observability.

Within a few minutes you'll have your first logs:

This might look similar to simple logging, but it's much more powerful — you get:

- **Structured data** from your logs
- **Nested logs & traces** to contextualize what you're viewing
- **Custom-built platform** to view your data, with no configuration required
- **Pretty display** of Python objects

Ready to try Logfire? [Get Started](../)! 🚀

## Python-centric insights

**Pydantic Logfire** automatically instruments your code for minimal manual effort, provides exceptional insights into async code, offers detailed performance analytics, and displays Python objects the same as the interpreter. **Pydantic Logfire** gives you a clearer view into how your Python is running than any other observability tool.

### Rich display of Python objects

In this example, you can see the parameters passed to a FastAPI endpoint formatted as a Python object.

### Profiling Python code

In this simple app example, you can see every interaction the user makes with the web app automatically traced to the Live view using the [Auto-tracing method](../guides/onboarding-checklist/add-auto-tracing/).

## Pydantic integration

**Logfire** has an out-of-the-box **Pydantic** integration that lets you understand the data passing through your Pydantic models and get analytics on validations. For existing Pydantic users, it delivers unparalleled insights into your usage of Pydantic models.

We can record Pydantic models directly:

```py
from datetime import date

import logfire
from pydantic import BaseModel

logfire.configure()

class User(BaseModel):
    name: str
    country_code: str
    dob: date

user = User(name='Anne', country_code='USA', dob='2000-01-01')
logfire.info('user processed: {user!r}', user=user)  # (1)!
```

1. This will show `user processed: User(name='Anne', country_code='US', dob=datetime.date(2000, 1, 1))`, but also allow you to see a "pretty" view of the model within the Logfire Platform.

Or we can record information about validations automatically:

```py
from datetime import date

import logfire
from pydantic import BaseModel

logfire.configure()
logfire.instrument_pydantic()  # (1)!

class User(BaseModel):
    name: str
    country_code: str
    dob: date

User(name='Anne', country_code='USA', dob='2000-01-01')  # (2)!
User(name='Ben', country_code='USA', dob='2000-02-02')
User(name='Charlie', country_code='GBR', dob='1990-03-03')
```

1. This configuration means details about all Pydantic model validations will be recorded. You can also record details about validation failures only, or just metrics; see the [pydantic plugin docs](../integrations/pydantic/).
1. Since we've enabled the Pydantic Plugin, all Pydantic validations will be recorded in Logfire.

Learn more about the [Pydantic Plugin here](../integrations/pydantic/).

## OpenTelemetry under the hood

Because **Pydantic Logfire** is built on [OpenTelemetry](https://opentelemetry.io/), you can use a wealth of existing tooling and infrastructure, including [instrumentation for many common Python packages](https://opentelemetry-python-contrib.readthedocs.io/en/latest/index.html). Logfire also supports cross-language data integration and data export to any OpenTelemetry-compatible backend or proxy.

For example, we can instrument a simple FastAPI app with just 2 lines of code:

main.py

```py
from datetime import date

import logfire
from pydantic import BaseModel
from fastapi import FastAPI

app = FastAPI()

logfire.configure()
logfire.instrument_fastapi(app)  # (1)!
# Here you'd instrument any other library that you use. (2)


class User(BaseModel):
    name: str
    country_code: str
    dob: date


@app.post('/')
async def add_user(user: User):
    # we would store the user here
    return {'message': f'{user.name} added'}
```

1. In addition to [configuring logfire](../reference/configuration/) this line is all you need to instrument a FastAPI app with Logfire. The same applies to most other popular Python web frameworks.
1. The [integrations](../integrations/) page has more information on how to instrument other parts of your app. Run the [inspect](../reference/cli/#inspect-inspect) command for package suggestions.

We'll need the [FastAPI contrib package](../integrations/web-frameworks/fastapi/), FastAPI itself and uvicorn installed to run this:

```bash
pip install 'logfire[fastapi]' fastapi uvicorn  # (1)!
uvicorn main:app # (2)!
```

1. Install the `logfire` package with the `fastapi` extra, FastAPI, and uvicorn.
1. Run the FastAPI app with uvicorn.

This will give you information on the HTTP request and details of results from successful input validations:

And, importantly, details of failed input validations:

In the example above, we can see the FastAPI arguments failing (`user` is null when it should always be populated). This demonstrates type-checking from Pydantic used out-of-the-box in FastAPI.

## Structured Data and SQL

Query your data with pure, canonical PostgreSQL — all the control and (for many) nothing new to learn. We even provide direct access to the underlying Postgres database, which means that you can query Logfire using any Postgres-compatible tools you like.

This includes BI tools and dashboard-building platforms like

- Superset
- Grafana
- Google Looker Studio

As well as data science tools like

- Pandas
- SQLAlchemy
- `psql`

Using vanilla PostgreSQL as the querying language throughout the platform ensures a consistent, powerful, and flexible querying experience.

Another big advantage of using the most widely used SQL databases is that generative AI tools like ChatGPT are excellent at writing SQL for you.

Just include your Python objects in **Logfire** calls (lists, dict, dataclasses, Pydantic models, DataFrames, and more), and it'll end up as structured data in our platform ready to be queried.

For example, using data from a `User` model, we could list users from the USA:

```sql
SELECT attributes->'result'->>'name' as name, extract(year from (attributes->'result'->>'dob')::date) as "birth year"
FROM records
WHERE attributes->'result'->>'country_code' = 'USA';
```

You can also filter to show only traces related to users in the USA in the live view with

```sql
attributes->'result'->>'name' = 'Ben'
```

Structured Data and Direct SQL Access means you can use familiar tools like Pandas, SQLAlchemy, or `psql` for querying, can integrate seamlessly with BI tools, and can even leverage AI for SQL generation, ensuring your Python objects and structured data are query-ready.

## Overview

Understanding the core building blocks of observability helps you monitor, debug, and optimize your applications. The four key concepts work together to give you complete insight into your application's behavior: spans and traces show you what's happening and how long it takes, metrics reveal trends and performance over time, and logs capture specific events and details.

New to observability? Another great resource is the [OpenTelemetry primer](https://opentelemetry.io/docs/concepts/observability-primer/).

## Concepts

| Concept | Description                                                     |
| ------- | --------------------------------------------------------------- |
| Span    | Atomic unit of telemetry data                                   |
| Trace   | Contains spans, tree structure shows parent/child relationships |
| Metric  | Values calculated using telemetry data                          |
| Log     | No duration, timestamped, emitted by services/loggers           |

## What is a Span?

A **span** is the building block of a trace. You might also think of spans as logs with extra functionality — a single row in our live view.

Info

Spans let you **add context** to your logs and **measure code execution time**. Multiple spans combine to form a trace, providing a complete picture of an operation's journey through your system.

## What is a Trace?

A trace is a tree structure of spans which shows the path of any client request, LLM run, API call through your application.

Spans are ordered and nested, meaning you can think of this like a stack trace - it shows you the whole history of all services touched and all responses returned.

Info

Traces are not limited to a single service — they can be propagated to completely different and isolated services. For example, a single trace could contain http requests to both a Python API and a SQL database.

### Example - File size counter

In this example:

1. The outer span measures the time to count the total size of files in the current directory (`cwd`).
1. Inner spans measure the time to read each individual file.
1. Finally, the total size is logged.

```py
from pathlib import Path
import logfire

logfire.configure()

cwd = Path.cwd()
total_size = 0

with logfire.span('counting size of {cwd=}', cwd=cwd):
    for path in cwd.iterdir():
        if path.is_file():
            with logfire.span('reading {path}', path=path.relative_to(cwd)):
                total_size += len(path.read_bytes())

    logfire.info('total size of {cwd} is {size} bytes', cwd=cwd, size=total_size)
```

______________________________________________________________________

### Example - Happy Birthday

In this example:

1. The outer span sets the topic — the user's birthday
1. The user input is captured in the terminal
1. `dob` (date of birth) is displayed in the span. Logfire calculates the age from the `dob` and displays age in the debug message

```py
from datetime import date

import logfire

logfire.configure()

with logfire.span('Asking the user for their {question}', question='birthday'):  # (1)!
    user_input = input('When were you born [YYYY-mm-dd]? ')
    dob = date.fromisoformat(user_input)  # (2)!
    logfire.debug('{dob=} {age=!r}', dob=dob, age=date.today() - dob)  # (3)!
```

1. Spans allow you to nest other Logfire calls, and also to measure how long code takes to run. They are the fundamental building block of traces!
1. Attempt to extract a date from the user input. If any exception is raised, the outer span will include the details of the exception.
1. This will log for example `dob=2000-01-01 age=datetime.timedelta(days=8838)` with `debug` level.

______________________________________________________________________

By instrumenting your code with traces and spans, you can see how long operations take, identify bottlenecks, and get a high-level view of request flows in your system — all invaluable for maintaining the performance and reliability of your applications.

## What is a Metric?

A metric a calculated value measuring your application through time.

- Metrics are collected at regular intervals—such as request latency, CPU load, or queue length
- Metrics are aggregated over time
- Metrics make it easy to chart long‑term trends, establish Service‑Level Objectives (SLOs), and trigger alerts when your system drifts outside acceptable thresholds

Alongside logs and traces, metrics complete the "three pillars" of observability, giving you a continuous, low‑overhead signal about the overall health and performance of your services.

Info

Good news: Logfire's [integrations](../integrations/) automatically set up many common metrics for you out of the box.

You can also explore our [standard dashboards](../guides/web-ui/dashboards/) which provide pre-built visualizations for **Web Server Metrics**, **Basic System Metrics (Logfire)** and **Basic System Metrics (OpenTelemetry)**.

### Example - Duration of HTTP requests

```python
import time
import logfire

logfire.configure()

# Create the histogram metric once at import time
request_duration = logfire.metric_histogram(
    'request_duration',
    unit='ms',  # milliseconds
    description='Duration of HTTP requests',
)


def handle_request():
    start = time.perf_counter()
    # … handle the request …
    duration_ms = (time.perf_counter() - start) * 1000
    # Record the observed latency
    request_duration.record(duration_ms)
```

Each call to `request_duration.record()` adds a sample to the histogram. On the backend you can visualise p50/p95 latency, set SLOs, and trigger alerts whenever performance degrades. For more metrics examples see the [adding metrics guide](../guides/onboarding-checklist/add-metrics/).

## What is a Log?

Logs record something which happened in your application. Importantly, they do not have a duration, compared to spans and traces.

A log is a timestamped text record, either structured (recommended) or unstructured, with optional metadata. Of all telemetry signals, logs are the best known and have the largest footprint on our collective understanding. Most programming languages have built-in logging capabilities or well-known, widely used logging libraries. Most Python users are familiar with the logging module for example.
# How To - Guides

Welcome to the **Logfire** Onboarding Checklist! Whether you're starting a new project or integrating Logfire with an existing application, this guide is intended to help you quickly instrument your code and start sending as much data to Logfire with as little development effort as possible.

Once you've completed the checklist, you'll be collecting all the data necessary to monitor performance, identify and fix bugs, analyze user behavior, and make data-driven decisions.

Note

If you aren't familiar with traces and spans, start with the [Tracing with Spans](../../concepts/) page.

#### Logfire Onboarding Checklist

- **[Integrate Logfire](integrate/)**: Fully integrate Logfire with your logging system and the packages you are using.
- **[Add Logfire manual tracing](add-manual-tracing/)**: Enhance your tracing data by manually adding custom spans and logs to your code for more targeted data collection.
- **[Add Logfire auto-tracing](add-auto-tracing/)**: Discover how to leverage Logfire's auto-tracing capabilities to automatically instrument your application with minimal code changes.
- **[Add Logfire metrics](add-metrics/)**: Learn how to create and use metrics to track and measure important aspects of your application's performance and behavior.

We'll walk you through the checklist step by step, introducing relevant features and concepts as we go. While the main focus of this guide is on getting data into Logfire so you can leverage it in the future, we'll also provide an introduction to the Logfire Web UI and show you how to interact with the data you're generating.

Note

For a more comprehensive walkthrough of the Logfire Web UI and its features, you may be interested in our [Logfire Web UI Guide](../web-ui/live/).

Let's get started!

# Auto-tracing

The logfire.install_auto_tracing() method will trace all function calls in the specified modules.

This works by changing how those modules are imported, so the function MUST be called before importing the modules you want to trace.

For example, suppose all your code lives in the `app` package, e.g. `app.main`, `app.server`, `app.db`, etc. Instead of starting your application with `python app/main.py`, you could create another file outside of the `app` package, e.g:

main.py

```py
import logfire

logfire.configure()
logfire.install_auto_tracing(modules=['app'], min_duration=0.01)

from app.main import main

main()
```

Note

Generator functions will not be traced for reasons explained [here](../../../reference/advanced/generators/).

## Only tracing functions above a minimum duration

In most situations you don't want to trace every single function call in your application. The most convenient way to exclude functions from tracing is with the min_duration argument. For example, the code snippet above will only trace functions that take longer than 0.01 seconds. This means you automatically get observability for the heavier parts of your application without too much overhead or data. Note that there are some caveats:

- A function will only start being traced after it runs longer than `min_duration` once. This means that:
  - If it runs faster than `min_duration` the first few times, you won't get data about those first calls.
  - The first time that it runs longer than `min_duration`, you also won't get data about that call.
- After a function runs longer than `min_duration` once, it will be traced every time it's called afterwards, regardless of how long it takes.
- Measuring the duration of a function call still adds a small overhead. For tiny functions that are called very frequently, it's best to still use the `@no_auto_trace` decorator to avoid any overhead. Auto-tracing with `min_duration` will still work for other undecorated functions.

If you want to trace all function calls from the beginning, set `min_duration=0`.

## Filtering modules to trace

The `modules` argument can be a list of module names. Any submodule within a given module will also be traced, e.g. `app.main` and `app.server`. Other modules whose names start with the same prefix will not be traced, e.g. `apples`.

If one of the strings in the list isn't a valid module name, it will be treated as a regex, so e.g. `modules=['app.*']` *will* trace `apples` in addition to `app.main` etc.

For even more control, the `modules` argument can be a function which returns `True` for modules that should be traced. This function will be called with an AutoTraceModule object, which has `name` and `filename` attributes. For example, this should trace all modules that aren't part of the standard library or third-party packages in a typical Python installation:

```py
import pathlib

import logfire

PYTHON_LIB_ROOT = str(pathlib.Path(pathlib.__file__).parent)


def should_trace(module: logfire.AutoTraceModule) -> bool:
    return not module.filename.startswith(PYTHON_LIB_ROOT)


logfire.install_auto_tracing(should_trace)
```

## Excluding functions from tracing

Once you've selected which modules to trace, you probably don't want to trace *every* function in those modules. To exclude a function from auto-tracing, add the no_auto_trace decorator to it:

```py
import logfire

@logfire.no_auto_trace
def my_function():
    # Nested functions will also be excluded
    def inner_function():
        ...

    return other_function()


# This function is *not* excluded from auto-tracing.
# It will still be traced even when called from the excluded `my_function` above.
def other_function():
    ...


# All methods of a decorated class will also be excluded
@no_auto_trace
class MyClass:
    def my_method(self):
        ...
```

The decorator is detected at import time. Only `@no_auto_trace` or `@logfire.no_auto_trace` are supported. Renaming/aliasing either the function or module won't work. Neither will calling this indirectly via another function.

This decorator simply returns the argument unchanged, so there is zero runtime overhead.

## Spans, logs, and traces

Here's a simple example of using Logfire:

```python
import time

import logfire

logfire.configure()

with logfire.span('This is a span'):
    time.sleep(1)
    logfire.info('This is an info log')
    time.sleep(2)
```

If you run this it should print something like:

```text
Logfire project URL: https://logfire.pydantic.dev/my_username/my_project_name
21:02:55.078 This is a span
21:02:56.084   This is an info log
```

Opening the project URL should show something like this in the Live view:

The blue box with `1+` means that the span contains 1 direct child. Clicking on that box expands the span to reveal its children:

Note that:

1. Any spans or logs created inside the `with logfire.span(...):` block will be children of that span. This lets you organize your logs nicely in a structured tree. You can also see this parent-child relationship in the console logs based on the indentation.
1. Spans have a start and an end time, and thus a duration. This span took 3 seconds to complete.
1. For logs, the start and end time are the same, so they don't have a duration. But you can still see in the UI that the log was created 1 second after the span started and 2 seconds before it ended.

If you click on the 'Explore' link in the top navbar, you can write SQL to explore further, e.g:

Note:

1. Spans and logs are stored together in the same `records` table.
1. The `parent_span_id` of the log is the `span_id` of the span.
1. Both have the same `trace_id`. You can click on it to open a new tab in the Live view filtered to that *trace*.

A *trace* is a tree of spans/logs sharing the same root. Whenever you create a new span/log when there's no active span, a new trace is created. If it's a span, any descendants of that span will be part of the same trace. To keep your logs organized nicely into traces, it's best to create spans at the top level representing high level operations such as handling web server requests.

## Attributes

Spans and logs can have structured data attached to them, e.g:

```python
logfire.info('Hello', name='world')
```

If you click on the 'Hello' log in the Live view, you should see this in the details panel on the right:

This data is stored in the `attributes` column in the `records` table as JSON. You can use e.g. `attributes->>'name' = 'world'` in the SQL filter at the top of the Live view to show only this log. This is used as the `WHERE` clause of a SQL query on the `records` table.

Both spans and logs can have attributes containing arbitrary values which will be intelligently serialized to JSON as needed. You can pass any keyword arguments to set attributes as long as they don't start with an underscore (`_`). That namespace is reserved for other keyword arguments with logfire-specific meanings.

Sometimes it's useful to attach an attribute to a span after it's been created but before it's finished. You can do this by calling the `span.set_attribute` method:

```python
with logfire.span('Calculating...') as span:
    result = 1 + 2
    span.set_attribute('result', result)
```

## Messages and span names

If you run this code:

```python
import logfire

logfire.configure()

for name in ['Alice', 'Bob', 'Carol']:
    logfire.info('Hello {name}', name=name)
```

Here you can see that:

1. The first argument `'Hello {name}'` becomes the value of the `span_name` column. You can use this to find all records coming from the same code even if the messages are different, e.g. with the SQL filter `span_name = 'Hello {name}'`.
1. The span name is also used as a `str.format`-style template which is formatted with the attributes to produce the `message` column. The message is what's shown in the console logs and the Live view.

You can also set `span.message` after a span is started but before it's finished, e.g:

```python
with logfire.span('Calculating...') as span:
    result = 1 + 2
    span.message = f'Calculated: {result}'
```

You could use `message` to filter for related records, e.g. `message like 'Hello%'`, but filtering on the `span_name` column is more efficient because it's indexed. Similarly, it's better to use `span_name = 'Hello {name}' and attributes->>'name' = 'Alice'` than `message = 'Hello Alice'`.

To allow efficiently filtering for related records, span names should be *low cardinality*, meaning they shouldn't vary too much. For example, this would be bad:

```python
name = get_username()
logfire.info('Hello ' + name, name=name)
```

because now the `span_name` column will have a different value for every username. But this would be fine:

```python
word = 'Goodbye' if leaving else 'Hello'
logfire.info(word + ' {name}', name=name)
```

because now the `span_name` column will only have two values (`'Goodbye {name}'` and `'Hello {name}'`) and it's both easier and more efficient to filter on `span_name = 'Hello {name}'` than `span_name = '{word} {name}' and attributes->>'word' = 'Hello'`.

You can use the `_span_name` argument when you want the span name to be different from the message template, e.g:

```python
logfire.info('Hello {name}', name='world', _span_name='Hello')
```

This will set the `span_name` to `'Hello'` and the `message` to `'Hello world'`. Note that the `_span_name` argument starts with an underscore to distinguish it from attributes.

## f-strings

Instead of this:

```python
logfire.info('Hello {name}', name=name)
```

it's much more convenient to use an f-string to avoid repeating `name` three times:

```python
logfire.info(f'Hello {name}')
```

Contrary to the previous section, this *will* work well in Python 3.11+ because Logfire will use special magic to both set the `span_name` to `'Hello {name}'` and set the `name` attribute to the value of the `name` variable, so it's equivalent to the previous snippet. Here's what you need to know about this:

- The feature is enabled by default in Python 3.11+. You can disable it with logfire.configure(inspect_arguments=False). You can also enable it in Python 3.9 and 3.10, but it's more likely to not work correctly.
- Inspecting arguments is expected to always work under normal circumstances. The main caveat is that the source code must be available, so e.g. deploying only `.pyc` files will cause it to fail.
- If inspecting arguments fails, you will get a warning, and the f-string argument will be used as a formatting template. This means you will get high-cardinality span names such as `'Hello Alice'` and no `name` attribute, but the information won't be completely lost.
- If inspecting arguments is enabled, then arguments will be inspected regardless of whether f-strings are being used. So if you write `logfire.info('Hello {name}', name=name)` and inspecting arguments fails, then you will still get a warning.
- The values inside f-strings are evaluated and formatted by Logfire a second time. This means you should avoid code like `logfire.info(f'Hello {get_username()}')` if `get_username()` (or the string conversion of whatever it returns) is expensive or has side effects.
- The first argument must be an actual f-string. `logfire.info(f'Hello {name}')` will work, but `message = f'Hello {name}'; logfire.info(message)` will not, nor will `logfire.info('Hello ' + name)`.
- Inspecting arguments is cached so that the performance overhead of repeatedly inspecting the same f-string is minimal. However, there is a non-negligible overhead of parsing a large source file the first time arguments need to be inspected inside it. Either way, avoiding this overhead requires disabling inspecting arguments entirely, not merely avoiding f-strings.
- If inspecting arguments is enabled, this is also used to extract attribute names from the arguments to `print()` when logfire.instrument_print() is used.

## Exceptions

The `logfire.span` context manager will automatically record any exceptions that cause it to exit, e.g:

```python
import logfire

logfire.configure()

with logfire.span('This is a span'):
    raise ValueError('This is an error')
```

If you click on the span in the Live view, the panel on the right will have an 'Exception Traceback' tab:

Exceptions which are caught and not re-raised will not be recorded, e.g:

```python
with logfire.span('This is a span'):
    try:
        raise ValueError('This is an acceptable error not worth recording')
    except ValueError:
        pass
```

If you want to record a handled exception, use the span.record_exception method:

```python
with logfire.span('This is a span') as span:
    try:
        raise ValueError('Catch this error, but record it')
    except ValueError as e:
        span.record_exception(e)
```

Alternatively, if you only want to log exceptions without creating a span for the normal case, you can use logfire.exception:

```python
try:
    raise ValueError('This is an error')
except ValueError:
    logfire.exception('Something went wrong')
```

`logfire.exception(...)` is equivalent to `logfire.error(..., _exc_info=True)`. You can also use `_exc_info` with the other logging methods if you want to record a traceback in a log with a non-error level. You can set `_exc_info` to a specific exception object if it's not the one being handled. Don't forget the leading underscore!

## Convenient function spans with `@logfire.instrument`

Often you want to wrap a whole function in a span. Instead of doing this:

```python
def my_function(x, y):
    with logfire.span('my_function', x=x, y=y):
        ...
```

you can use the @logfire.instrument decorator:

```python
@logfire.instrument()
def my_function(x, y):
    ...
```

By default, this will add the function arguments to the span as attributes. To disable this (e.g. if the arguments are large objects not worth collecting), use `instrument(extract_args=False)`.

The default span name will be something like `Calling module_name.my_function`. You can pass an alternative span name as the first argument to `instrument`, and it can even be a template into which arguments will be formatted, e.g:

```python
@logfire.instrument('Applying my_function to {x=} and {y=}')
def my_function(x, y):
    ...

my_function(3, 4)
# Logs: Applying my_function to x=3 and y=4
```

Note

- The @logfire.instrument decorator MUST be applied first, i.e., UNDER any other decorators.
- The source code of the function MUST be accessible.

## Log levels

The following methods exist for creating logs with different levels:

- `logfire.trace`
- `logfire.debug`
- `logfire.info`
- `logfire.notice`
- `logfire.warn`
- `logfire.error`
- `logfire.fatal`

To log a message with a variable level you can use `logfire.log`, e.g. `logfire.log('info', 'This is an info log')` is equivalent to `logfire.info('This is an info log')`.

Spans are level `info` by default. You can change this with the `_level` argument, e.g. `with logfire.span('This is a debug span', _level='debug'):`. You can also change the level after the span has started but before it's finished with span.set_level, e.g:

```python
with logfire.span('Doing a thing') as span:
    success = do_thing()
    if not success:
        span.set_level('error')
```

If a span finishes with an unhandled exception, then in addition to recording a traceback as described above, the span's log level will be set to `error`. This will not happen when using the span.record_exception method.

To skip creating logs/spans below a certain level, use the min_level argument to `logfire.configure`, e.g:

```python
import logfire

logfire.configure(min_level='info')
```

For spans, this only applies when `_level` is explicitly specified in `logfire.span`. Setting the level after will be ignored by this. If a span is not created because of the minimum level, this has no effect on parents or children. For example, this code:

```python
import logfire

logfire.configure(min_level='info')

with logfire.span('root') as root:
    root.set_level('debug')  # (1)!
    with logfire.span('debug span excluded', _level='debug'):  # (2)!
        logfire.info('info message')
```

1. This span has already been created with the default level of `info`, so `min_level` won't affect it. It's also logged to the console because that happens at creation time. But it will show in the Live view as having level `debug`.
1. This span is not created because its level is below the minimum. That makes this line a complete no-op.

creates, logs, and sends the `root` span and the `info message` log, with the log being a direct child of the span.

To set the minimum level for console logging only (`info` by default):

```python
import logfire

logfire.configure(console=logfire.ConsoleOptions(min_log_level='debug'))
```

**Pydantic Logfire** can be used to collect metrics from your application and send them to a metrics backend.

Metrics are a great way to record numerical values where you want to see an aggregation of the data (e.g. over time), rather than the individual values.

## System Metrics

The easiest way to start using metrics is to enable system metrics. See the [System Metrics](../../../integrations/system-metrics/) documentation to learn more.

## Manual Metrics

Let's see how to create and use custom metrics in your application.

```py
import logfire

# Create a counter metric
messages_sent = logfire.metric_counter('messages_sent')

# Increment the counter
def send_message():
    messages_sent.add(1)
```

### Counter

The Counter metric is particularly useful when you want to measure the frequency or occurrence of a certain event or state in your application.

You can use this metric for counting things like:

- The number of exceptions caught.
- The number of requests received.
- The number of items processed.

To create a counter metric, use the logfire.metric_counter function:

```py
import logfire

counter = logfire.metric_counter(
    'exceptions',
    unit='1',  # (1)!
    description='Number of exceptions caught'
)

try:
    raise Exception('oops')
except Exception:
    counter.add(1)
```

1. The `unit` parameter is optional, but it's a good practice to specify it. It should be a string that represents the unit of the counter. If the metric is *unitless*, you can use `'1'`.

You can read more about the Counter metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#counter).

### Histogram

The Histogram metric is particularly useful when you want to measure the distribution of a set of values.

You can use this metric for measuring things like:

- The duration of a request.
- The size of a file.
- The number of items in a list.

To create a histogram metric, use the logfire.metric_histogram function:

```py
import logfire

histogram = logfire.metric_histogram(
    'request_duration',
    unit='ms',  # (1)!
    description='Duration of requests'
)

for duration in [10, 20, 30, 40, 50]:
    histogram.record(duration)
```

1. The `unit` parameter is optional, but it's a good practice to specify it. It should be a string that represents the unit of the histogram.

You can read more about the Histogram metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#histogram).

### Up-Down Counter

The "Up-Down Counter" is a type of counter metric that allows both incrementing (up) and decrementing (down) operations. Unlike a regular counter that only allows increments, an up-down counter can be increased or decreased based on the events or states you want to track.

You can use this metric for measuring things like:

- The number of active connections.
- The number of items in a queue.
- The number of users online.

To create an up-down counter metric, use the logfire.metric_up_down_counter function:

```py
import logfire

active_users = logfire.metric_up_down_counter(
    'active_users',
    unit='1',  # (1)!
    description='Number of active users'
)

def user_logged_in():
    active_users.add(1)

def user_logged_out():
    active_users.add(-1)
```

1. The `unit` parameter is optional, but it's a good practice to specify it. It should be a string that represents the unit of the up-down counter. If the metric is *unitless*, you can use `'1'`.

You can read more about the Up-Down Counter metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#updowncounter).

### Gauge

The Gauge metric is particularly useful when you want to measure the current value of a certain state or event in your application. Unlike the counter metric, the gauge metric does not accumulate values over time.

You can use this metric for measuring things like:

- The current temperature.
- The current memory usage.
- The current number of active connections.
- The current number of users online.

To create a gauge metric, use the logfire.metric_gauge function:

```py
import logfire

temperature = logfire.metric_gauge(
    'temperature',
    unit='°C',
    description='Temperature'
)

def set_temperature(value: float):
    temperature.set(value)
```

You can read more about the Gauge metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#gauge).

### Callback Metrics

Callback metrics, or observable metrics, are a way to create metrics that are automatically emitted every 60 seconds in a background thread.

#### Counter Callback

To create a counter callback metric, use the logfire.metric_counter_callback function:

```py
import logfire
from opentelemetry.metrics import CallbackOptions, Observable


def cpu_time_callback(options: CallbackOptions) -> Iterable[Observation]:
    observations = []
    with open("/proc/stat") as procstat:
        procstat.readline()  # skip the first line
        for line in procstat:
            if not line.startswith("cpu"):
                break
            cpu, user_time, nice_time, system_time = line.split()
            observations.append(
                Observation(int(user_time) // 100, {"cpu": cpu, "state": "user"})
            )
            observations.append(
                Observation(int(nice_time) // 100, {"cpu": cpu, "state": "nice"})
            )
            observations.append(
                Observation(int(system_time) // 100, {"cpu": cpu, "state": "system"})
            )
    return observations

logfire.metric_counter_callback(
    'system.cpu.time',
    unit='s',
    callbacks=[cpu_time_callback],
    description='CPU time',
)
```

You can read more about the Counter metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#asynchronous-counter).

#### Gauge Callback

The gauge metric is particularly useful when you want to measure the current value of a certain state or event in your application. Unlike the counter metric, the gauge metric does not accumulate values over time.

To create a gauge callback metric, use the logfire.metric_gauge_callback function:

```py
import logfire


def get_temperature(room: str) -> float:
    ...


def temperature_callback(options: CallbackOptions) -> Iterable[Observation]:
    for room in ["kitchen", "living_room", "bedroom"]:
        temperature = get_temperature(room)
        yield Observation(temperature, {"room": room})


logfire.metric_gauge_callback(
    'temperature',
    unit='°C',
    callbacks=[temperature_callback],
    description='Temperature',
)
```

You can read more about the Gauge metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#asynchronous-gauge).

#### Up-Down Counter Callback

This is the callback version of the [up-down counter metric](#up-down-counter).

To create an up-down counter callback metric, use the logfire.metric_up_down_counter_callback function:

```py
import logfire


def get_active_users() -> int:
    ...


def active_users_callback(options: CallbackOptions) -> Iterable[Observation]:
    active_users = get_active_users()
    yield Observation(active_users, {})


logfire.metric_up_down_counter_callback(
    'active_users',
    unit='1',
    callbacks=[active_users_callback],
    description='Number of active users',
)
```

You can read more about the Up-Down Counter metric in the [OpenTelemetry documentation](https://opentelemetry.io/docs/specs/otel/metrics/api/#asynchronous-updowncounter).

In this section, we'll focus on integrating **Logfire** with your application.

## OpenTelemetry Instrumentation

Harnessing the power of [OpenTelemetry](https://opentelemetry.io/), **Logfire** not only offers broad compatibility with any [OpenTelemetry](https://opentelemetry.io/) instrumentation package, but also includes a user-friendly CLI command that effortlessly highlights any missing components in your project.

To inspect your project, run the following command:

```bash
logfire inspect
```

This will output the projects you need to install to have optimal OpenTelemetry instrumentation:

To install the missing packages, copy the command provided by the `inspect` command, and run it in your terminal.

Each instrumentation package has its own way to be configured. Check our [Integrations](../../../integrations/) page to learn how to configure them.

## Logging Integration (Optional)

Attention

If you are creating a new application or are not using a logging system, you can skip this step.

You should use **Logfire** itself to collect logs from your application.

All the standard logging methods are supported e.g. logfire.info().

There are many logging systems within the Python ecosystem, and **Logfire** provides integrations for the most popular ones: [Standard Library Logging](../../../integrations/logging/), [Loguru](../../../integrations/loguru/), and [Structlog](../../../integrations/structlog/).

### Standard Library

To integrate **Logfire** with the standard library logging module, you can use the LogfireLoggingHandler class.

The minimal configuration would be the following:

```py
from logging import basicConfig

import logfire

logfire.configure()
basicConfig(handlers=[logfire.LogfireLoggingHandler()])
```

Now imagine, that you have a logger in your application:

main.py

```py
from logging import basicConfig, getLogger

import logfire

logfire.configure()
basicConfig(handlers=[logfire.LogfireLoggingHandler()])

logger = getLogger(__name__)
logger.error("Hello %s!", "Fred")
```

If we run the above code, with `python main.py`, we will see the following output:

If you go to the link, you will see the `"Hello Fred!"` log in the Web UI:

It is simple as that! Cool, right? 🤘

### Loguru

To integrate with Loguru, check out the [Loguru](../../../integrations/loguru/) page.

### Structlog

To integrate with Structlog, check out the [Structlog](../../../integrations/structlog/) page.

With **Logfire**, use Alerts to notify you when certain conditions are met.

## Create an alert

Let's see in practice how to create an alert.

1. Go to the **Alerts** tab in the left sidebar.
1. Click the **Create alert** button.

Then you'll see the following form:

The **Query** field is where you define the conditions that will trigger the alert. For example, you can set up an alert to notify you when the number of errors in your logs exceeds a certain threshold.

On our example, we're going to set up an alert that will trigger when an exception occurs in the `api` service and the route is `/members/{user_id}`.

```sql
SELECT trace_id, exception_type, exception_message FROM records  -- (1)!
WHERE
    is_exception and
    service_name = 'api' and
    attributes->>'http.route' = '/members/{user_id}'  -- (2)!
```

1. The `SELECT ... FROM records` statement is the base query that will be executed. The **records** table contains the spans and logs data. `trace_id` links to the trace in the live view when viewing the alert run results in the web UI.
1. The `attributes` field is a JSON field that contains additional information about the record. In this case, we're using the `http.route` attribute to filter the records by route.

The **Time window** field allows you to specify the time range over which the query will be executed.

The **Webhook URL** field is where you can specify a URL to which the alert will send a POST request when triggered. For now, **Logfire** alerts only send the requests in [Slack format](https://api.slack.com/reference/surfaces/formatting).

Get a Slack webhook URL

To get a Slack webhook URL, follow the instructions in the [Slack documentation](https://api.slack.com/messaging/webhooks).

After filling in the form, click the **Create alert** button. And... Alert created!

## Alert History

After creating an alert, you'll be redirected to the alerts' list. There you can see the alerts you've created and their status.

If the query was not matched in the last time window, you'll see **no matches** next to the alert name, and no results in the histogram table of the selected time period.

Otherwise, you'll see the number of matches highlighted in orange.

In this case, you'll also receive a notification in the Webhook URL you've set up.

## Edit an alert

You can configure an alert by clicking on the **Configuration** button on the right side of the alert.

You can update the alert, or delete it by clicking the **Delete** button. If instead of deleting the alert, you want to disable it, you can click on the **Active** switch.

# Dashboards

This guide explains how to use dashboards in the Logfire UI to visualize your observability data. Dashboards allow you to create custom visualizations using SQL queries.

## Overview

There are two types of dashboards:

- **Standard dashboards**: Pre-configured dashboards created and maintained by the Logfire team, providing you with continuous updates and improvements without any effort on your part. You can enable or disable them for your project, but you can't modify them directly.
- **Custom dashboards**: Dashboards that you create and maintain. They are fully editable and customizable, allowing you to define queries, layouts, chart types, and variables.

In general, it's a good idea to start with standard dashboards. If they don't meet your needs, you can either use one as a [template for a custom dashboard](#using-a-standard-dashboard-as-a-template) or build a new one from scratch.

## Standard Dashboards

### Usage Overview

This dashboard is recommended for all users to [manage their costs](../../../logfire-costs/#standard-usage-dashboard). It breaks down your data by [environment](../../../reference/sql/#deployment_environment), [service](../../../reference/sql/#service_name), [scope](../../../reference/sql/#otel_scope_name) (i.e. instrumentation), and [`span_name`](../../../reference/sql/#span_name)/`metric_name` for `records`/`metrics` respectively. This lets you see which services and operations are generating the most data.

### Exceptions

This dashboard is recommended for all users, especially for monitoring Python applications. It shows the most common exceptions grouped by [service](../../../reference/sql/#service_name), [scope](../../../reference/sql/#otel_scope_name) (i.e. instrumentation), [`span_name`](../../../reference/sql/#span_name), and [`exception_type`](../../../reference/sql/#exception_type). You can also filter by any of these four columns in the variable fields at the top.

Within each row you can also see the most common [`message`](../../../reference/sql/#message) and [`exception_message`](../../../reference/sql/#exception_message) values. These are more variable (higher cardinality) which is why they don't each produce a new row. If there are multiple different values, each will be shown with a count in brackets at the start, on a separate line. **Double-click on a cell to see all the values within.** Note that `message` is often just the same as `span_name`.

Exceptions are usually errors, but not always. Some exceptions are special-cased and set the [`level`](../../../reference/sql/#level) to `warn`. By default, the dashboard is filtered to `level >= 'error'`, set the 'Errors only' dropdown to 'No' to see all exceptions.

Finally, scroll all the way to the right to see the 'SQL filter to copy to Live View' column to investigate the details of any group.

### Web Server Metrics

This dashboard gives an overview of how long each of your web server endpoints takes to respond to requests and how often they succeed and fail. It relies on the standard OpenTelemetry `http.server.duration`/`http.server.request.duration` metric which is collected by many instrumentation libraries, including those for FastAPI, Flask, Django, ASGI, and WSGI. The charts give a breakdown by endpoint (and sometimes status code) both overall and over time. Hover over each time series to see the most impactful endpoint at the top of the tooltip. The charts show:

- **Total duration:** Endpoints which need to either be optimized or called less often.
- **Average duration:** Endpoints which are slow on average and need to be optimized.
- **2xx request count:** Number of successful requests (HTTP status code between 200 and 299) per endpoint.
- **5xx request count:** Number of server errors (HTTP status code of 500 or greater) per endpoint.
- **4xx request count:** Number of bad requests (HTTP status code between 400 and 499) per endpoint.

### Token Usage

This dashboard breaks down input and output LLM token usage by model. It comes in two variants. Both have the same charts, but they use different data sources:

- **Token Usage (from `records`):** Uses data from the `records` table, specifically span [attributes](../../../reference/sql/#attributes) following OpenTelemetry conventions. This variant works with more instrumentations, as some don't emit metrics. It's also easier to [use as a template](#using-a-standard-dashboard-as-a-template) if you want to filter by other attributes.
- **Token Usage (from `metrics`):** Uses data from the `metrics` table, specifically the [`gen_ai.client.token.usage`](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage) metric. This variant is more performant, so you can load data over bigger time ranges more quickly. It's also more accurate if your spans are [sampled](../../../how-to-guides/sampling/).

If you're only using the [Pydantic AI](../../../integrations/llms/pydanticai/) instrumentation, and you have version 0.2.17 of Pydantic AI or later, we recommend using the `metrics` variant. Otherwise we suggest enabling both variants and checking. If they look roughly identical (some small differences are expected), you can disable the `records` variant to improve performance.

### Basic System Metrics

This dashboard shows essential system resource utilization metrics. It comes in two variants:

- **Basic System Metrics (Logfire):** Uses the data exported by [`logfire.instrument_system_metrics()`](../../../integrations/system-metrics/).
- **Basic System Metrics (OpenTelemetry):** Uses data exported by any OpenTelemetry-based instrumentation following the standard semantic conventions.

Both variants include the following metrics:

- **Number of Processes:** Total number of running processes on the system.
- **System CPU usage %:** Percentage of total available processing power utilized by the whole system, i.e. the average across all CPU cores.
- **Process CPU usage %:** CPU used by a single process, where e.g. using 2 CPU cores to full capacity would result in a value of 200%.
- **Memory Usage %:** Percentage of memory currently in use by the system.
- **Swap Usage %:** Percentage of swap space currently in use by the system.

### Enabling a Standard Dashboard

To enable a standard dashboard:

1. Go to the **Dashboards** tab in the top navigation bar.
1. Click the **+ Dashboard** button.
1. Browse the list of available dashboards under the **Standard** tab.
1. Click **Enable dashboard** to add it to your project.

You can view and interact with standard dashboards, but you cannot edit them.

### Using a Standard Dashboard as a Template

You can use any standard dashboard as a template by exporting it to JSON and then importing it from JSON for a new custom dashboard.

1. From a standard dashboard, click the **Download dashboard as code** icon in the toolbar on the top right. This will download a JSON file to your machine.
1. Go to the **Custom** tab and select the **Import JSON** option.
1. Import the file you downloaded. This creates a new, fully editable custom dashboard from the template.

______________________________________________________________________

## Creating custom dashboards

To create a dashboard from scratch:

1. Click the **+ Dashboard** button.
1. Select the **Custom** tab.

Custom dashboards are structured in a hierarchy:

- Dashboard
  - Panel Group (1 or more)
    - Panel (1 or more)
      - Chart (1 only, a [specific type](#chart-types))

By default, new dashboards start with one panel group.

You can add more panel groups to better organize your dashboard. This is useful for grouping related visualizations, effectively allowing you to have multiple views within a single dashboard.

To add a new group, click the **Panel Group** button in the top right. You can name the group and set whether it should be expanded or collapsed by default when the dashboard loads.

To add a new visualization, you add a panel to a group. Click the **Panel** button in the top right. Inside each panel, you'll configure a chart and the SQL query that powers it.

You can rearrange and resize panels by dragging and dropping them after clicking the **Edit layout** button.

### Chart Types

Logfire uses SQL as the query language for dashboard visualizations. Each chart in your dashboard requires one of two types of queries:

- **Time Series Query**: This query type is for visualizing data over time. It must include a timestamp in the selected columns, typically `time_bucket($resolution, start_timestamp)` when querying `records` or `time_bucket($resolution, recorded_timestamp)` when querying `metrics` - see [below](#resolution-variable). This will be used as the x-axis.
- **Non-Time Series Query**: This query type is for displaying data where the evolution of data over time is not the primary focus, e.g., a bar chart showing your top slowest endpoints.

Here's a list of the chart types and the query type they require.

| Chart Type  | Query Type              |
| ----------- | ----------------------- |
| Time Series | `Time Series Query`     |
| Table       | `Non Time Series Query` |
| Bar Chart   | `Non Time Series Query` |
| Pie Chart   | `Non Time Series Query` |
| Values      | `Non Time Series Query` |

To configure a chart:

1. Choose the chart type.
1. Write your SQL query.
1. Customize the formatting, labels, and appearance.

______________________________________________________________________

### Variables

You can define variables to make dashboards dynamic.

#### Variable Types

- **Text variable**: Allows users to enter any string value.
- **List variable**: Allows users to select a value from a predefined list.

To add variables to a custom dashboard:

1. Open the dashboard you want to edit.
1. Click **Variables** in the top right to open the variable settings panel.
1. Click **+ Add variable**.
1. Define and configure your variables.

Once defined, variables can be referenced in SQL queries using the format `$your_variable_name`

______________________________________________________________________

## Writing Queries

As mentioned in the [Chart Types](#chart-types) section, there are two main types of queries you'll write for dashboards. Here are some useful examples for each type.

### Time Series Queries

These queries visualize data over time and must include a timestamp column.

**Request count over time:**

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    count() as count
FROM records
GROUP BY x
```

### Non Time Series Queries

These queries focus on aggregating data without the time dimension, perfect for tables, bar charts, and pie charts.

**Most common operations:**

```sql
SELECT
    COUNT() AS count,
    span_name
FROM records
GROUP BY span_name
ORDER BY count DESC
LIMIT 10
```

For comprehensive examples, advanced patterns, and chart-specific configuration tips, see the [Writing SQL Queries for Dashboards](../../../how-to-guides/write-dashboard-queries/) guide.

Please also refer to the [SQL Reference](../../../reference/sql/) and [Metrics Schema](../explore/#metrics-schema) for more information on the data available to you.

______________________________________________________________________

### Variable Usage

You can reference dashboard variables in SQL queries using the `$variable` syntax:

```sql
SELECT count() FROM records WHERE service_name = $service_name
```

Variables can only be used in SQL queries. They cannot be used in chart titles or other non-query fields.

### Resolution Variable

All dashboards have access to a special `$resolution` variable that can be used in your queries. This value is dynamically selected based on the dashboard's time duration to ensure optimal performance and data density. You can use it for time bucketing:

```sql
SELECT
  time_bucket($resolution, start_timestamp) AS x,
  count(1) as count
FROM records
GROUP BY x;
```

______________________________________________________________________

## Editing the layout

You can edit the layout of a dashboard by clicking the **Edit layout** button in the top left. This will allow you to drag panels to move and resize them. You can also reorder panel groups . Once you're done making changes, click the **Save** button to persist your changes.

### Move panels

While in **Edit layout** mode, you can move panels by dragging them from the top right corner.

### Resize panels

While in **Edit layout** mode, you can resize panels by dragging the bottom right corner.

### Reorder panel groups

While in **Edit layout** mode, you can reorder panel groups by clicking the up and down arrows in the top right corner of each panel group.

## Duration, and Refresh

Each dashboard has settings for:

- **Duration**: Controls the time window for the data shown. You can select from predefined ranges like `last 5 minutes`, `last 15 minutes`, `last 30 minutes`, `last 6 hours` up to `last 14 days`, or specify a custom time range.
- **Refresh Interval**: Sets how often the dashboard automatically refreshes its data. Options include `off`, `5s`, `10s`, `15s`, `30s`, and `1m`. The duration and refresh settings are in the top-right corner of the dashboard view.

With **Logfire**, you can use the Explore page to run arbitrary SQL queries against your trace and metric data to analyze and investigate your system.

## Querying Traces

The primary table you will query is the `records` table, which contains all the spans/logs from your traced requests.

To query the records, simply start your query with `SELECT ... FROM records` and add a `WHERE` clause to filter the spans you want.

For example, here is a query that returns the message, start_timestamp, duration, and attributes for all spans that have exceptions:

```sql
SELECT
  message,
  start_timestamp,
  duration * 1000 AS duration_ms,
  attributes
FROM records
WHERE is_exception
```

You can run more complex queries as well, using subqueries, CTEs, joins, aggregations, custom expressions, and any other standard SQL.

See the [SQL Reference](../../../reference/sql/) for details on the SQL syntax supported by Logfire as well as the tables and their columns.

## Cross-linking with Live View

After running a query, you can take any `trace_id` and/or `span_id` and use it to look up data shown as traces in the Live View.

Simply go to the Live View and enter a query like:

```text
trace_id = '7bda3ddf6e6d4a0c8386093209eb0bfc' -- replace with a real trace_id of your own
```

This will show all the spans with that specific trace ID.

## Metrics Schema

In addition to traces, you can also query your metrics data using the `metrics` table.

The schema of the `metrics` table is:

```sql
CREATE TABLE metrics AS (
    recorded_timestamp timestamp with time zone,
    metric_name text,
    metric_type text,
    unit text,
    start_timestamp timestamp with time zone,
    aggregation_temporality public.aggregation_temporality,
    is_monotonic boolean,
    metric_description text,
    scalar_value double precision,
    histogram_min double precision,
    histogram_max double precision,
    histogram_count integer,
    histogram_sum double precision,
    exp_histogram_scale integer,
    exp_histogram_zero_count integer,
    exp_histogram_zero_threshold double precision,
    exp_histogram_positive_bucket_counts integer[],
    exp_histogram_positive_bucket_counts_offset integer,
    exp_histogram_negative_bucket_counts integer[],
    exp_histogram_negative_bucket_counts_offset integer,
    histogram_bucket_counts integer[],
    histogram_explicit_bounds double precision[],
    attributes jsonb,
    otel_scope_name text,
    otel_scope_version text,
    otel_scope_attributes jsonb,
    service_namespace text,
    service_name text,
    service_version text,
    service_instance_id text,
    process_pid integer
)
```

You can query metrics using standard SQL, just like traces. For example:

```sql
SELECT *
FROM metrics
WHERE metric_name = 'system.cpu.time'
  AND recorded_timestamp > now() - interval '1 hour'
```

## Executing Queries

To execute a query, type or paste it into the query editor and click the "Run Query" button.

You can modify the time range of the query using the dropdown next to the button. There is also a "Limit" dropdown that controls the maximum number of result rows returned.

The Explore page provides a flexible interface to query your traces and metrics using standard SQL.

Happy querying!

# Issues

Issues in Beta

The Issues feature is still under construction, it could break or behave in unexpected ways.

Please [create an issue](https://github.com/pydantic/logfire/issues/new/choose) if you find bugs, get frustrated, or have questions.

Issues provide automatic exception tracking and grouping. Enable issues to identify, prioritize, and resolve problems in your application.

## What are Issues?

Issues are automatically created when Logfire detects exceptions in your application. Instead of manually searching through individual exception logs, Logfire groups similar exceptions together based on their fingerprint, giving you a clear overview of problems affecting your application.

Each issue shows:

- **Exception type and message** - What went wrong
- **Hits** - How many times this exception occurred
- **Last seen and Age** - When the issue was last seen and how long since it first appeared
- **Affected services and environments** - Where the problem is happening
- **Stack trace** - Technical details for debugging

## Issue States

Issues can be in one of three states:

### Open

Active issues that need attention. New exceptions automatically create open issues.

### Resolved

Issues you've marked as fixed. **Resolved issues will automatically reopen** if the same exception occurs again in the future.

### Ignored

Issues you want to permanently ignore. **Ignored issues never reopen** and won't send notifications, even if the same exception continues to occur.

## Managing Issues

### Individual Issue Actions

For each issue, you can:

- **Resolve** - Mark as fixed (will reopen if exception occurs again)
- **Ignore** - Silence permanently (no alerts even if exception recurs)
- **Re-open** - Mark a resolved or ignored issue as active again
- **Delete** - Permanently delete resolved issues

## Turn on Issue Alerts

By default, Issues are only visible in the Logfire web interface. To be notified when Issues occur in your other tools, you can select external channels.

### Select an Alert Channel

You can:

1. Create a new channel - Add a webhook URL for services like Slack, Discord, Microsoft Teams, or any service that accepts webhooks
1. Use an existing channel - Select from previously configured notification channels.

### Create a new channel:

1. Go to **Settings** on the **Issues** page
1. Click **Add another channel**
1. Enter a channel name and webhook URL
1. Test the channel before saving

### Managing Notifications

- Select one or many channels where you want to receive issue notifications
- Edit existing channels using the edit icon

Notifications are sent when new issues open and when resolved issues reopen. Ignored issues never send notifications.

### Bulk Actions

To select multiple issues at once, hold down `shift` or `cmd` (macOS) / `ctrl` (windows).

After selecting more than one issue you can:

- Ignore all selected issues
- Resolve all selected issues

## Fix with AI

Use this feature to debug your exceptions using your local LLM coding tool plus the Logfire MCP server.

The Fix with AI button uses Logfire's MCP server to give your AI assistant access to your application data for debugging. You can choose between:

- Claude Code - Debug with Claude Code
- OpenAI Codex - Debug with OpenAI Codex

Run the command provided to retrieve the exception information from the Logfire MCP server so your AI assistant can analyze that specific exception and provide debugging suggestions based on your application context.

Want us to integrate more AI Code assistants? [Let us know](https://logfire.pydantic.dev/docs/help/)

## Sorting and Searching

Search for exception message text using the Search field.

Use the sort options to find specific issues: *Click twice on any sort to reverse the order*

- **Sort by Last Seen** - most \<> least recent issues
- **Sort by First Seen** - youngest \<> oldest issues issues
- **Sort by Message** - sort exception message alphabetically (A-Z) / (Z-A)
- **Sort by Hits** - most \<> least hits
- **Sort by Exception** - sort exception alphabetically (A-Z) / (Z-A)

## Best Practices

### Issue Management Workflow

1. **Monitor open issues regularly** - Check for new exceptions affecting your application
1. **Investigate high-frequency issues first** - Focus on problems impacting the most users
1. **Resolve issues after fixing** - Mark issues as resolved once you've deployed a fix
1. **Ignore noise, not problems** - Only ignore issues that are truly safe to disregard

### When to Resolve vs Ignore

**Resolve** when:

- You've fixed the underlying problem
- You want to be notified if the issue returns
- The exception indicates a real bug or problem

**Ignore** when:

- The exception is expected behavior (e.g., user input validation errors)
- Third-party service errors you can't control
- Legacy code issues you've decided not to fix

### Cleanup

Periodically delete resolved issues that haven't reoccurred to keep your issues list manageable. Remember that deleting an issue is permanent - if the same exception occurs later, it will create a new issue.

## Disabling Issues

If you want to stop tracking issues for a project:

1. Go to **Alerts** menu for the project
1. Click **Settings** on the **Issues** page
1. Click **Disable issues**

Permanent Data Loss

Disabling issues will permanently delete all existing issues data, states, and history. You can re-enable issues later, but all previous issue data will be gone and you'll start fresh.

# Live View

The live view is the focal point of **Logfire**, where you can see traces arrive in real-time.

The live view is useful for watching what's going on within your application in real-time (as the name suggests). You can also explore historical data in the **search pane**.

## SQL search pane

To search the live view, click `Search your spans` (keyboard shortcut `/`), this opens the search pane:

### SQL Search

For confident SQL users, write your queries directly here. For devs who want a bit of help, try the new [Pydantic AI](https://ai.pydantic.dev/) feature which generates a SQL query based on your prompt. You can also review the fields available and populate your SQL automatically using the `Reference` list, see more on this below.

**WHERE clause** As the greyed out `SELECT * FROM RECORDS WHERE` implies, you're searching inside the `WHERE` clause of a SQL query. It has auto-complete & schema hints, so try typing something to get a reminder. To run your query click `Run` or keyboard shortcut `cmd+enter` (or `ctrl+enter` on Windows/Linux).

Note: you can run more complex queries on the [explore screen](../explore/)

The schema for the records table is:

```sql
CREATE TABLE records AS (
    start_timestamp timestamp with time zone,
    created_at timestamp with time zone,
    trace_id text,
    span_id text,
    parent_span_id text,
    kind span_kind,
    end_timestamp timestamp with time zone,
    level smallint,
    span_name text,
    message text,
    attributes_json_schema text,
    attributes jsonb,
    tags text[],
    otel_links jsonb,
    otel_events jsonb,
    is_exception boolean,
    otel_status_code status_code,
    otel_status_message text,
    otel_scope_name text,
    otel_scope_version text,
    otel_scope_attributes jsonb,
    service_namespace text,
    service_name text,
    service_version text,
    service_instance_id text,
    process_pid integer
)
```

You can search for any of these in the `Reference` list:

If you're not sure where to start, scroll down to the `Start here` for beginner-friendly suggestions.

### Ask in Language -> Get SQL

Write your question in your native language, and the model will convert that question to a SQL query.

This is useful if you're not confident with SQL and/or can't quite remember how to format more complicated clauses. You have the option to create a completely new query with `Get new SQL`, or (if you have some SQL already) modify the existing query with `Modify existing SQL`.

Under the hood this feature uses an LLM running with [Pydantic AI](https://github.com/pydantic/pydantic-ai).

### Reference

Reference: A list of pre-populated query clauses. Clicking any of the clauses will populate the SQL editor, and (where applicable) you can choose a value from the autopopulated dropdown.

This list gives you a powerful way to rapidly generate the query you need, while simultaneously learning more about all the ways you can search your data. Clicking multiple clauses will add them to your query with a SQL `AND` statement. If you'd like something other than an `AND` statement, you can replace this with alternative SQL operators like `OR`, or `NOT`.

## Details panel closed

This is what you'll see when you come to the live view of a project with some data.

1. **Organization and project labels:** In this example, the organization is `christophergs`, and the project is `docs-app`. You can click the organization name to go to the organization overview page; the project name is a link to this page.
1. **Environment:** In the above screenshot, this is set to `all envs`. See the [environments docs](../../../how-to-guides/environments/) for details.
1. **Timeline:** This shows a histogram of the counts of spans matching your query over time. The blue-highlighted section corresponds to the time range currently visible in the scrollable list of traces below. You can click at points on this line to move to viewing logs from that point in time.
1. **Status label:** This should show "Connected" if your query is successful and you are receiving live data. If you have a syntax error in your query or run into other issues, you should see details about the problem here.
1. **Level, Service, scope, and tags visibility filters:** Here you can control whether certain spans are displayed based on their level, service, scope, or tags. Important note: this only filters data **currently on the screen**.
1. **A collapsed trace:** The `+` symbol to the left of the span message indicates that this span has child spans, and can be expanded to view them by clicking on the `+` button.
1. **Scope label:** This pill contains the `otel_scope_name` of the span. This is the name of the OpenTelemetry scope that produced the span. Generally, OpenTelemetry scopes correspond to instrumentations, so this generally gives you a sense of what library's instrumentation produced the span. This will be logfire when producing spans using the logfire APIs, but will be the name of the OpenTelemetry instrumentation package if the span was produced by another instrumentation. You can hover to see version info.

## Details panel open

When you click on a span in the Traces Scroll, it will open the details panel, which you can see here.

1. **Level icon:** This icon represents the highest level of this span and any of its descendants.
1. **Details panel orientation toggle, and other buttons:** The second button copies a link to view this specific span. The X closes the details panel for this span.
1. **Exception warning:** This exception indicator is present because an exception bubbled through this span. You can see more details in the Exception Traceback details tab.
1. **Pinned span attributes:** This section contains some details about the span. The link icons on the "Trace ID" and "Span ID" pills can be clicked to take you to a view of the trace or span, respectively.
1. **Details tabs:** These tabs include more detailed information about the span. Some tabs, such as the Exception Details tab, will only be present for spans with data relevant to that tab.
1. **Arguments panel:** If a span was created with one of the logfire span/logging APIs, and some arguments were present, those arguments will be shown here, displayed as a Python dictionary.
1. **Attributes:** Full span attributes panel - when any attributes are present, this panel will show the full list of OpenTelemetry attributes on the span.

# LLM Panels

Use **Logfire’s LLM panels** to inspect every round‑trip between your application and a large‑language model (LLM) in real time. For each span Logfire captures:

- The ordered list of **system / user / assistant** messages
- Any **tool calls** (name, arguments, structured return value)
- **Files** referenced in the prompt (previewed inline or via link)
- **Model metadata** – latency, input / output tokens, and total cost

That context makes it easy to debug prompts, shrink token counts, and compare model performance side‑by‑side.

## Understand token & cost badges

Spans in the Live view may have a token usage badge on the right, indicated by a coin icon. If the badge contains a ∑ symbol, that means the badge is showing the sum of token usages across all descendants (children and nested children) of that span. If there's no ∑ symbol, then that specific span represents an LLM request and has recorded token usage on it directly.

Hover over either to see:

- Model name
- Input, Output & Total tokens
- Input, Output & Total cost (USD)

______________________________________________________________________

## Open the LLM details panel

Click an LLM span to open the details panel.

| Section        | What you’ll see                                            |
| -------------- | ---------------------------------------------------------- |
| **Messages**   | System, user, assistant, and tool messages in order.       |
| **Tool calls** | Name, arguments, and returned payload (objects or arrays). |
| **Files**      | Links or inline previews of binary or blob uploads.        |
| **Metadata**   | Model name, token counts, and cost.                        |

______________________________________________________________________

## Supported Instrumentations

| Instrumentation                                                                       | Token badges | Costs | LLM details panel |
| ------------------------------------------------------------------------------------- | ------------ | ----- | ----------------- |
| [Pydantic AI](../../../integrations/llms/pydanticai/)                                 | ✅           | ✅    | ✅                |
| [OpenAI](../../../integrations/llms/openai/)                                          | ✅           | ✅    | ✅                |
| [LangChain](../../../integrations/llms/langchain/)                                    | ✅           | ✅    | ✅                |
| [Anthropic](../../../integrations/llms/anthropic/)                                    |              |       | ✅                |
| [Google ADK](https://github.com/pydantic/logfire/issues/1201#issuecomment-3012423974) | ✅           |       |                   |

Tokens and costs are more generally supported by any instrumentation that follows the standard [OpenTelemetry semantic conventions for GenAI spans](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/). The following snippet shows the attributes required if you want to log the data manually:

```python
import logfire

logfire.configure()

logfire.info(
    'LLM request',
    **{
        'gen_ai.system': 'google',
        'gen_ai.request.model': 'gemini-2.0-flash',
        'gen_ai.response.model': 'gemini-2.0-flash-001',
        'gen_ai.usage.input_tokens': 20,
        'gen_ai.usage.output_tokens': 40,
    },
)
```

We are actively engaged with the OpenTelemetry community to improve the GenAI specification, so expect more instrumentations to be fully supported in the future.

## Example LLM panel views

### Single‑prompt calls

```python
agent = Agent("google-gla:gemini-1.5-flash")
result = agent.run_sync("Which city is the capital of France?")
print(result.output)
```

Add a system prompt and Logfire captures it too:

```python
agent = Agent(
    "google-gla:gemini-1.5-flash",
    system_prompt="You are a helpful assistant."
)
result = agent.run_sync("Please write me a limerick about Python logging.")
```

______________________________________________________________________

### Agents and tool calls

Logfire displays every tool invocation and its structured response.

______________________________________________________________________

### File uploads

When a prompt includes a file, binary, blob, or URL, Logfire attaches a preview so you can verify exactly what the model received.

#### LLM panel with image url:

#### LLM panel with PDF file:

# Public Traces

Public Traces lets you to create shareable links for specific traces. Perfect for collaborating with anyone who doesn't have Logfire access, or sharing debugging information.

## Overview

When you create a public trace link, anyone with access to the link can view the complete trace data, **including all spans and their attributes**. This is particularly useful for:

- Sharing debugging information with external collaborators
- Providing trace examples in support tickets
- Collaborating across teams without granting full project access

## Creating a Public Trace

1. Navigate to any trace in your project

1. Select the inner span you want to highlight (optional)

1. In the details panel, click the **Private** button

1. Configure the link expiration

1. If you want the link to navigate to the inner span you've selected, use **This span selected**, otherwise uncheck this and the public link will point to root span of the trace.

1. Click **Create** to generate the shareable link

Warning

Due to data retention limits, the shared data might expire before the selected expiration.

## Managing Public Traces

All public trace links are managed in the **Shared Traces** section under Project Settings. From here you can:

- View all active public links with their trace/span IDs
- See creation dates and expiration times
- Copy existing public links
- Delete public links when no longer needed

## Security Considerations

- Public traces are accessible to anyone with the link
- Consider the sensitivity of the data before creating public links
- Use appropriate expiration times to limit exposure
- Regularly review and clean up unused public links

# Saved Searches

Logfire's **Saved Searches** feature allows you to quickly access, organize, and share your most useful queries in the web UI. This feature is available on the Live page, and can be tested on the [public demo](https://logfire-eu.pydantic.dev/demo/logfire-demo).

## 1. Opening the Search Panel

To get started, click the **Search & filter with SQL** button to open the search panel.

You'll see a panel where you can enter a query using either:

- **Direct SQL**: Write your own SQL `WHERE` clause to filter records.
- **Natural Language (AI)**: Describe your query in plain English (or another language), and Logfire will generate the SQL for you using AI.

## 2. Writing and Generating Queries

- **Direct SQL**: Type your SQL directly in the editor. Autocomplete and schema hints are available.

- **Generate SQL with AI**: Click the "Generate SQL with AI" option, then describe what you want (e.g., "all exceptions in the last hour"). The AI will convert your description into a SQL query.

- You can also enable the **Include existing SQL code** option. This allows the AI to modify or build upon your current SQL query, rather than starting from scratch.

- **Filter by...**: Use the filter pills below the editor to quickly add common filters (e.g., tags, service name, level).

## 3. Running and Reviewing Queries

- Click **Run** to execute your query.
- The results will appear in the main view, and your query will be added to the **Recent** tab for easy access.

## 4. Saving Searches

- To save a query, click the **Save** button next to the editor.
- In the popover, enter a name for your search.
- Choose whether to make it **Public** (visible to all project members) or **Private** (visible only to you) using the toggle.
- Click **Save**. Your search will now appear in the **Saved** tab.

## 5. Managing Recent and Saved Searches

- **Recent**: Shows your most recently run queries. You can search by title or sort them.
- **Saved**: Shows all saved searches. You can search by title. Public searches are visible to all project members; private searches are only visible to you.
- To delete a saved search, click the three-dot menu next to it and select **Delete**.
- **Tip:** Clicking any item in the Recent or Saved list will immediately execute the corresponding query and display the results.

## 6. Sharing and Collaboration

- Public saved searches make it easy to share useful queries with your team.
- Private searches help you keep personal or experimental queries organized.

______________________________________________________________________

## Visual Workflow

```
flowchart TD
    A["Open Logfire Demo or Project"] --> B["Click 'Search & filter with SQL'"]
    B --> C["Enter SQL Query or Describe Query in Natural Language"]
    C --> D["Optionally Use 'Filter by...' for Additional Filters"]
    D --> E["Click 'Run' to Execute Query"]
    E --> F["Query Appears in 'Recent' Tab"]
    F --> G["Click 'Save' to Save Query"]
    G --> H["Name the Search and Choose Public/Private"]
    H --> I["Query Appears in 'Saved' Tab"]
    F --> J["Search or Sort Recent Queries"]
    I --> K["Search Saved Queries by Title"]
```

______________________________________________________________________

## Tips

- You can test this feature on the [public demo page](https://logfire-eu.pydantic.dev/demo/logfire-demo).
- Use the search fields in both **Recent** and **Saved** tabs to quickly find queries by name.
- Use the **Filter by...** pills to build queries faster without writing SQL.

______________________________________________________________________

**Next Steps:**

- [Explore more about querying with SQL](../explore/)
- [Learn about dashboards and alerts](../dashboards/), (alerts.md)

# Alternative backends

**Logfire** uses the OpenTelemetry standard. This means that you can configure the SDK to export to any backend that supports OpenTelemetry.

The easiest way is to set the `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable to a URL that points to your backend. This will be used as a base, and the SDK will append `/v1/traces` and `/v1/metrics` to the URL to send traces and metrics, respectively.

Alternatively, you can use the `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`, `OTEL_EXPORTER_OTLP_METRICS_ENDPOINT` and `OTEL_EXPORTER_OTLP_LOGS_ENDPOINT` environment variables to specify the URLs for traces, metrics and logs separately. These URLs should include the full path, including `/v1/traces` and `/v1/metrics`.

Note

The data will be encoded using **Protobuf** (not JSON) and sent over **HTTP** (not gRPC).

Make sure that your backend supports this!

## Example with Jaeger

Run this minimal command to start a [Jaeger](https://www.jaegertracing.io/) container:

```text
docker run --rm \
  -p 16686:16686 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

Then run this code:

```python
import os

import logfire

# Jaeger only supports traces, not metrics, so only set the traces endpoint
# to avoid errors about failing to export metrics.
# Use port 4318 for HTTP, not 4317 for gRPC.
traces_endpoint = 'http://localhost:4318/v1/traces'
os.environ['OTEL_EXPORTER_OTLP_TRACES_ENDPOINT'] = traces_endpoint

logfire.configure(
    # Setting a service name is good practice in general, but especially
    # important for Jaeger, otherwise spans will be labeled as 'unknown_service'
    service_name='my_logfire_service',

    # Sending to Logfire is on by default regardless of the OTEL env vars.
    # Keep this line here if you don't want to send to both Jaeger and Logfire.
    send_to_logfire=False,
)

with logfire.span('This is a span'):
    logfire.info('Logfire logs are also actually just spans!')
```

Finally open <http://localhost:16686/search?service=my_logfire_service> to see the traces in the Jaeger UI.

You can click on a specific trace to get a more detailed view:

And this is how a more "complex" trace would look like:

## Other environment variables

If `OTEL_TRACES_EXPORTER` and/or `OTEL_METRICS_EXPORTER` are set to any non-empty value other than `otlp`, then **Logfire** will ignore the corresponding `OTEL_EXPORTER_OTLP_*` variables. This is because **Logfire** doesn't support other exporters, so we assume that the environment variables are intended to be used by something else. Normally you don't need to worry about this, and you don't need to set these variables at all unless you want to prevent **Logfire** from setting up these exporters.

See the [OpenTelemetry documentation](https://opentelemetry-python.readthedocs.io/en/latest/exporter/otlp/otlp.html) for information about the other headers you can set, such as `OTEL_EXPORTER_OTLP_HEADERS`.

# Alternative clients

**Logfire** uses the OpenTelemetry standard. This means that you can configure standard OpenTelemetry SDKs in many languages to export to the **Logfire** backend, including those outside our [first-class supported languages](../../languages/). Depending on your SDK, you may need to set only these [environment variables](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/):

- `OTEL_EXPORTER_OTLP_ENDPOINT=https://logfire-us.pydantic.dev` for both traces and metrics, or:
  - `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://logfire-us.pydantic.dev/v1/traces` for just traces
  - `OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=https://logfire-us.pydantic.dev/v1/metrics` for just metrics
  - `OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://logfire-us.pydantic.dev/v1/logs` for just logs
- `OTEL_EXPORTER_OTLP_HEADERS='Authorization=your-write-token'` - see [Create Write Tokens](../create-write-tokens/) to obtain a write token and replace `your-write-token` with it.
- `OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf` to export in Protobuf format over HTTP (not gRPC). The **Logfire** backend supports both Protobuf and JSON, but only over HTTP for now. Some SDKs (such as Python) already use this value as the default so setting this isn't required, but other SDKs use `grpc` as the default.

Note

This page shows `https://logfire-us.pydantic.dev` as the base URL which is for the US [region](../../reference/data-regions/). If you are using the EU region, use `https://logfire-eu.pydantic.dev` instead.

## Example with Python

First, run these commands:

```sh
pip install opentelemetry-exporter-otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=https://logfire-us.pydantic.dev
export OTEL_EXPORTER_OTLP_HEADERS='Authorization=your-write-token'
```

Then run this script with `python`:

```python
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

exporter = OTLPSpanExporter()
span_processor = BatchSpanProcessor(exporter)
tracer_provider = TracerProvider()
tracer_provider.add_span_processor(span_processor)
tracer = tracer_provider.get_tracer('my_tracer')

tracer.start_span('Hello World').end()
```

Then navigate to the Live view for your project in your browser. You should see a trace with a single span named `Hello World`.

To configure the exporter without environment variables:

```python
exporter = OTLPSpanExporter(
    endpoint='https://logfire-us.pydantic.dev/v1/traces',
    headers={'Authorization': 'your-write-token'},
)
```

## Example with NodeJS

> See also our [JS/TS SDK](https://github.com/pydantic/logfire-js) which supports many JS environments, including NodeJS, web browsers, and Cloudflare Workers.

Create a `main.js` file containing the following:

main.js

```js
import {NodeSDK} from "@opentelemetry/sdk-node";
import {OTLPTraceExporter} from "@opentelemetry/exporter-trace-otlp-proto";
import {BatchSpanProcessor} from "@opentelemetry/sdk-trace-node";
import {trace} from "@opentelemetry/api";
import {Resource} from "@opentelemetry/resources";
import {ATTR_SERVICE_NAME} from "@opentelemetry/semantic-conventions";

const traceExporter = new OTLPTraceExporter();
const spanProcessor = new BatchSpanProcessor(traceExporter);
const resource = new Resource({[ATTR_SERVICE_NAME]: "my_service"});
const sdk = new NodeSDK({spanProcessor, resource});
sdk.start();

const tracer = trace.getTracer("my_tracer");
tracer.startSpan("Hello World").end();

sdk.shutdown().catch(console.error);
```

Then run these commands:

```sh
export OTEL_EXPORTER_OTLP_ENDPOINT=https://logfire-us.pydantic.dev
export OTEL_EXPORTER_OTLP_HEADERS='Authorization=your-write-token'

npm init es6 -y # creates package.json with type module
npm install @opentelemetry/sdk-node
node main.js
```

## Example with Rust

> See also our [Rust SDK](https://github.com/pydantic/logfire-rust) which provides a more streamlined developer experience for Rust applications.

First, set up a new Cargo project:

```sh
cargo new --bin otel-example && cd otel-example
export OTEL_EXPORTER_OTLP_ENDPOINT=https://logfire-us.pydantic.dev
export OTEL_EXPORTER_OTLP_HEADERS='Authorization=your-write-token'
```

Update the `Cargo.toml` and `main.rs` files with the following contents:

Cargo.toml

```toml
[package]
name = "otel-example"
version = "0.1.0"
edition = "2021"

[dependencies]
opentelemetry = { version = "*", default-features = false, features = ["trace"] }
# Note: `reqwest-rustls` feature is necessary else you'll have a cryptic failure to export;
# see https://github.com/open-telemetry/opentelemetry-rust/issues/2169
opentelemetry-otlp = { version = "*", default-features = false, features = ["trace", "http-proto", "reqwest-blocking-client", "reqwest-rustls"] }
```

src/main.rs

```rust
use opentelemetry::{
    global::ObjectSafeSpan,
    trace::{Tracer, TracerProvider},
};

fn main() -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let otlp_exporter = opentelemetry_otlp::new_exporter()
        .http()
        .with_protocol(opentelemetry_otlp::Protocol::HttpBinary)
        // If you don't want to export environment variables, you can also configure
        // programmatically like so:
        //
        // (You'll need to add `use opentelemetry_otlp::WithExportConfig;` to the top of the
        // file to access the `.with_endpoint` method.)
        //
        // .with_endpoint("https://logfire-us.pydantic.dev/v1/traces")
        // .with_headers({
        //     let mut headers = std::collections::HashMap::new();
        //     headers.insert(
        //         "Authorization".into(),
        //         "your-write-token".into(),
        //     );
        //     headers
        // })
        ;

    let tracer_provider = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(otlp_exporter)
        .install_simple()?;
    let tracer = tracer_provider.tracer("my_tracer");

    tracer.span_builder("Hello World").start(&tracer).end();

    Ok(())
}
```

Finally, use `cargo run` to execute.

## Example with Go

Create a file `main.go` containing the following:

```go
package main

import (
    "context"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
    "go.opentelemetry.io/otel/sdk/trace"
)

func main() {
    ctx := context.Background()
    traceExporter, _ := otlptracehttp.New(ctx)
    batchSpanProcessor := trace.NewBatchSpanProcessor(traceExporter)
    tracerProvider := trace.NewTracerProvider(trace.WithSpanProcessor(batchSpanProcessor))
    tracer := tracerProvider.Tracer("my_tracer")

    ctx, span := tracer.Start(ctx, "Hello World")
    span.End()

    tracerProvider.Shutdown(ctx)
}
```

Then run these commands:

```sh
export OTEL_EXPORTER_OTLP_ENDPOINT=https://logfire-us.pydantic.dev
export OTEL_EXPORTER_OTLP_HEADERS='Authorization=your-write-token'

# Optional, but otherwise you will see the service name set to `unknown_service:otel_example`
export OTEL_RESOURCE_ATTRIBUTES="service.name=my_service"

go mod init otel_example
go mod tidy
go run .
```

# Collecting Metrics from Cloud Providers

Cloud metrics provide valuable insights into the performance, health, and usage of your cloud infrastructure. By collecting metrics from your cloud provider and centralizing them in Logfire, you can create a single pane of glass for monitoring your entire infrastructure stack.

Key benefits of collecting cloud metrics include:

- **Single pane of glass visibility**: Correlate metrics across different cloud providers and services
- **Centralized alerting**: Set up consistent alerting rules across your entire infrastructure
- **Cost optimization**: Identify resource usage patterns and optimize spending
- **Performance monitoring**: Track application performance alongside infrastructure metrics

## 1. Why Use the OpenTelemetry Collector?

Rather than you giving us access to your cloud provider directly we recommend using the [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) to collect metrics from your cloud provider. The OpenTelemetry Collector is a vendor-agnostic service that can collect, process, and export telemetry data (metrics, logs, traces) from various sources. The advantages of this approach include:

- **Security**: You maintain control over your cloud credentials and don't need to share them with external services
- **Data governance**: Filter sensitive or unnecessary metrics before they leave your environment
- **Cost control**: Reduce data transfer costs by filtering and sampling metrics locally
- **Flexibility**: Transform, enrich, or aggregate metrics before sending them to Logfire
- **Vendor lock in**: Send the same metrics to multiple monitoring systems if needed

For general information about setting up and configuring the OpenTelemetry Collector, see our [OpenTelemetry Collector guide](../otel-collector/otel-collector-overview/).

One important consideration before you embark on this guide is what your overall data flow is going to be. For example, you don't want to export your application metrics to Logfire and Google Cloud Monitoring and *also* export your Google Cloud Monitoring metrics to Logfire, you'll end up with duplicate metrics!

We recommend you export all application metrics to Logfire directly and then use the OpenTelemetry Collector to collect metrics from your cloud provider that are *not* already being exported to Logfire.

## 2. Collecting Metrics from Google Cloud Platform (GCP)

The [Google Cloud Monitoring receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudmonitoringreceiver) allows you to collect metrics from Google Cloud Monitoring (formerly Stackdriver) and forward them to Logfire.

### 2.1 Prerequisites

1. A GCP project with the Cloud Monitoring API enabled
1. Service account credentials with appropriate IAM permissions (see IAM Setup below)
1. OpenTelemetry Collector with the `googlecloudmonitoring` receiver

### 2.2 Enabling the Cloud Monitoring API

To enable the Cloud Monitoring API for your GCP project follow the steps listed in [the official documentation](https://cloud.google.com/monitoring/api/enable-api).

### 2.3 IAM Setup

To collect metrics from Google Cloud Monitoring, you need to create a service account with the appropriate permissions:

#### 2.3.1 Required Permissions

The service account needs the following specific roles:

- `roles/monitoring.viewer`: grants read-only access to Monitoring in the Google Cloud console and the Cloud Monitoring API.

See the [official documentation](https://cloud.google.com/monitoring/access-control) for a complete list of permissions required for the Monitoring API.

#### 2.3.2 Creating a Service Account

To create a service account you can use the Google Cloud CLI or the GCP Console. Here are the steps using the CLI:

```bash
gcloud iam service-accounts create logfire-metrics-collector \
    --display-name="Logfire Metrics Collector" \
    --description="Service account for collecting metrics to send to Logfire"
```

Grant the service account the necessary permissions:

```bash
export PROJECT_ID="your-gcp-project-id"
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:logfire-metrics-collector@$PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/monitoring.viewer"
```

### 2.4 Configuration

Create a collector configuration file with the Google Cloud Monitoring receiver:

gcp-metrics-collector.yaml

```yaml
receivers:
  googlecloudmonitoring:
    # Your GCP project ID
    project_id: "${env:PROJECT_ID}"
    # Collection interval
    collection_interval: 60s
    # Example of metric names to collect
    # See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudmonitoringreceiver#configuration
    metrics_list:
      - metric_name: "cloudsql.googleapis.com/database/cpu/utilization"
      - metric_name: "kubernetes.io/container/memory/limit_utilization"
      # This will collect the CPU usage for the container we are deploying the collector itself in!
      - metric_name: "run.googleapis.com/container/cpu/usage"

exporters:
  debug:
  otlphttp:
    # Configure the US / EU endpoint for Logfire.
    # - US: https://logfire-us.pydantic.dev
    # - EU: https://logfire-eu.pydantic.dev
    endpoint: "https://logfire-us.pydantic.dev"
    headers:
      Authorization: "Bearer ${env:LOGFIRE_TOKEN}"

extensions:
  health_check:
    # The PORT env var is set by CloudRun
    endpoint: "0.0.0.0:${env:PORT:-13133}"

service:
  pipelines:
    metrics:
      receivers: [googlecloudmonitoring]
      exporters: [otlphttp, debug]
  extensions: [health_check]
```

### 2.5 Authentication

Authentication to Google Cloud via [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/application-default-credentials). If you are running on Kubernetes you will have to set up [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) to allow the OpenTelemetry Collector to access Google Cloud resources. If you are running on Cloud Run or other GCP services, the default service account will be used automatically. You can either give the default service account the necessary permissions (in which case you can skip creating the service account above) or create a new service account and configure the workload running the OpenTelemetry Collector to use this service account. The latter is advisable from a security perspective, as it allows you to limit the permissions of the service account to only what is necessary for the OpenTelemetry Collector.

Authentication to Logfire must happen via a write token. It is recommended that you store the write token as a secret (e.g. in Kubernetes secrets) and reference it in the collector configuration file as an environment variable to avoid hardcoding sensitive information in the configuration file.

### 2.6 Example deployment using Cloud Run

This section shows how to deploy the OpenTelemetry Collector to Google Cloud Run using the service account created in section 2.3.

#### 2.6.1 Create a Dockerfile

First, create a Dockerfile that uses the official OpenTelemetry Collector contrib image and copies your configuration:

Dockerfile

```dockerfile
# Update the base image to the latest version as needed
# It's good practice to use a specific version tag for stability
FROM otel/opentelemetry-collector-contrib:0.128.0

# Copy the collector configuration created previously to the default location
COPY gcp-metrics-collector.yaml /etc/otelcol-contrib/config.yaml
```

#### 2.6.2 Create a secret with your Logfire token

To securely store your Logfire write token, create a secret in Google Secret Manager.

First [enable the Secrets Manager API](https://cloud.google.com/secret-manager/docs/configuring-secret-manager) for your project. Using the Google Cloud CLI:

```bash
# Enable the Secret Manager API
gcloud services enable secretmanager.googleapis.com
```

Then, create a secret and grant the service account access to it:

```bash
# Set your project ID
export PROJECT_ID="your-gcp-project-id"
export LOGFIRE_TOKEN="your-logfire-write-token"
# Create the secret
echo -n "$LOGFIRE_TOKEN" | gcloud secrets create logfire-token --data-file=-
# Grant the service account access to the secret
gcloud secrets add-iam-policy-binding logfire-token \
  --member="serviceAccount:logfire-metrics-collector@$PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

#### 2.6.2 Build and push the container image

Build and push your container image to Google Container Registry or Artifact Registry.

With the following project structure:

```text
.
├── Dockerfile
└── gcp-metrics-collector.yaml
```

You can run:

```bash
# Set your project ID
export PROJECT_ID="your-gcp-project-id"

# Set the port for health checks
# Set no CPU throttling so that the collector runs even though it is not receiving external HTTP requests
# Do not allow any external HTTP traffic to the service
# Set the service to use the service account created earlier
# Inject the project ID as an environment variable
# Set the minimum number of instances to 1 so that the collector is always running
# Inject the Logfire token secret as an environment variable

gcloud run deploy otel-collector-gcp-metrics \
--source . \
--project $PROJECT_ID \
--port 13133 \
--no-allow-unauthenticated \
--service-account logfire-metrics-collector@$PROJECT_ID.iam.gserviceaccount.com \
--set-env-vars PROJECT_ID=$PROJECT_ID \
--no-cpu-throttling \
--min-instances 1 \
--update-secrets=LOGFIRE_TOKEN=logfire-token:latest
```

Once the deployment is complete you should be able to run the following query in Logfire to verify metrics are being received:

```sql
SELECT metric_name, count(*) AS metric_count
FROM metrics
WHERE metric_name IN ('cloudsql.googleapis.com/database/cpu/utilization', 'kubernetes.io/container/memory/limit_utilization')
GROUP BY metric_name;
```

#### 2.6.4 Configuring scaling

Depending on the amount of metrics data points you are collecting you may need to do more advanced configuration of the OpenTelemetry Collector to handle the load. For example, you may want to configure the `batch` processor to batch metrics before sending them to Logfire, or use the `memory_limiter` processor to limit memory usage. You also may need to tweak the resources allocated to the Cloud Run service to ensure it can handle the load.

## 3. Collecting Metrics from Amazon Web Services (AWS)

The [AWS CloudWatch metrics receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/awscloudwatchmetricsreceiver) allows you to collect metrics from Amazon CloudWatch and forward them to Logfire.

### 3.1 Prerequisites

1. An AWS account with CloudWatch metrics enabled
1. IAM credentials with appropriate permissions (see IAM Setup below)
1. OpenTelemetry Collector with the `awscloudwatchmetrics` receiver

### 3.2 IAM Setup

To collect metrics from AWS CloudWatch, you need to configure IAM credentials with the appropriate permissions:

#### 3.2.1 Required Permissions

The IAM role or user needs the following CloudWatch permissions:

- `cloudwatch:GetMetricData`: Retrieve metric data points
- `cloudwatch:GetMetricStatistics`: Get aggregated metric statistics
- `cloudwatch:ListMetrics`: List available metrics

For ECS-specific metrics, you may also need EC2 permissions:

- `ec2:DescribeTags`: Get resource tags
- `ec2:DescribeInstances`: Get instance information
- `ec2:DescribeRegions`: List available regions

### 3.3 Configuration

Create a collector configuration file with the AWS ECS container metrics receiver:

aws-metrics-collector.yaml

```yaml
receivers:
  # Collect ECS container metrics directly from the ECS task metadata endpoint
  awsecscontainermetrics:
    # Collection interval
    collection_interval: 60s

exporters:
  debug:
  otlphttp:
    # Configure the US / EU endpoint for Logfire.
    # - US: https://logfire-us.pydantic.dev
    # - EU: https://logfire-eu.pydantic.dev
    endpoint: "https://logfire-us.pydantic.dev"
    headers:
      Authorization: "Bearer ${env:LOGFIRE_TOKEN}"

extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

service:
  pipelines:
    metrics:
      receivers: [awsecscontainermetrics]
      exporters: [otlphttp, debug]
  extensions: [health_check]
```

This configuration collects metrics directly from the ECS task metadata endpoint including:

- Container CPU utilization
- Container memory utilization
- Container network I/O metrics
- Container storage I/O metrics

**For CloudWatch Metrics**: If you need to collect metrics from other AWS services (RDS, ALB, etc.), use the [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/docs/introduction) collector image `public.ecr.aws/aws-observability/aws-otel-collector` which includes the `awscloudwatchmetrics` receiver.

### 3.4 Authentication

The AWS ECS container metrics receiver collects metrics directly from the ECS task metadata endpoint, so **no AWS credentials or IAM permissions are required** for the metrics collection itself.

However, you still need:

1. **ECS Task Execution Role permissions** for:
   - Pulling container images from ECR
   - Writing logs to CloudWatch Logs
   - Reading secrets from AWS Secrets Manager
1. **Logfire authentication** via a write token. Store the write token as a secret in AWS Secrets Manager and reference it as an environment variable.

### 3.5 Example deployment using Amazon ECS

This section shows how to deploy the OpenTelemetry Collector to Amazon ECS using an IAM role for tasks.

#### 3.5.1 Create ECS Task Role

Since the ECS container metrics receiver doesn't require AWS API access, we only need a basic ECS task execution role:

```bash
# Create the ECS task trust policy file
cat > ecs-task-trust-policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

# Create the Secrets Manager policy for accessing the Logfire token
cat > secretsmanager-policy.json << 'EOF'
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "secretsmanager:GetSecretValue"
            ],
            "Resource": "arn:aws:secretsmanager:*:*:secret:logfire-token*"
        }
    ]
}
EOF

# Get your AWS account ID
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# Create the IAM role
aws iam create-role \
    --role-name LogfireMetricsCollectorRole \
    --assume-role-policy-document file://ecs-task-trust-policy.json \
    --description "ECS task role for Logfire metrics collector"

# Attach ECS task execution permissions
aws iam attach-role-policy \
    --role-name LogfireMetricsCollectorRole \
    --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy

# Grant access to read the Logfire token secret
aws iam put-role-policy \
    --role-name LogfireMetricsCollectorRole \
    --policy-name LogfireSecretsAccess \
    --policy-document file://secretsmanager-policy.json
```

#### 3.5.2 Store Logfire token in AWS Secrets Manager

Store your Logfire write token as an ECS secret using AWS Secrets Manager. This follows the [ECS best practices for specifying sensitive data](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html):

```bash
# Create the secret for Logfire token
aws secretsmanager create-secret \
    --name logfire-token \
    --description "Logfire write token for metrics collection" \
    --secret-string "your-logfire-write-token-here"
```

#### 3.5.3 Create a Dockerfile

Create a Dockerfile that uses the official OpenTelemetry Collector contrib image:

Dockerfile

```dockerfile
# Update the base image to the latest version as needed
FROM otel/opentelemetry-collector-contrib:0.128.0

# Copy the collector configuration to the default location
COPY aws-metrics-collector.yaml /etc/otelcol-contrib/config.yaml
```

#### 3.5.4 Create an ECS Task Definition

Create an ECS task definition that uses the IAM role. First, get your AWS account ID and create the task definition:

```bash
# Get your AWS account ID (reuse from earlier or get it again)
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export AWS_REGION="us-east-1"  # Change to your preferred region

# Create the task definition using the account ID
cat > task-definition.json << EOF
{
  "family": "logfire-metrics-collector",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "256",
  "memory": "512",
  "executionRoleArn": "arn:aws:iam::${AWS_ACCOUNT_ID}:role/LogfireMetricsCollectorRole",
  "taskRoleArn": "arn:aws:iam::${AWS_ACCOUNT_ID}:role/LogfireMetricsCollectorRole",
  "containerDefinitions": [
    {
      "name": "otel-collector",
      "image": "${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/logfire-metrics-collector:latest",
      "essential": true,
      "portMappings": [
        {
          "containerPort": 13133,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "AWS_REGION",
          "value": "${AWS_REGION}"
        }
      ],
      "secrets": [
        {
          "name": "LOGFIRE_TOKEN",
          "valueFrom": "arn:aws:secretsmanager:${AWS_REGION}:${AWS_ACCOUNT_ID}:secret:logfire-token-XXXXXX"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/logfire-metrics-collector",
          "awslogs-region": "${AWS_REGION}",
          "awslogs-stream-prefix": "ecs"
        }
      }
    }
  ]
}
EOF
```

**Important**: Replace `XXXXXX` in the secret ARN with the actual suffix generated by AWS Secrets Manager. You can get the complete ARN by running:

```bash
aws secretsmanager describe-secret --secret-id logfire-token --query 'ARN' --output text
```

**Note**: This task definition uses a single IAM role (`LogfireMetricsCollectorRole`) for both execution and task permissions, which simplifies the setup while providing all necessary permissions:

- **ECS Task Execution**: Pull images from ECR, access secrets, write logs
- **CloudWatch Access**: Collect metrics from AWS CloudWatch APIs
- **Secrets Access**: Retrieve Logfire token from AWS Secrets Manager

The next step will show you how to create the ECR repository and build the image.

#### 3.5.5 Create ECR Repository and Build Container Image

Create an ECR repository and build your container image with the OpenTelemetry Collector configuration:

```bash
# Create an ECR repository
aws ecr create-repository \
    --repository-name logfire-metrics-collector \
    --region ${AWS_REGION}

# Get the ECR repository URI
export ECR_REPOSITORY_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/logfire-metrics-collector"

# Create the project structure
mkdir -p logfire-otel-collector
cd logfire-otel-collector

# Create the OpenTelemetry Collector configuration (same as section 3.3)
cat > aws-metrics-collector.yaml << 'EOF'
receivers:
  awsecscontainermetrics:
    collection_interval: 60s

exporters:
  debug:
  otlphttp:
    endpoint: "https://logfire-us.pydantic.dev"
    headers:
      Authorization: "Bearer ${env:LOGFIRE_TOKEN}"

extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

service:
  pipelines:
    metrics:
      receivers: [awsecscontainermetrics]
      exporters: [otlphttp, debug]
  extensions: [health_check]
EOF

# Create the Dockerfile
cat > Dockerfile << 'EOF'
FROM otel/opentelemetry-collector-contrib:0.128.0
COPY aws-metrics-collector.yaml /etc/otelcol-contrib/config.yaml
EOF

# Build and push the container image
aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${ECR_REPOSITORY_URI}
docker build --platform linux/amd64 -t logfire-metrics-collector .
docker tag logfire-metrics-collector:latest ${ECR_REPOSITORY_URI}:latest
docker push ${ECR_REPOSITORY_URI}:latest
```

#### 3.5.6 Deploy to ECS

**Prerequisites**: Before deploying, ensure you have the following AWS infrastructure:

- **ECS Cluster**: [Creating an Amazon ECS cluster](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-cluster.html)
- **VPC and Subnets**: Use existing VPC/subnets or [create new ones](https://docs.aws.amazon.com/vpc/latest/userguide/create-vpc.html)

For this example, we'll use the default VPC and public subnets for simplicity:

```bash
# Get your default VPC and subnets
export VPC_ID=$(aws ec2 describe-vpcs --filters "Name=is-default,Values=true" --query 'Vpcs[0].VpcId' --output text)
export SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0:2].SubnetId' --output text | tr '\t' ',')

# Create ECS cluster
aws ecs create-cluster --cluster-name logfire-metrics-cluster

# Register the task definition
aws ecs register-task-definition --cli-input-json file://task-definition.json

# Create an ECS service
aws ecs create-service \
    --cluster logfire-metrics-cluster \
    --service-name logfire-metrics-collector \
    --task-definition logfire-metrics-collector:1 \
    --desired-count 1 \
    --launch-type FARGATE \
    --network-configuration "awsvpcConfiguration={subnets=[${SUBNET_IDS}],assignPublicIp=ENABLED}"
```

**Important Security Considerations:**

⚠️ **This example uses public subnets for simplicity**. For production deployments, you should:

- **Use private subnets** with NAT Gateway or VPC endpoints for outbound internet access
- **Disable public IP assignment** (`assignPublicIp=DISABLED`)
- **Create restrictive security groups** that only allow outbound HTTPS (port 443)
- **Use VPC endpoints** for AWS services (ECR, Secrets Manager, CloudWatch) to avoid internet routing

For production networking setup, see:

- [VPC and subnets](https://docs.aws.amazon.com/vpc/latest/userguide/create-vpc.html)
- [NAT Gateway](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html)
- [VPC endpoints](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html)

Once the deployment is complete, you should be able to run the following query in Logfire to verify metrics are being received:

```sql
-- For ECS container metrics
SELECT metric_name, count(*) AS metric_count
FROM metrics
WHERE metric_name IN ('ecs.task.memory.utilized', 'ecs.task.cpu.utilized', 'ecs.task.network.rate.rx', 'ecs.task.network.rate.tx')
GROUP BY metric_name;
```

#### 3.5.7 Cost Considerations

**ECS Container Metrics**: The ECS container metrics receiver collects data directly from the ECS task metadata endpoint at **no additional cost**. This is much more cost-effective than using CloudWatch APIs.

**CloudWatch Metrics**: If you use the AWS Distro for OpenTelemetry (ADOT) to collect CloudWatch metrics, be aware that the `GetMetricData` API is **not included in the AWS free tier**. Monitor your CloudWatch API usage and costs, especially when collecting metrics at high frequencies or from many resources.

# How to Convert a Personal Account to an Organization

Logfire allows you to convert your personal account into an organization, making it easier to collaborate with a team and manage projects at scale. This is also handy for users upgrading to Pro, who might want to move projects under a corporate organization name.

This guide walks you through the conversion process step by step.

______________________________________________________________________

## 1. Open Account Settings

Navigate to your account settings. On the page, you'll see an option to **Convert to organization**.

______________________________________________________________________

## 2. Start the Conversion

Click **Convert to org**. A modal will appear, outlining the main points of the conversion:

- All existing **projects, members, alerts, dashboards, and settings** will be moved to the new organization.
- **Write tokens** will continue to work; you do not need to change any ingest URLs.
- You'll define your new organization's **handle** and **display name**.
- You can optionally edit the username and display name for your new personal account.

Click **Acknowledge & continue** to proceed.

______________________________________________________________________

## 3. Set Up Your Organization

In the next modal, you can:

- Upload an **organization avatar**.
- Specify the **organization handle** (used in URLs).
- Set the **organization display name**.

On the right, you'll see a summary of the migration:

- All your projects and members will be moved to the new organization.
- The project URLs will change from: `https://logfire-eu.pydantic.dev/your-username/project-name` to `https://logfire-eu.pydantic.dev/your-org-handle/project-name`.

______________________________________________________________________

## 4. Confirm New Personal Account

After setting up the organization, you'll be prompted to create a new (empty) personal account with the same name as before. You can confirm and complete the conversion, or go back if you wish to make changes.

______________________________________________________________________

## 5. Complete the Conversion

Click **Confirm & convert**. The conversion process will complete, and you'll be redirected to your new organization's projects page.

______________________________________________________________________

## Summary

- All your data, projects, and settings are preserved during the migration.
- Only the URL changes to reflect the new organization handle.
- Your new personal account will be empty, ready for individual use if needed.

______________________________________________________________________

**See also:** [Organization Structure Reference](../../reference/organization-structure/)

To send data to **Logfire**, you need to create a write token. A write token is a unique identifier that allows you to send data to a specific **Logfire** project. If you set up Logfire according to the [getting started guide](../../), you already have a write token locally tied to the project you created. But if you want to configure other computers to write to that project, for example in a deployed application, you need to create a new write token.

You can create a write token by following these steps:

1. Open the **Logfire** web interface at [logfire.pydantic.dev](https://logfire.pydantic.dev).
1. Select your project from the **Projects** section on the left hand side of the page.
1. Click on the ⚙️ **Settings** tab in the top right corner of the page.
1. Select the **Write tokens** tab from the left hand menu.
1. Click on the **New write token** button.

After creating the write token, you'll see a dialog with the token value. **Copy this value and store it securely, it will not be shown again**.

Now you can use this write token to send data to your **Logfire** project from any computer or application.

We recommend you inject your write token via environment variables in your deployed application. Set the token as the value for the environment variable `LOGFIRE_TOKEN` and logfire will automatically use it to send data to your project.

## Setting `send_to_logfire='if-token-present'`

You may want to not send data to logfire during local development, but still have the option to send it in production without changing your code. To do this we provide the parameter `send_to_logfire='if-token-present'` in the `logfire.configure()` function. If you set it to `'if-token-present'`, logfire will only send data to logfire if a write token is present in the environment variable `LOGFIRE_TOKEN` or there is a token saved locally. If you run tests in CI no data will be sent.

You can also set the environment variable `LOGFIRE_SEND_TO_LOGFIRE` to configure this option. For example, you can set it to `LOGFIRE_SEND_TO_LOGFIRE=true` in your deployed application and `LOGFIRE_SEND_TO_LOGFIRE=false` in your tests setup.

For now, **Logfire** doesn't have a built-in way to detect if a service is down, in the sense that we don't ping services via HTTP or any other protocol to check if they are up or down.

For now we don't have it, but...

If you would like to see this feature in **Logfire**, [let us know](../../help/)!

It's useful for us to understand the use cases and requirements for this feature.

However, you can create alerts to notify you when a log message is not received for a certain amount of time. This can be used to detect if a service is down.

Let's say you have a [FastAPI application](../../integrations/web-frameworks/fastapi/) that has a health check endpoint at `/health`.

```py
import logfire
from fastapi import FastAPI

logfire.configure(service_name="backend")
app = FastAPI()
logfire.instrument_fastapi(app)

@app.get("/health")
async def health():
    return {"status": "ok"}
```

You probably have this endpoint because you have a mechanism that restarts the service if it's down. In this case, you can use **Logfire** to send you an alert if the health check endpoint is not called for a certain amount of time.

## Create the Alert

Go to [your alerts tab](https://logfire.pydantic.dev/-/redirect/latest-project/alerts/) and click on "New Alert". Then add the following query to the alert:

```sql
SELECT
    CASE
        WHEN COUNT(*) = 0 THEN 'backend is down'
        ELSE 'backend is up'
    END AS message
FROM
    records
WHERE
    service_name = 'backend' and span_name = 'GET /health';
```

This query will return `backend is down` if the `/health` endpoint on the `'backend'` service is not called.

On the "Alert Parameters", we want to be notified as soon as possible, so we should execute the query `"every minute"`, include rows from `"the last minute"`, and notify us if `"the query's results change"`.

Then you need to set up a channel to send this notification, which can be a Slack channel or a webhook. See more about it on the [alerts documentation](../../guides/web-ui/alerts/).

**Logfire** builds on OpenTelemetry, which keeps track of *context* to determine the parent trace/span of a new span/log and whether it should be included by sampling. *Context propagation* is when this context is serialized and sent to another process, so that tracing can be distributed across services while allowing the full tree of spans to be cleanly reconstructed and viewed together.

## Manual Context Propagation

**Logfire** provides a thin wrapper around the OpenTelemetry context propagation API to make manual distributed tracing easier. You shouldn't usually need to do this yourself, but it demonstrates the concept nicely. Here's an example:

```python
import logfire

logfire.configure()

with logfire.span('parent'):
    ctx = logfire.get_context()

print(ctx)

# Attach the context in another execution environment
with logfire.attach_context(ctx):
    logfire.info('child')  # This log will be a child of the parent span.
```

`ctx` will look something like this:

```python
{'traceparent': '00-d1b9e555b056907ee20b0daebf62282c-7dcd821387246e1c-01'}
```

This contains 4 fields:

- `00` is a version number which you can ignore.
- `d1b9e555b056907ee20b0daebf62282c` is the `trace_id`.
- `7dcd821387246e1c` is the `span_id` of the parent span, i.e. the `parent_span_id` of the child log.
- `01` is the `trace_flags` field and indicates that the trace should be included by sampling.

See the [API reference](../../reference/api/propagate/) for more details about these functions.

## Integrations

OpenTelemetry instrumentation libraries (which **Logfire** uses for its integrations) handle context propagation automatically, even across different programming languages. For example:

- Instrumented HTTP clients such as [`requests`](../../integrations/http-clients/requests/) and [`httpx`](../../integrations/http-clients/httpx/) will automatically set the `traceparent` header when making requests.
- Instrumented web servers such as [`flask`](../../integrations/web-frameworks/flask/) and [`fastapi`](../../integrations/web-frameworks/fastapi/) will automatically extract the `traceparent` header and use it to set the context for server spans.
- The [`celery` integration](../../integrations/event-streams/celery/) will automatically propagate the context to child tasks.

## Thread and Pool executors

**Logfire** automatically patches ThreadPoolExecutor and ProcessPoolExecutor to propagate context to child threads and processes. This means that logs and spans created in child threads and processes will be correctly associated with the parent span. Here's an example to demonstrate:

```python
import logfire
from concurrent.futures import ThreadPoolExecutor

logfire.configure()


@logfire.instrument("Doubling {x}")
def double(x: int):
    return x * 2


with logfire.span("Doubling everything") as span:
    executor = ThreadPoolExecutor()
    results = list(executor.map(double, range(3)))
    span.set_attribute("results", results)
```

## Unintentional Distributed Tracing

Because instrumented web servers automatically extract the `traceparent` header by default, your spans can accidentally pick up the wrong context from an externally instrumented client, or from your cloud provider such as Google Cloud Run. This can lead to:

- Spans missing their parent.
- Spans being mysteriously grouped together.
- Spans missing entirely because the original trace was excluded by sampling.

By default, **Logfire** warns you when trace context is extracted, e.g. when server instrumentation finds a `traceparent` header. You can deal with this by setting the distributed_tracing argument of logfire.configure() or by setting the `LOGFIRE_DISTRIBUTED_TRACING` environment variable:

- Setting to `False` will prevent trace context from being extracted. This is recommended for web services exposed to the public internet. You can still attach/inject context to propagate to other services and create distributed traces with the web service as the root.
- Setting to `True` implies that the context propagation is intentional and will silence the warning.

The `distributed_tracing` configuration (including the warning by default) only applies when the raw OpenTelemetry API is used to extract context, as this is typically done by third-party libraries. By default, logfire.attach_context assumes that context propagation is intended by the application. If you are writing a library, use `attach_context(context, third_party=True)` to respect the `distributed_tracing` configuration.

As developers, we find ourselves working on different environments for a project: local, production, sometimes staging, and depending on your company deployment strategy... You can have even more! 😅

With **Logfire** you can distinguish which environment you are sending data to. You just need to set the `environment` parameter in logfire.configure().

main.py

```py
import logfire

logfire.configure(environment='local')  # (1)!
```

1. Usually you would retrieve the environment information from an environment variable.

Under the hood, this sets the OpenTelemetry resource attribute [`deployment.environment.name`](https://opentelemetry.io/docs/specs/semconv/resource/deployment-environment/). Note that you can also set this via the `LOGFIRE_ENVIRONMENT` environment variable.

#### Setting environments in other languages

If you are using languages other than Python, you can set the environment like this: `OTEL_RESOURCE_ATTRIBUTES="deployment.environment.name=prod"`

______________________________________________________________________

Once set, you will see your environment in the Logfire UI `all envs` dropdown, which is present on the [Live View](../../guides/web-ui/live/), [Dashboards](../../guides/web-ui/dashboards/) and [Explore](../../guides/web-ui/explore/) pages:

Info

When using an environment for the first time, it may take a **few minutes** for the environment to appear in the UI.

Note that by default there are system generated environments:

- `all envs`: Searches will include everything, including spans that had no environment set.
- `not specified`: Searches will *only* include spans that had no environment set.

So `not specified` is a subset of `all envs`.

Any environments you create via the SDK will appear below the system generated environments. When you select an environment, all subsequent queries (e.g. on live view, dashboards or explore) will filter by that environment.

## Can I create an environment in the UI?

No, you cannot create or delete set environments via the UI, instead use the SDK.

## How do I delete an environment?

Once an environment has been configured and received by logfire, technically it’s available for the length of the data retention period while that environment exists in the data. You can however add new ones, and change the configuration of which data is assigned to which environment name.

## Should I use environments or projects?

Environments are more lightweight than projects. Projects give you the ability to assign specific user groups and permissions levels (see this [organization structure diagram](../../reference/organization-structure/) for details). So if you need to allow different team members to view dev vs. prod traces, then projects would be a better fit.

We support linking to the source code on GitHub, GitLab, and any other VCS provider that uses the same URL format.

## Usage

Here's an example:

```python
import logfire

logfire.configure(
    code_source=logfire.CodeSource(
        repository='https://github.com/pydantic/logfire',  #(1)!
        revision='<hash of commit used on release>',  #(2)!
        root_path='path/within/repo',  #(3)!
    )
)
```

1. The URL of the repository e.g. `https://github.com/pydantic/logfire`.
1. The specific branch, tag, or commit hash to link to e.g. `main`.
1. The path from the root of the repository to the current working directory of the process. If your code is in a subdirectory of your repo, you can specify it here. Otherwise you can probably omit this.

You can learn more in our logfire.CodeSource API reference.

## Alternative Configuration

For other OpenTelemetry SDKs, you can configure these settings using resource attributes, e.g. by setting the [`OTEL_RESOURCE_ATTRIBUTES`](https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/#general-sdk-configuration) environment variable:

```text
OTEL_RESOURCE_ATTRIBUTES=vcs.repository.url.full=https://github.com/pydantic/platform
OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES},vcs.repository.ref.revision=main
OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES},vcs.root.path=path/within/repo
```

# Logfire MCP Server

An [MCP (Model Context Protocol) server](https://modelcontextprotocol.io/introduction) that provides access to OpenTelemetry traces and metrics through Logfire. This server enables LLMs to query your application's telemetry data, analyze distributed traces, and perform custom queries using **Logfire**'s OpenTelemetry-native API.

You can check the [Logfire MCP server](https://github.com/pydantic/logfire-mcp) repository for more information.

## Installation

The MCP server is a CLI tool that you can run from the command line.

You'll need a read token to use the MCP server. See [Create Read Token](../query-api/#how-to-create-a-read-token) for more information.

You can then start the MCP server with the following command:

```bash
LOGFIRE_READ_TOKEN=<your-token> uvx logfire-mcp@latest
```

Note

The `uvx` command will download the PyPI package [`logfire-mcp`](https://pypi.org/project/logfire-mcp/), and run the `logfire-mcp` command.

### Configuration

The way to configure the MCP server depends on the software you're using.

Note

If you are in the EU region, you need to set the `LOGFIRE_BASE_URL` environment variable to `https://api-eu.pydantic.dev`. You can also use the `--base-url` flag to set the base URL.

#### Cursor

[Cursor](https://www.cursor.com/) is a popular IDE that supports MCP servers. You can configure it by creating a `.cursor/mcp.json` file in your project root:

```json
{
  "mcpServers": {
    "logfire": {
      "command": "uvx",
      "args": ["logfire-mcp", "--read-token=YOUR-TOKEN"],
    }
  }
}
```

Note

You need to pass the token via the `--read-token` flag, because Cursor doesn't support the `env` field in the MCP configuration.

For more detailed information, you can check the [Cursor documentation](https://docs.cursor.com/context/model-context-protocol).

#### Claude Desktop

[Claude Desktop](https://claude.ai/download) is a desktop application for the popular LLM Claude.

You can configure it to use the MCP server by adding the following configuration to the `~/claude_desktop_config.json` file:

```json
{
  "mcpServers": {
    "logfire": {
      "command": "uvx",
      "args": [
        "logfire-mcp",
      ],
      "env": {
        "LOGFIRE_READ_TOKEN": "your_token"
      }
    }
  }
}
```

Check out the [MCP quickstart](https://modelcontextprotocol.io/quickstart/user) for more information.

#### Claude Code

[Claude Code](https://claude.ai/code) is a coding tool that is used via CLI.

You can run the following command to add the Logfire MCP server to your Claude Code:

```bash
claude mcp add logfire -e LOGFIRE_READ_TOKEN="your-token" -- uvx logfire-mcp@latest
```

#### Cline

[Cline](https://docs.cline.bot/) is a popular chatbot platform that supports MCP servers.

You can configure it to use the MCP server by adding the following configuration to the `cline_mcp_settings.json` file:

```json
{
  "mcpServers": {
    "logfire": {
      "command": "uvx",
      "args": [
        "logfire-mcp",
      ],
      "env": {
        "LOGFIRE_READ_TOKEN": "your_token"
      },
      "disabled": false,
      "autoApprove": []
    }
  }
}
```

### Tools

There are four tools available in the MCP server:

1. `find_exceptions(age: int)` - Get exception counts from traces grouped by file.

   Required arguments:

   - `age`: Number of minutes to look back (e.g., 30 for last 30 minutes, max 7 days)

1. `find_exceptions_in_file(filepath: str, age: int)` - Get detailed trace information about exceptions in a specific file.

   Required arguments:

   - `filepath`: Path to the file to analyze
   - `age`: Number of minutes to look back (max 7 days)

1. `arbitrary_query(query: str, age: int)` - Run custom SQL queries on your OpenTelemetry traces and metrics.

   Required arguments:

   - `query`: SQL query to execute
   - `age`: Number of minutes to look back (max 7 days)

1. `get_logfire_records_schema()` - Get the OpenTelemetry schema to help with custom queries.

**Logfire** provides a web API for programmatically running arbitrary SQL queries against the data in your **Logfire** projects. This API can be used to retrieve data for export, analysis, or integration with other tools, allowing you to leverage your data in a variety of ways.

The API is available at `https://logfire-api.pydantic.dev/v1/query` and requires a **read token** for authentication. Read tokens can be generated from the Logfire web interface and provide secure access to your data.

The API can return data in various formats, including JSON, Apache Arrow, and CSV, to suit your needs. See [here](#additional-configuration) for more details about the available response formats.

## How to Create a Read Token

If you've set up Logfire following the [getting started guide](../../), you can generate read tokens either from the Logfire web interface or via the CLI.

### Via Web Interface

To create a read token using the web interface:

1. Open the **Logfire** web interface at [logfire.pydantic.dev](https://logfire.pydantic.dev).
1. Select your project from the **Projects** section on the left-hand side of the page.
1. Click on the ⚙️ **Settings** tab in the top right corner of the page.
1. Select the **Read tokens** tab from the left-hand menu.
1. Click on the **Create read token** button.

After creating the read token, you'll see a dialog with the token value. **Copy this value and store it securely, it will not be shown again.**

### Via CLI

You can also create read tokens programmatically using the Logfire CLI:

```bash
logfire read-tokens --project <organization>/<project> create
```

This command will output the read token directly to stdout, making it convenient for use in scripts.

## Using the Read Clients

While you can [make direct HTTP requests](#making-direct-http-requests) to Logfire's querying API, we provide Python clients to simplify the process of interacting with the API from Python.

Logfire provides both synchronous and asynchronous clients. To use these clients, you can import them from the `query_client` namespace:

```python
from logfire.query_client import AsyncLogfireQueryClient, LogfireQueryClient
```

Additional required dependencies

To use the query clients provided in `logfire.query_client`, you need to install `httpx`.

If you want to retrieve Arrow-format responses, you will also need to install `pyarrow`.

### Client Usage Examples

The `AsyncLogfireQueryClient` allows for asynchronous interaction with the Logfire API. If blocking I/O is acceptable and you want to avoid the complexities of asynchronous programming, you can use the plain `LogfireQueryClient`.

Here's an example of how to use these clients:

```python
from io import StringIO

import polars as pl
from logfire.query_client import AsyncLogfireQueryClient


async def main():
    query = """
    SELECT start_timestamp
    FROM records
    LIMIT 1
    """

    async with AsyncLogfireQueryClient(read_token='<your_read_token>') as client:
        # Load data as JSON, in column-oriented format
        json_cols = await client.query_json(sql=query)
        print(json_cols)

        # Load data as JSON, in row-oriented format
        json_rows = await client.query_json_rows(sql=query)
        print(json_rows)

        # Retrieve data in arrow format, and load into a polars DataFrame
        # Note that JSON columns such as `attributes` will be returned as
        # JSON-serialized strings
        df_from_arrow = pl.from_arrow(await client.query_arrow(sql=query))
        print(df_from_arrow)

        # Retrieve data in CSV format, and load into a polars DataFrame
        # Note that JSON columns such as `attributes` will be returned as
        # JSON-serialized strings
        df_from_csv = pl.read_csv(StringIO(await client.query_csv(sql=query)))
        print(df_from_csv)

        # Get read token info
        read_token_info = await client.info()
        print(read_token_info)


if __name__ == '__main__':
    import asyncio

    asyncio.run(main())
```

```python
from io import StringIO

import polars as pl
from logfire.query_client import LogfireQueryClient


def main():
    query = """
    SELECT start_timestamp
    FROM records
    LIMIT 1
    """

    with LogfireQueryClient(read_token='<your_read_token>') as client:
        # Load data as JSON, in column-oriented format
        json_cols = client.query_json(sql=query)
        print(json_cols)

        # Load data as JSON, in row-oriented format
        json_rows = client.query_json_rows(sql=query)
        print(json_rows)

        # Retrieve data in arrow format, and load into a polars DataFrame
        # Note that JSON columns such as `attributes` will be returned as
        # JSON-serialized strings
        df_from_arrow = pl.from_arrow(client.query_arrow(sql=query))
        print(df_from_arrow)

        # Retrieve data in CSV format, and load into a polars DataFrame
        # Note that JSON columns such as `attributes` will be returned as
        # JSON-serialized strings
        df_from_csv = pl.read_csv(StringIO(client.query_csv(sql=query)))
        print(df_from_csv)

        # Get read token info
        read_token_info = client.info()
        print(read_token_info)


if __name__ == '__main__':
    main()
```

## Making Direct HTTP Requests

If you prefer not to use the provided clients, you can make direct HTTP requests to the Logfire API using any HTTP client library, such as `requests` in Python. Below are the general steps and an example to guide you:

### General Steps to Make a Direct HTTP Request

1. **Set the Endpoint URL**: The base URL for the Logfire API is `https://logfire-us.pydantic.dev` for accounts in the US region, and `https://logfire-eu.pydantic.dev` for accounts in the EU region.
1. **Add Authentication**: Include the read token in your request headers to authenticate. The header key should be `Authorization` with the value `Bearer <your_read_token_here>`.
1. **Define the SQL Query**: Write the SQL query you want to execute.
1. **Send the Request**: Use an HTTP GET request to the `/v1/query` endpoint with the SQL query as a query parameter.

**Note:** You can provide additional query parameters to control the behavior of your requests. You can also use the `Accept` header to specify the desired format for the response data (JSON, Arrow, or CSV).

### Example: Using Python `requests` Library

```python
import requests

# Define the base URL and your read token
base_url = 'https://logfire-us.pydantic.dev'  # or 'https://logfire-eu.pydantic.dev' for EU accounts
read_token = '<your_read_token_here>'

# Set the headers for authentication
headers = {'Authorization': f'Bearer {read_token}'}

# Define your SQL query
query = """
SELECT start_timestamp
FROM records
LIMIT 1
"""

# Prepare the query parameters for the GET request
params = {
    'sql': query
}

# Send the GET request to the Logfire API
response = requests.get(f'{base_url}/v1/query', params=params, headers=headers)

# Check the response status
if response.status_code == 200:
    print("Query Successful!")
    print(response.json())
else:
    print(f"Failed to execute query. Status code: {response.status_code}")
    print(response.text)
```

### Additional Configuration

The Logfire API supports various response formats and query parameters to give you flexibility in how you retrieve your data:

- **Response Format**: Use the `Accept` header to specify the response format. Supported values include:
  - `application/json`: Returns the data in JSON format. By default, this will be column-oriented unless specified otherwise with the `json_rows` parameter.
  - `application/vnd.apache.arrow.stream`: Returns the data in Apache Arrow format, suitable for high-performance data processing.
  - `text/csv`: Returns the data in CSV format, which is easy to use with many data tools.
  - If no `Accept` header is provided, the default response format is JSON.
- **Query Parameters**:
  - **`sql`**: The SQL query to execute. This is the only required query parameter.
  - **`min_timestamp`**: An optional ISO-format timestamp to filter records with `start_timestamp` greater than this value for the `records` table or `recorded_timestamp` greater than this value for the `metrics` table. The same filtering can also be done manually within the query itself.
  - **`max_timestamp`**: Similar to `min_timestamp`, but serves as an upper bound for filtering `start_timestamp` in the `records` table or `recorded_timestamp` in the `metrics` table. The same filtering can also be done manually within the query itself.
  - **`limit`**: An optional parameter to limit the number of rows returned by the query. If not specified, **the default limit is 500**. The maximum allowed value is 10,000.
  - **`row_oriented`**: Only affects JSON responses. If set to `true`, the JSON response will be row-oriented; otherwise, it will be column-oriented.

All query parameters besides `sql` are optional and can be used in any combination to tailor the API response to your needs.

# Sampling

Sampling is the practice of discarding some traces or spans in order to reduce the amount of data that needs to be stored and analyzed. Sampling is a trade-off between cost and completeness of data.

*Head sampling* means the decision to sample is made at the beginning of a trace. This is simpler and more common.

*Tail sampling* means the decision to sample is delayed, possibly until the end of a trace. This means there is more information available to make the decision, but this adds complexity.

Sampling usually happens at the trace level, meaning entire traces are kept or discarded. This way the remaining traces are generally complete.

## Random head sampling

Here's an example of randomly sampling 50% of traces:

```python
import logfire

logfire.configure(sampling=logfire.SamplingOptions(head=0.5))

for x in range(10):
    with logfire.span(f'span {x}'):
        logfire.info(f'log {x}')
```

This outputs something like:

```text
11:09:29.041 span 0
11:09:29.041   log 0
11:09:29.041 span 1
11:09:29.042   log 1
11:09:29.042 span 4
11:09:29.042   log 4
11:09:29.042 span 5
11:09:29.042   log 5
11:09:29.042 span 7
11:09:29.042   log 7
```

Note that 5 out of 10 traces are kept, and that the child log is kept if and only if the parent span is kept.

## Tail sampling by level and duration

Random head sampling often works well, but you may not want to lose any traces which indicate problems. In this case, you can use tail sampling. Here's a simple example:

```python
import time

import logfire

logfire.configure(sampling=logfire.SamplingOptions.level_or_duration())

for x in range(3):
    # None of these are logged
    with logfire.span('excluded span'):
        logfire.info(f'info {x}')

    # All of these are logged
    with logfire.span('included span'):
        logfire.error(f'error {x}')

for t in range(1, 10, 2):
    with logfire.span(f'span with duration {t}'):
        time.sleep(t)
```

This outputs something like:

```text
11:37:45.484 included span
11:37:45.484   error 0
11:37:45.485 included span
11:37:45.485   error 1
11:37:45.485 included span
11:37:45.485   error 2
11:37:49.493 span with duration 5
11:37:54.499 span with duration 7
11:38:01.505 span with duration 9
```

logfire.SamplingOptions.level_or_duration() creates an instance of logfire.SamplingOptions with simple tail sampling. With no arguments, it means that a trace will be included if and only if it has at least one span/log that:

1. has a log level greater than `info` (the default of any span), or
1. has a duration greater than 5 seconds.

This way you won't lose information about warnings/errors or long-running operations. You can customize what to keep with the `level_threshold` and `duration_threshold` arguments.

## Combining head and tail sampling

You can combine head and tail sampling. For example:

```python
import logfire

logfire.configure(sampling=logfire.SamplingOptions.level_or_duration(head=0.1))
```

This will only keep 10% of traces, even if they have a high log level or duration. Traces that don't meet the tail sampling criteria will be discarded every time.

## Keeping a fraction of all traces

To keep some traces even if they don't meet the tail sampling criteria, you can use the `background_rate` argument. For example, this script:

```python
import logfire

logfire.configure(sampling=logfire.SamplingOptions.level_or_duration(background_rate=0.3))

for x in range(10):
    logfire.info(f'info {x}')
for x in range(5):
    logfire.error(f'error {x}')
```

will output something like:

```text
12:24:40.293 info 2
12:24:40.293 info 3
12:24:40.293 info 7
12:24:40.294 error 0
12:24:40.294 error 1
12:24:40.294 error 2
12:24:40.294 error 3
12:24:40.295 error 4
```

i.e. about 30% of the info logs and 100% of the error logs are kept.

(Technical note: the trace ID is compared against the head and background rates to determine inclusion, so the probabilities don't depend on the number of spans in the trace, and the rates give the probabilities directly without needing any further calculations. For example, with a head sample rate of `0.6` and a background rate of `0.3`, the chance of a non-notable trace being included is `0.3`, not `0.6 * 0.3`.)

## Caveats of tail sampling

### Memory usage

For tail sampling to work, all the spans in a trace must be kept in memory until either the trace is included by sampling or the trace is completed and discarded. In the above example, the spans named `included span` don't have a high enough level to be included, so they are kept in memory until the error logs cause the entire trace to be included. This means that traces with a large number of spans can consume a lot of memory, whereas without tail sampling the spans would be regularly exported and freed from memory without waiting for the rest of the trace.

In practice this is usually OK, because such large traces will usually exceed the duration threshold, at which point the trace will be included and the spans will be exported and freed. This works because the duration is measured as the time between the start of the trace and the start/end of the most recent span, so the tail sampler can know that a span will exceed the duration threshold even before it's complete. For example, running this script:

```python
import time

import logfire

logfire.configure(sampling=logfire.SamplingOptions.level_or_duration())

with logfire.span('span'):
    for x in range(1, 10):
        time.sleep(1)
        logfire.info(f'info {x}')
```

will do nothing for the first 5 seconds, before suddenly logging all this at once:

```text
12:29:43.063 span
12:29:44.065   info 1
12:29:45.066   info 2
12:29:46.072   info 3
12:29:47.076   info 4
12:29:48.082   info 5
```

followed by additional logs once per second. This is despite the fact that at this stage the outer span hasn't completed yet and the inner logs each have 0 duration.

However, memory usage can still be a problem in any of the following cases:

- The duration threshold is set to a high value
- Spans are produced extremely rapidly
- Spans contain large attributes

### Distributed tracing

Logfire's tail sampling is implemented in the SDK and only works for traces within one process. If you need tail sampling with distributed tracing, consider deploying the [Tail Sampling Processor in the OpenTelemetry Collector](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md).

If a trace was started on another process and its context was propagated to the process using the Logfire SDK tail sampling, the whole trace will be included.

If you start a trace with the Logfire SDK with tail sampling, and then propagate the context to another process, the spans generated by the SDK may be discarded, while the spans generated by the other process may be included, leading to an incomplete trace.

### Spans starting after root ended, e.g. background tasks

When the root span of a trace ends, if the trace doesn't meet the tail sampling criteria, all spans in the trace are discarded. If you start a new span in that trace (i.e. as a descendant of the root span) after the root span has ended, the new span will always be included anyway, and its parent will be missing. This is because the tail sampling mechanism only keeps track of active traces to save memory. This is similar to the distributed tracing case above.

Here's an example with a FastAPI background task which starts after the root span corresponding to the request has ended:

```python
import uvicorn
from fastapi import BackgroundTasks, FastAPI

import logfire

app = FastAPI()

logfire.configure(
    sampling=logfire.SamplingOptions.level_or_duration(
        duration_threshold=0.1,
    ),
)
logfire.instrument_fastapi(app)


async def background_task():
    # This will be included even if the root span was excluded.
    logfire.info('background')


@app.get('/')
async def index(background_tasks: BackgroundTasks):
    # Uncomment to prevent request span from being sampled out.
    # await asyncio.sleep(0.2)

    background_tasks.add_task(background_task)
    return {}


uvicorn.run(app)
```

A workaround is to explicitly put the new spans in their own trace using attach_context:

```python
import logfire


async def background_task():
   # `attach_context({})` forgets existing context
   # so that spans within start a new trace.
   with logfire.attach_context({}):
      with logfire.span('new trace'):
         await asyncio.sleep(0.2)
         logfire.info('background')
```

## Custom head sampling

If you need more control than random sampling, you can pass an [OpenTelemetry `Sampler`](https://opentelemetry-python.readthedocs.io/en/latest/sdk/trace.sampling.html). For example:

```python
from opentelemetry.sdk.trace.sampling import (
    ALWAYS_OFF,
    ALWAYS_ON,
    ParentBased,
    Sampler,
    TraceIdRatioBased,
)

import logfire


class MySampler(Sampler):
    def should_sample(
            self,
            parent_context,
            trace_id,
            name,
            *args,
            **kwargs,
    ):
        if name == 'exclude me':
            sampler = ALWAYS_OFF
        elif name == 'include me minimally':
            sampler = TraceIdRatioBased(0.01)  # 1% sampling
        elif name == 'include me partially':
            sampler = TraceIdRatioBased(0.5)   # 50% sampling
        else:
            sampler = ALWAYS_ON
        return sampler.should_sample(
            parent_context,
            trace_id,
            name,
            *args,
            **kwargs,
        )

    def get_description(self):
        return 'MySampler'


logfire.configure(
    sampling=logfire.SamplingOptions(
        head=ParentBased(
            MySampler(),
        )
    )
)

with logfire.span('keep me'):
    logfire.info('kept child')

for i in range(5):
    with logfire.span('include me partially'):
        logfire.info(f'partial sample {i}')

for i in range(270):
    with logfire.span('include me minimally'):
        logfire.info(f'minimal sample {i}')

with logfire.span('exclude me'):
    logfire.info('excluded child')
```

This will output something like:

```text
10:37:30.897 keep me
10:37:30.898   kept child
10:37:30.899 include me partially
10:37:30.900   partial sample 0
10:37:30.901 include me partially
10:37:30.902   partial sample 3
10:37:30.905 include me minimally
10:37:30.906   minimal sample 47
10:37:30.910 include me minimally
10:37:30.911   minimal sample 183
```

The sampler applies different strategies based on span names:

- `exclude me`: Never sampled (0% using `ALWAYS_OFF`)
- `include me partially`: 50% sampling (roughly half appear)
- `include me minimally`: 1% sampling (roughly 1 in a 100 appears)
- `keep me` and all others: Always sampled (100% using `ALWAYS_ON`)

The sampler is wrapped in a `ParentBased` sampler, which ensures child spans follow their parent's sampling decision. If you remove that and simply pass `head=MySampler()`, child spans might be included even when their parents are excluded, resulting in incomplete traces.

You can also pass a `Sampler` to the `head` argument of `SamplingOptions.level_or_duration` to combine tail sampling with custom head sampling.

## Custom tail sampling

If you want tail sampling with more control than `level_or_duration`, you can pass a function to tail which will accept an instance of TailSamplingSpanInfo and return a float between 0 and 1 representing the probability that the trace should be included. For example:

```python
import logfire


def get_tail_sample_rate(span_info):
    if span_info.duration >= 1:
        return 0.5  # (1)!

    if span_info.level > 'warn':  # (2)!
        return 0.3  # (3)!

    return 0.1  # (4)!


logfire.configure(
    sampling=logfire.SamplingOptions(
        head=0.5,  # (5)!
        tail=get_tail_sample_rate,
    ),
)
```

1. Keep 50% of traces with duration >= 1 second
1. `span_info.level` is a special object that can be compared to log level names
1. Keep 30% of traces with a warning or error and with duration < 1 second
1. Keep 10% of other traces
1. Discard 50% of traces at the beginning to reduce the overhead of generating spans. This is optional, but improves performance, and we know that `get_tail_sample_rate` will always return at most 0.5 so the other 50% of traces will be discarded anyway. The probabilities are not independent - this will not discard traces that would otherwise have been kept by tail sampling.

# Scrubbing sensitive data

The **Logfire** SDK scans for and redacts potentially sensitive data from logs and spans before exporting them.

## Disabling scrubbing

To disable scrubbing entirely, set scrubbing to `False`:

```python
import logfire

logfire.configure(scrubbing=False)
```

## Scrubbing more with custom patterns

By default, the SDK looks for some sensitive regular expressions. To add your own patterns, set extra_patterns to a list of regex strings:

```python
import logfire

logfire.configure(scrubbing=logfire.ScrubbingOptions(extra_patterns=['my_pattern']))

logfire.info('Hello', data={
    'key_matching_my_pattern': 'This string will be redacted because its key matches',
    'other_key': 'This string will also be redacted because it matches MY_PATTERN case-insensitively',
    'password': 'This will be redacted because custom patterns are combined with the default patterns',
})
```

Here are the default scrubbing patterns:

```python
['password', 'passwd', 'mysql_pwd', 'secret', 'auth(?!ors?\\b)', 'credential', 'private[._ -]?key', 'api[._ -]?key',
 'session', 'cookie', 'social[._ -]?security', 'credit[._ -]?card', '(?:\\b|_)csrf(?:\\b|_)', '(?:\\b|_)xsrf(?:\\b|_)',
 '(?:\\b|_)jwt(?:\\b|_)', '(?:\\b|_)ssn(?:\\b|_)']
```

## Scrubbing less with a callback

On the other hand, if the scrubbing is too aggressive, you can pass a function to callback to prevent certain data from being redacted.

The function will be called for each potential match found by the scrubber. If it returns `None`, the value is redacted. Otherwise, the returned value replaces the matched value. The function accepts a single argument of type logfire.ScrubMatch.

Here's an example:

```python
import logfire

def scrubbing_callback(match: logfire.ScrubMatch):
    # `my_safe_value` often contains the string 'password' but it's not actually sensitive.
    if match.path == ('attributes', 'my_safe_value') and match.pattern_match.group(0) == 'password':
        # Return the original value to prevent redaction.
        return match.value

logfire.configure(scrubbing=logfire.ScrubbingOptions(callback=scrubbing_callback))
```

## Security tips

### Use message templates

The full span/log message is not scrubbed, only the fields within. For example, this:

```python
logfire.info('User details: {user}', user=User(id=123, password='secret'))
```

...may log something like:

```text
User details: [Scrubbed due to 'password']
```

...but this:

```python
user = User(id=123, password='secret')
logfire.info('User details: ' + str(user))
```

will log:

```text
User details: User(id=123, password='secret')
```

This is necessary so that safe messages such as 'Password is correct' are not redacted completely.

Using f-strings (e.g. `logfire.info(f'User details: {user}')`) *is* safe if `inspect_arguments` is enabled (the default in Python 3.11+) and working correctly. [See here](../../guides/onboarding-checklist/add-manual-tracing/#f-strings) for more information.

In short, don't format the message yourself. This is also a good practice in general for [other reasons](../../guides/onboarding-checklist/add-manual-tracing/#messages-and-span-names).

### Keep sensitive data out of URLs

The attribute `"http.url"` which is recorded by OpenTelemetry instrumentation libraries is considered safe so that URLs like `"http://example.com/users/123/authenticate"` are not redacted.

As a general rule, not just for Logfire, assume that URLs (including query parameters) will be logged, so sensitive data should be put in the request body or headers instead.

### Use parameterized database queries

The `"db.statement"` attribute which is recorded by OpenTelemetry database instrumentation libraries is considered safe so that SQL queries like `"SELECT secret_value FROM table WHERE ..."` are not redacted.

Use parameterized queries (e.g. prepared statements) so that sensitive data is not interpolated directly into the query string, even if you use an interpolation method that's safe from SQL injection.

# Setup Slack Alerts

**Logfire** allows you to send alerts via **Slack** based upon the configured alert criteria.

## Creating a Slack Incoming Webhook

**Logfire** uses **Slack's** Incoming Webhooks feature to send alerts.

The [Incoming Webhooks Slack docs](https://api.slack.com/messaging/webhooks) has all the details on setting up and using incoming webhooks.

For brevity, here's a list of steps you will need to perform:

1. In your Slack Workspace, create or identify a channel where you want to send Logfire alerts.

1. Create a new Slack app (or use an existing one) by navigating to <https://api.slack.com/apps/new>. Give this a meaningful name such as "Logfire Alerts" or similar so you can identify it later.

1. In the [Apps Management Dashboard](https://api.slack.com/apps), Underneath the **Features** heading on the side bar, select **Incoming Webhooks**

1. Click on the **Add New Webhook** button. This will guide you to a page where you select the channel you want to send alerts to.

1. Click the **Allow** button. You will be redirected back to the **Incoming Webhooks** page, and in the list, you will see your new Webhook URL. This will be a URL that looks similar to something like this:

   ```text
   https://hooks.slack.com/services/...
   ```

1. Copy that somewhere, and save it for the next step

## Creating an Alert

There are a few ways to create an alert. You can:

- Follow our [Detect Service is Down](../detect-service-is-down/) guide
- Have a look at the [alerts documentation](../../guides/web-ui/alerts/).

### Define alert

We'll create an alert that will let us know if any HTTP request takes longer than a second to execute.

- Login to **Logfire** and [navigate to your project](https://logfire-us.pydantic.dev/-/redirect/latest-project)

- Click on **Alerts** in the top navigation bar

- Select the **New Alert** button in the top right

- Let's give this Alert a name of **Slow Requests**

- For the query, we'll group results by the http path and duration. We want to include the **max** duration in a given time frame. We also want to filter out any traces that aren't http requests, and order by the max duration, so we can see which routes are the slowest. This query looks like:

  ```sql
  SELECT
      max(duration),
      attributes->>'http.route'
  FROM
      records
  WHERE
      duration > 1
      AND attributes->>'http.route' IS NOT NULL
  GROUP BY
      attributes->>'http.route'
  ORDER BY
    max(duration) desc
  ```

- Click **Preview query results** and make sure you get some results back. If your service is lightning fast, firstly congratulations! Secondly try adjust the duration cutoff to something smaller, like `duration > 0.1` (i.e, any requests taking longer than 100ms).

- You can adjust when alerts are sent based upon the alert parameters. With this style of alert, we just want to know if anything within the last 5 minutes has been slow. So we can use the following options:

  - **Execute the query**: every 5 minutes
  - **Include rows from**: the last 5 minutes
  - **Notify me when**: the query has any results

### Send Alert to a Slack Channel

Our alert is almost done, let's send it to a slack channel.

For this, you will need the [Webhook URL](#creating-a-slack-incoming-webhook) you created & copied from the Slack [Apps Management Dashboard](https://api.slack.com/apps).

Let's set up a channel, then test that alerts can be sent with the URL:

- Select **New channel** to open the New Channel dialog
- Put in a name such as **Logfire Alerts**. This does need to be the name of your Slack channel
- Select **Slack** as the format
- Paste in your Webhook URL from the Slack [Apps Management Dashboard] (https://api.slack.com/apps)
- Click on **Send a test alert** and check that you can see the alert in Slack.
- Click **Create Channel** to create the channel and close the dialog
- Click the checkbox next to your new channel to select it

Once your Slack channel is connected, click **Create alert** to save all your changes. Your alert is now live!

You will now receive notifications within your slack channel when the alert is triggered!

# Suppress Spans and Metrics

At **Logfire** we want to provide you with the best experience possible. We understand that sometimes you might want to fine tune the data you're sending to **Logfire**. That's why we provide you with the ability to suppress spans and metrics.

We provide two ways to suppress the data you're sending to **Logfire**: [Suppress Scopes](#suppress-scopes) and [Suppress Instrumentation](#suppress-instrumentation).

## Suppress Scopes

You can suppress spans and metrics from a specific OpenTelemetry scope. This is useful when you want to suppress data from a specific package.

For example, if you have [BigQuery](../../integrations/databases/bigquery/) installed, it automatically instruments itself with OpenTelemetry. Which means that you need to opt-out of instrumentation if you don't want to send data to **Logfire** related to BigQuery.

You can do this by calling the suppress_scopes method.

```py
import logfire

logfire.configure()
logfire.suppress_scopes("google.cloud.bigquery.opentelemetry_tracing")
```

In this case, we're suppressing the scope `google.cloud.bigquery.opentelemetry_tracing`. All spans and metrics related to BigQuery will not be sent to **Logfire**.

## Suppress Instrumentation

Sometimes you might want to suppress spans from a specific part of your code, and not a whole package.

For example, assume you are using [HTTPX], but you don't want to suppress all the spans and metrics related to it. You just want to suppress a small part of the code that you know will generate a lot of spans.

You can do this by using the suppress_instrumentation context manager.

```py
import httpx
import logfire

logfire.configure()

client = httpx.Client()
logfire.instrument_httpx(client)

# The span generated will be sent to Logfire.
client.get("https://httpbin.org/get")

# The span generated will NOT be sent to Logfire.
with logfire.suppress_instrumentation():
    client.get("https://httpbin.org/get")
```

In this case, the span generated inside the `with logfire.suppress_instrumentation():` block will not be sent to **Logfire**.

# Writing SQL Queries for Dashboards

This guide provides practical recipes and patterns for writing useful SQL queries in **Logfire**. We'll focus on querying the [`records`](../../reference/sql/#records-columns) table, which contains your logs and spans. The goal is to help you create useful dashboards, but we recommend using the Explore view to learn and experiment.

For a complete list of available tables and columns, please see the [SQL Reference](../../reference/sql/).

## Finding common cases

### Simple examples

Here are two quick useful examples to try out immediately in the Explore view.

To find the most common operations based on [`span_name`](../../reference/sql/#span_name):

```sql
SELECT
    COUNT() AS count,
    span_name
FROM records
GROUP BY span_name
ORDER BY count DESC
```

Similarly, to find the most common [`exception_type`](../../reference/sql/#exception_type)s:

```sql
SELECT
    COUNT() AS count,
    exception_type
FROM records
WHERE is_exception
GROUP BY exception_type
ORDER BY count DESC
```

Finally, to find the biggest traces, which may be a sign of an operation doing too many things:

```sql
SELECT
    COUNT() AS count,
    trace_id
FROM records
GROUP BY trace_id
ORDER BY count DESC
```

### The general pattern

The basic template is:

```sql
SELECT
    COUNT() AS count,
    <columns_to_group_by>
FROM records
WHERE <filter_conditions>
GROUP BY <columns_to_group_by>
ORDER BY count DESC
LIMIT 10
```

- The alias `AS count` allows us to refer to the count in the `ORDER BY` clause.
- `ORDER BY count DESC` sorts the results to show the most common groups first.
- `WHERE <filter_conditions>` is optional and depends on your specific use case.
- `LIMIT 10` isn't usually needed in the Explore view, but is helpful when creating charts.
- `<columns_to_group_by>` can be one or more columns and should be the same in the `SELECT` and `GROUP BY` clauses.

### Useful things to group by

- [`span_name`](../../reference/sql/#span_name): this is nice and generic and shouldn't have too much variability, creating decently sized meaningful groups. It's especially good for HTTP server request spans, where it typically contains the HTTP method and route (the path template) without the specific parameters. To focus on such spans, trying filtering by the appropriate [`otel_scope_name`](../../reference/sql/#otel_scope_name), e.g. `WHERE otel_scope_name = 'opentelemetry.instrumentation.fastapi'` if you're using [`logfire.instrument_fastapi()`](../../integrations/web-frameworks/fastapi/).
- [`exception_type`](../../reference/sql/#exception_type)
- [`http_response_status_code`](../../reference/sql/#http_response_status_code)
- [`attributes->>'...'`](../../reference/sql/#attributes): this will depend on your specific data. Try taking a look at some relevant spans in the Live view first to see what attributes are available.
  - If you have a [custom attribute](../../guides/onboarding-checklist/add-manual-tracing/#attributes) like `attributes->>'user_id'`, you can group by that to see which users are most active or have the most errors.
  - If you're using [`logfire.instrument_fastapi()`](../../integrations/web-frameworks/fastapi/), there's often useful values inside the `fastapi.arguments.values` attribute, e.g. `attributes->'fastapi.arguments.values'->>'user_id'`.
  - All web frameworks also typically capture several other potentially useful HTTP attributes. In particular if you [capture headers](../../integrations/web-frameworks/#capturing-http-server-request-and-response-headers) (e.g. with `logfire.instrument_django(capture_headers=True)`) then you can try e.g. `attributes->>'http.request.header.host'`.
  - For database query spans, try `attributes->>'db.statement'`, `attributes->>'db.query.text'`, or `attributes->>'db.query.summary'`.
- [`message`](../../reference/sql/#message) - this is often very variable which may make it less useful depending on your data. But if you're using a SQL instrumentation with the Python **Logfire** SDK, then this contains a rough summary of the SQL query, helping to group similar queries together if `attributes->>'db.statement'` or `attributes->>'db.query.text'` is too granular. In this case you probably want a filter such as `otel_scope_name = 'opentelemetry.instrumentation.sqlalchemy'` to focus on SQL queries.
- [`service_name`](../../reference/sql/#service_name) if you've configured multiple services. This can easily be combined with any other column.
- [`deployment_environment`](../../reference/sql/#deployment_environment) if you've configured multiple [environments](../environments/) (e.g. `production` and `staging`, `development`) and have more than one selected in the UI at the moment. Like `service_name`, this is good for combining with other columns.

### Useful `WHERE` clauses

- [`is_exception`](../../reference/sql/#is_exception)
- [`level >= 'error'`](../../reference/sql/#level) - maybe combined with the above with either `AND` or `OR`, since you can have non-error exceptions and non-exception errors.
- [`level > 'info'`](../../reference/sql/#level) to also find warnings and other notable records, not just errors.
- [`http_response_status_code >= 400`](../../reference/sql/#http_response_status_code) or `500` to find HTTP requests that didn't go well.
- [`service_name = '...'`](../../reference/sql/#service_name) can help a lot to make queries faster.
- [`otel_scope_name = '...'`](../../reference/sql/#otel_scope_name) to focus on a specific (instrumentation) library, e.g. `opentelemetry.instrumentation.fastapi`, `logfire.openai`, or `pydantic-ai`.
- [`duration > 2`](../../reference/sql/#duration) to find slow spans. Replace `2` with your desired number of seconds.
- [`parent_span_id IS NULL`](../../reference/sql/#parent_span_id) to find top-level spans, i.e. the root of a trace.

### Using in Dashboards

For starters:

1. Create a panel in a [new or existing custom (not standard) dashboard](../../guides/web-ui/dashboards/#creating-custom-dashboards).
1. Click the **Type** dropdown and select **Table**.
1. Paste your query into the SQL editor.
1. Give your panel a name.
1. Click **Apply** to save it.

Tables are easy and flexible. They can handle any number of `GROUP BY` columns, and don't really need a `LIMIT` to be practical. But they're not very pretty. Try making a bar chart instead:

1. Click the pencil icon to edit your panel.
1. Change the **Type** to **Bar Chart**.
1. Add `LIMIT 10` or so at the end of your query if there are too many bars.
1. If you have multiple `GROUP BY` columns, you will need to convert them to one expression by concatenating strings. Replace each `,` in the `GROUP BY` clause with `|| ' - ' ||` to create a single string with dashes between the values, e.g. replace `service_name, span_name` with `service_name || ' - ' || span_name`. Do this in both the `SELECT` and `GROUP BY` clauses. You can optionally add an `AS` alias to the concatenated string in the `SELECT` clause, like `service_name || ' - ' || span_name AS service_and_span`, and then use that alias in the `GROUP BY` clause instead of repeating the concatenation.
1. If your grouping column happens to be a number, add `::text` to it to convert it to a string so that it's recognized as a category by the chart.
1. Bar charts generally put the first rows in the query result at the bottom. If you want the most common items at the top, wrap the whole query in `SELECT * FROM (<original query>) ORDER BY count ASC` to flip the order. Now the inner `ORDER BY count DESC` ensures that we have the most common items (before the limit is applied) and the outer `ORDER BY count ASC` is for appearance.

For a complete example, you can replace this:

```sql
SELECT
    COUNT() AS count,
    service_name,
    span_name
FROM records
GROUP BY service_name, span_name
ORDER BY count DESC
```

with:

```sql
SELECT * FROM (
    SELECT
        COUNT() AS count,
        service_name || ' - ' || span_name AS service_and_span
    FROM records
    GROUP BY service_and_span
    ORDER BY count DESC
    LIMIT 10
) ORDER BY count ASC
```

Finally, check the **Settings** tab in the panel editor to tweak the appearance of your chart.

## Aggregating Numerical Data

Instead of just counting rows, you can perform calculations on numerical expressions. [`duration`](../../reference/sql/#duration) is a particularly useful column for performance analysis. Tweaking our first example:

```sql
SELECT
    SUM(duration) AS total_duration_seconds,
    span_name
FROM records
WHERE duration IS NOT NULL -- Ignore logs, which have no duration
GROUP BY span_name
ORDER BY total_duration_seconds DESC
```

This will show you which operations take the most time overall, whether it's because they run frequently or take a long time each time.

Alternatively you could use `AVG(duration)` (for the mean) or `MEDIAN(duration)` to find which operations are slow on average, or `MAX(duration)` to find the worst case scenarios. See the [Datafusion documentation](https://datafusion.apache.org/user-guide/sql/aggregate_functions.html) for the full list of available aggregation functions.

Other numerical values can typically be found inside `attributes` and will depend on your data. LLM spans often have `attributes->'gen_ai.usage.input_tokens'` and `attributes->'gen_ai.usage.output_tokens'` which you can use to monitor costs.

Warning

`SUM(attributes->'...')` and other numerical aggregations will typically return an error because the database doesn't know the type of the JSON value inside `attributes`, so use `SUM((attributes->'...')::numeric)` to convert it to a number.

### Percentiles

A slightly more advanced aggregation is to calculate percentiles. For example, the 95th percentile means the value below which 95% of the data falls. This is typically referred to as P95, and tells you a more 'typical' worst-case scenario while ignoring the extreme outliers found by using `MAX()`. P90 and P99 are also commonly used.

To calculate this requires a bit of extra syntax:

```sql
SELECT
    approx_percentile_cont(0.95) WITHIN GROUP (ORDER BY duration) as P95,
    span_name
FROM records
WHERE duration IS NOT NULL
GROUP BY span_name
ORDER BY P95 DESC
```

This query calculates the 95th percentile of the `duration` for each `span_name`. The general pattern is `approx_percentile_cont(<percentile>) WITHIN GROUP (ORDER BY <column>)` where `<percentile>` is a number between 0 and 1.

## Time series

Create a new panel in a dashboard, and by default it will have the type **Time Series Chart** with a query like this:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    count() as count
FROM records
GROUP BY x
```

Here the `time_bucket($resolution, start_timestamp)` is essential. [`$resolution` is a special variable that exists in all dashboards](../../guides/web-ui/dashboards/#resolution-variable) and adjusts automatically based on the time range. You can adjust it while viewing the dashboard using the dropdown in the top left corner. It doesn't exist in the Explore view, so you have to use a concrete interval like `time_bucket('1 hour', start_timestamp)` there. Tick **Show rendered query** in the panel editor to fill in the resolution and other variables so that you can copy the query to the Explore view.

Warning

If you're querying `metrics`, use `recorded_timestamp` instead of `start_timestamp`.

You can give the time bucket expression any name (`x` in the example above), but it must be the same in both the `SELECT` and `GROUP BY` clauses. The chart will detect that the type is a timestamp and use it as the x-axis.

You can replace the `count()` with any aggregation(s) you want. For example, you can show multiple levels of percentiles:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    approx_percentile_cont(0.80) WITHIN GROUP (ORDER BY duration) AS p80,
    approx_percentile_cont(0.90) WITHIN GROUP (ORDER BY duration) AS p90,
    approx_percentile_cont(0.95) WITHIN GROUP (ORDER BY duration) AS p95
FROM records
GROUP BY x
```

### Grouping by Dimension

You can make time series charts with multiple lines (series) for the same numerical metric by grouping by a dimension. This means adding a column to the `SELECT` and `GROUP BY` clauses and then selecting it from the 'Dimension' dropdown next to the SQL editor.

#### Low cardinality dimensions

For a simple example, paste this into the SQL editor:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    log(count()) as log_count,
    level_name(level) as level
FROM records
GROUP BY x, level
```

Then set the **Dimension** dropdown to `level` and the **Metrics** dropdown to `log_count`. This will create a time series chart with multiple lines, one for each log level (e.g. 'info', 'warning', 'error'). Here's what the configuration and result would look like:

Note

You can only set one dimension, but you can set multiple metrics.

Here we use `log(count())` instead of just `count()` because you probably have way more 'info' records than 'error' records, making it hard to notice any spikes in the number of errors. This compresses the y-axis into a logarithmic scale to make it more visually useful, but the numbers are harder to interpret. The `level_name(level)` function converts the numeric [`level`](../../reference/sql/#level) value to a human-readable string.

You can replace `level` with [other useful things to group by](#useful-things-to-group-by), but they need to be very low cardinality (i.e. not too many unique values) for this simple query to work well in a time series chart. Good examples are:

- `service_name`
- `deployment_environment`
- `exception_type`
- `http_response_status_code`
- `attributes->>'http.method'` (or `attributes->>'http.request.method'` for newer OpenTelemetry instrumentations)

#### Multiple dimensions

Because time series charts can only have one dimension, if you want to group by multiple columns, you need to concatenate them into a single string, e.g. instead of:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    log(count()) as log_count,
    service_name,
    level_name(level) as level
FROM records
GROUP BY x, service_name, level
```

You would do:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    log(count()) as log_count,
    service_name || ' - ' || level_name(level) AS service_and_level
FROM records
GROUP BY x, service_and_level
```

Then set the 'Dimension' dropdown to `service_and_level`. This will create a time series chart with a line for each combination of `service_name` and `level`. Of course this increases the cardinality quickly, making the next section more relevant.

#### High cardinality dimensions

If you try grouping by something with more than a few unique values, you'll end up with a cluttered chart with too many lines. For example, this will look like a mess unless your data is very simple:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    count() as count,
    span_name
FROM records
GROUP BY x, span_name
```

The quick and dirty solution is to add these lines at the end:

```sql
ORDER BY count DESC
LIMIT 200
```

This will give you a point for the 200 most common *combinations of `x` and `span_name`*. This will often work reasonably well, but the limit will need to be tuned based on the data, and the number of points at each time bucket will vary. Here's the better version:

```sql
WITH original AS (
    SELECT
        time_bucket($resolution, start_timestamp) AS x,
        count() as count,
        span_name
    FROM records
    GROUP BY x, span_name
),
ranked AS (
    SELECT
        x,
        count,
        span_name,
        ROW_NUMBER() OVER (PARTITION BY x ORDER BY count DESC) AS row_num
    FROM original
)
SELECT
    x,
    count,
    span_name
FROM ranked
WHERE row_num <= 5
ORDER BY x
```

This selects the top 5 `span_name`s for each time bucket, and will usually work perfectly. It may look intimidating, but constructing a query like this can be done very mechanically. Start with a basic query in this form:

```sql
SELECT
    time_bucket($resolution, start_timestamp) AS x,
    <aggregation_expression> AS metric,
    <dimension_expression> as dimension
FROM records
GROUP BY x, dimension
```

Fill in `<aggregation_expression>` with your desired aggregation (e.g. `count()`, `SUM(duration)`, etc.) and `<dimension_expression>` with the column(s) you want to group by (e.g. `span_name`). Set the 'Dimension' dropdown to `dimension` and the 'Metrics' dropdown to `metric`. That should give you a working (but probably cluttered) time series chart. Then simply paste it into `<original_query>` in the following template:

```sql
WITH original AS (
    <original_query>
),
ranked AS (
    SELECT
        x,
        metric,
        dimension,
        ROW_NUMBER() OVER (PARTITION BY x ORDER BY metric DESC) AS row_num
    FROM original
)
SELECT
    x,
    metric,
    dimension
FROM ranked
WHERE row_num <= 5
ORDER BY x
```

## Linking to the Live view

While aggregating data with `GROUP BY` is powerful for seeing trends, sometimes you need to investigate specific events, like a single slow operation or a costly API call. In these cases, it's good to include the [`trace_id`](../../reference/sql/#trace_id) column in your `SELECT` clause. Tables in dashboards, the explore view, or alert run results with this column will render the `trace_id` values as clickable links to the Live View.

For example, to find the 10 slowest spans in your system, you can create a 'Table' panel with this query:

```sql
SELECT
    trace_id,
    duration,
    message
FROM records
ORDER BY duration DESC
LIMIT 10
```

The table alone won't tell you much, but you can click on the `trace_id` of any row to investigate the full context further.

You can also select the [`span_id`](../../reference/sql/#span_id) column to get a link directly to a specific span within the trace viewer. However, this only works if the `trace_id` column is also included in your `SELECT` statement.

Other columns that may be useful to include in such queries:

- [`message`](../../reference/sql/#message) is a human readable description of the span, as seen in the Live view list of records.
- [`start_timestamp`](../../reference/sql/#start_timestamp) and [`end_timestamp`](../../reference/sql/#end_timestamp)
- [`attributes`](../../reference/sql/#attributes)
- [`service_name`](../../reference/sql/#service_name)
- [`otel_scope_name`](../../reference/sql/#otel_scope_name)
- [`deployment_environment`](../../reference/sql/#deployment_environment)
- [`otel_resource_attributes`](../../reference/sql/#otel_resource_attributes)
- [`exception_type`](../../reference/sql/#exception_type) and [`exception_message`](../../reference/sql/#exception_message)
- [`http_response_status_code`](../../reference/sql/#http_response_status_code)
- [`level_name(level)`](../../reference/sql/#level)

## Creating Histograms

Histograms are useful for visualizing the distribution of numerical data, such as the duration of spans. They provide richer information than simple summary statistics like averages. Currently the UI has no built in way to display histograms, but it's possible with SQL. Just copy this template and fill in the `source_data` CTE with your actual data:

```sql
WITH
source_data AS (
    -- Replace this with your actual source data query.
    -- It must return a single column named `amount` with numeric values.
    select duration as amount from records
),
histogram_config AS (
    -- Tweak this number if you want.
    SELECT 40 AS num_buckets
),

-- The rest of the query is fully automatic, leave it as is.
raw_params AS (
    SELECT MIN(amount)::numeric AS min_a, MAX(amount)::numeric AS max_a, num_buckets
    FROM source_data, histogram_config GROUP BY num_buckets),
params_with_shift AS (
    SELECT *, CASE WHEN min_a <= 0 THEN 1 - min_a ELSE 0 END AS shift
    FROM raw_params),
params AS (
    SELECT *, CASE WHEN min_a = max_a THEN 1.000000001 ELSE exp(ln((max_a + shift) / (min_a + shift)) / num_buckets::double) END AS b
    FROM params_with_shift),
actual_counts AS (
    SELECT floor(log(b, (amount + shift) / (min_a + shift)))::int AS ind, COUNT() AS count
    FROM source_data, params GROUP BY ind),
all_buckets AS (
    SELECT UNNEST(generate_series(0, num_buckets - 1)) as ind
    FROM params),
midpoints AS (
    SELECT ind, (min_a + shift) * power(b, ind + 0.5) - shift as mid
    FROM all_buckets, params)
SELECT round(mid, 3)::text as approx_amount, COALESCE(count, 0) as count
FROM midpoints m LEFT JOIN actual_counts c ON m.ind = c.ind ORDER BY mid;
```

Then set the chart type to **Bar Chart**. Each bar represents a 'bucket' that actual values are placed into. The x-axis will show the approximate amount that the values in the bucket can be rounded to, and the y-axis will show the count of rows in each bucket. This is an exponential histogram, meaning that the buckets are wider for larger values, which is useful for data that has a long tail distribution.

How does this work?

Here's the query again with detailed comments:

```sql
-- =============================================================================
-- STEP 1: DEFINE YOUR DATA SOURCE
-- =============================================================================
WITH source_data AS (
    -- PASTE YOUR QUERY HERE
    -- This query must select one numerical column named 'amount'.
    -- Example: Test case including negative, zero, and positive values.
    select duration as amount from records
),

-- =============================================================================
-- STEP 2: CONFIGURE HISTOGRAM
-- =============================================================================
histogram_config AS (
    -- This sets the desired number of bars in the final chart.
    SELECT 40 AS num_buckets
),

-- =============================================================================
-- The rest of the query is fully automatic.
-- =============================================================================

-- This CTE performs a single pass over the source data to find its true range.
raw_params AS (
    SELECT
        MIN(amount)::numeric AS min_a,
        MAX(amount)::numeric AS max_a,
        num_buckets
    FROM source_data, histogram_config
    GROUP BY num_buckets
),

-- This CTE calculates a 'shift' value. Logarithms are only defined for positive
-- numbers, so if the data contains 0 or negative values, we must temporarily
-- shift the entire dataset into the positive domain (where the new minimum is 1).
params_with_shift AS (
    SELECT
        *,
        CASE WHEN min_a <= 0 THEN 1 - min_a ELSE 0 END AS shift
    FROM raw_params
),

-- This CTE calculates the final exponential 'base' for the histogram scaling.
-- The base determines how quickly the bucket sizes grow. It is calculated such
-- that 'num_buckets' steps will perfectly cover the shifted data range.
params AS (
    SELECT
        *,
        -- If min = max, we use a base slightly > 1. This prevents log(1) errors
        -- and allows the binning logic to work without a special case.
        CASE
            WHEN min_a = max_a THEN 1.000000001
            ELSE exp(ln((max_a + shift) / (min_a + shift)) / num_buckets::double)
        END AS base
    FROM params_with_shift
),

-- This CTE takes every record from the source data and assigns it to a bucket
-- index (0, 1, 2, ...). This is the core binning logic.
actual_counts AS (
    SELECT
        -- The formula log_base(value/start) calculates "how many exponential steps
        -- of size 'base' are needed to get from the start of the range to the
        -- current value". We use the shifted values for this calculation.
        floor(log(base, (amount + shift) / (min_a + shift)))::int AS bucket_index,
        COUNT() AS count
    FROM source_data, params
    GROUP BY bucket_index
),

-- This CTE generates a perfect, gap-free template of all possible bucket indices,
-- ensuring the final chart has exactly 'num_buckets' bars.
all_buckets AS (
    SELECT UNNEST(generate_series(0, num_buckets - 1)) as bucket_index
    FROM params
),

-- This CTE calculates the representative midpoint for every possible bucket.
-- This logic is separated from the final join to prevent query planner bugs.
midpoints AS (
    SELECT
        bucket_index,
        -- We calculate the geometric midpoint of each bucket in the shifted space
        -- using 'power(base, index + 0.5)' and then shift it back to the
        -- original data's scale. This provides a more representative label for
        -- an exponential scale than a simple arithmetic mean.
        (min_a + shift) * power(base, bucket_index + 0.5) - shift as bucket_midpoint
    FROM all_buckets
    CROSS JOIN params
)

-- This final step assembles the chart data. It joins the calculated midpoints
-- with the actual counts and formats the output.
SELECT
    round(m.bucket_midpoint, 3)::text as bucket_midpoint,
    -- If a bucket has no items, its count will be NULL after the join.
    -- COALESCE turns these NULLs into 0 for the chart.
    COALESCE(c.count, 0) as count
FROM midpoints m
LEFT JOIN actual_counts c ON m.bucket_index = c.bucket_index
ORDER BY m.bucket_midpoint;
```

# OpenTelemetry Collector

The OpenTelemetry Collector is a powerful tool that can be used to collect, process, and export telemetry data from various sources. It is designed to work with a wide range of data sources and can be easily configured to meet your specific needs. It can be run in a multitude of topologies, including as a standalone service, as a sidecar in a container, or as an agent on a host.

Although it is very powerful and versatile the Collector is also an advanced tool that is not required to use Logfire. If you don't need any of the Collectors features it is perfectly reasonable to send data from the Logfire SDK directly to our backend, and this is the default configuration for our SDK.

Use cases for the OpenTelemetry Collector include:

- **Centralized configuration**: keep Logfire credentials in a single place. Configure exporting to multiple backends (e.g. Logfire and audit logging) in a single place. All with the ability to update the configuration without needing to make changes to applications.
- **Data transformation**: transform data before sending it to Logfire. For example, you can use the OpenTelemetry Collector to filter out sensitive information, extract structured data from logs or otherwise modify the data before sending it to Logfire.
  - For a detailed guide on common transformation patterns, see our guide on [Advanced Scrubbing](../otel-collector-scrubbing/) with the OTel Collector.
- **Data enrichment**: add additional context to your logs before sending them to Logfire. For example, you can use the OpenTelemetry Collector to add information about the host or container where the log was generated.
- **Collecting existing data sources**: the Collector can be used to collect system logs (e.g. Kubernetes logs) or metrics from other formats. For example, you can use it to collect container logs from Kubernetes and scrape Prometheus metrics.

As Logfire is a fully compliant OpenTelemetry SDK and backend it does not require any special configuration to be used with the OpenTelemetry collector. Below we include a couple of examples for using the OpenTelemetry collector, assuming the deployment is being done on Kubernetes, but you can deploy the collector in any system, see the [official documentation](https://opentelemetry.io/docs/collector/deployment/) for more information.

This documentation does not attempt to be a complete guide to the OpenTelemetry collector, but rather a gentle introduction along with some key examples. For more information on the collector please see the [official documentation](https://opentelemetry.io/docs/collector/).

## Back up data in AWS S3

Data older than **30 days** is pruned from our backend (except for customers on our [enterprise plans](../../../enterprise/)). If you want to keep your data stored long-term, you can configure the **Logfire** SDK to also send data to the OpenTelemetry Collector, which will then forward the data to AWS S3.

Tip

This uses the [OpenTelemetry Collector AWS S3 Exporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/awss3exporter), see their docs for more details.

There are many other exporters available, such as for [Azure Blob Storage](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/azureblobexporter).

Here's how you can try this out right now. First, copy the below OpenTelemetry Collector configuration into a file called `config.yaml` and fill in the `region` and `s3_bucket` fields.

config.yaml

```yaml
receivers:
  otlp:
    protocols:
      http:
        endpoint: "0.0.0.0:4318"
exporters:
  awss3:
    s3uploader:
      region: <REPLACE-WITH-YOUR-REGION>
      s3_bucket: <REPLACE-WITH-YOUR-BUCKET-NAME>
processors:
  batch:
    timeout: 10s
    send_batch_size: 32768
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [awss3]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [awss3]
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [awss3]
```

Next, run the OpenTelemetry Collector locally with the above configuration using Docker:

```shell
docker run \
    -v ./config.yaml:/etc/otelcol-contrib/config.yaml \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    -p 4318:4318 \
    otel/opentelemetry-collector-contrib
```

Now send some data to the OpenTelemetry Collector using the Logfire SDK. See the [Alternative Backends guide](../../alternative-backends/) for more details.

script.py

```python
import os

import logfire

# This will make the Logfire SDK send data to the OpenTelemetry Collector
os.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'

# Keep the default send_to_logfire=True, so it will also send data to Logfire.
logfire.configure()

logfire.info('Hello, {name}!', name='world')
```

After running the script, you should see the data in both the **Logfire** UI and your S3 bucket. The files in S3 will have keys like `year=2025/month=06/day=25/hour=14/minute=09/traces_312302042.json`.

Logfire doesn't support importing this data, but you can use other OpenTelemetry-compatible tools. For example, run this command to start a [Jaeger](https://www.jaegertracing.io/) container:

```text
docker run --rm \
  -p 16686:16686 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

then open <http://localhost:16686/> and click on 'Upload'.

Alternatively, install [`otel-tui`](https://github.com/ymtdzzz/otel-tui) and run `otel-tui --from-json-file <path-to-file>` to view the data in your terminal.

However, these simple options don't work well for searching through many files. For that, you can set up another OTel collector with the [S3 receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/awss3receiver) to read directly from S3, or the [OTLP JSON File Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/otlpjsonfilereceiver) to read from locally downloaded files. Then you can point the collector at a tool like Jaeger, `otel-tui`, or Grafana Tempo to visualize the data.

## Collecting system logs

This example shows how you can use the OpenTelemetry collector to collect systems logs (logs on stdoutt/stderr) from Kubernetes and send them to Logfire. This may be useful as part of a migration to Logfire if you aren't able to immediately edit all of the applications to install the Logfire SDK, although the data you receive won't be as rich as it would be from tracing with the Logfire SDK.

This relatively simple example is enough in many cases to replace existing systems like ElasticSearch, Loki or Splunk.

To follow this guide you'll need to have a local Kubernetes cluster running. There are many options for this including [Docker Desktop](https://www.docker.com/blog/how-to-set-up-a-kubernetes-cluster-on-docker-desktop/), [Rancher Desktop](https://docs.rancherdesktop.io/), [Minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fmacos%2Farm64%2Fstable%2Fbinary+download), [Kind](https://kind.sigs.k8s.io/), [k3s](https://docs.k3s.io/quick-start).

We'll first create an application via `apps.yaml` that emits some structured and unstructured logs to stdout/stderr:

apps.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: plain-app
  namespace: default
  labels:
    app: plain-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: plain-app
  template:
    metadata:
      labels:
        app: plain-app
    spec:
      terminationGracePeriodSeconds: 1
      containers:
        - name: plain-app
          image: busybox
          command: ["sh", "-c", "while true; do echo 'Hello World'; sleep 1; done"]
          resources:
            limits:
              memory: "64Mi"
              cpu: "500m"
            requests:
              memory: "64Mi"
              cpu: "500m"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: json-app
  namespace: default
  labels:
    app: json-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: json-app
  template:
    metadata:
      labels:
        app: json-app
    spec:
      terminationGracePeriodSeconds: 1
      containers:
        - name: json-app
          image: busybox
          command:
            - "sh"
            - "-c"
            - |
              while true; do
                now=$(date -u '+%Y-%m-%dT%H:%M:%SZ')
                echo "{\"message\":\"Hello world!\",\"level\":\"warn\",\"timestamp\":\"$now\"}"
                sleep 1
              done
          resources:
            limits:
              memory: "64Mi"
              cpu: "500m"
            requests:
              memory: "64Mi"
              cpu: "500m"
```

Deploy this application via `kubectl apply -f apps.yaml`.

Now we will set up a collector that can scrape logs from these apps, process them and send them to logfire.

We'll need to store Logfire credentials somewhere, a Kubernetes Secret is a reasonable choice, a better choice for a production environment would be to use [External Secrets Operator](https://external-secrets.io/latest/).

First create a Logfire write token, see [Create Write Tokens](../../create-write-tokens/).

Now to save it as a secret in Kubernetes run the following command, replacing `your-write-token` with the value of the write token you just created:

```shell
kubectl create secret generic logfire-token --from-literal=logfire-token=your-write-token
```

Note that this is equivalent to the following `secrets.yaml` file, but using `kubectl` is easier because it will base64 encode the secret for you.

secrets.yaml

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: logfire-token
type: Opaque
data:
  logfire-token: base64-encoded-logfire-token
```

For the OTel Collector to scrape logs it will need permissions into the Kubernetes API which Kubernetes does not give out by default (you wouldn't want random pods being able to see logs from other pods by default!).

To do this we'll create an `rbac.yaml` file with the following content:

rbac.yaml

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector-role
rules:
- apiGroups: [""]
  resources: ["pods", "namespaces"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["replicasets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector-role
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: default
```

Apply this configuration via `kubectl apply -f rbac.yaml`.

Now we can create the deployment for the collector itself. There are [several options for deploying the OTel collector](https://opentelemetry.io/docs/platforms/kubernetes/collector/components) including:

- As a sidecar container on each / some pods. This requires less permissions but implies manual configuration of each deployment with a sidecar. This option may work well if you want to bolt on **Logfire** to specific existing applications you control without modifying the application itself or deploying the collector cluster wide.
- As a DaemonSet, this will deploy the collector on every node in the cluster. This is a good option if you want to collect logs from all pods in the cluster without modifying each deployment. Additionally DaemonSets can collect certain information that is not available to sidecars or services. This is the option we will use in this guide.
- As a Service/Gateway, this option that allows you to deploy the collector as a standalone Kubernetes service.

Create a `collector.yaml` file with the following content:

collector.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  config.yaml: |-
    receivers:
      filelog:
        include_file_path: true
        include:
          - /var/log/pods/*/*/*.log
        exclude:
          # Exclude logs from all containers named otel-collector
          - /var/log/pods/*/otel-collector/*.log
        operators:
          - id: container-parser
            type: container
          - id: json_parser
            type: json_parser
            if: 'hasPrefix(body, "{\"")'
            parse_from: body
            parse_to: attributes
            parse_ints: true
            timestamp:
              parse_from: attributes.timestamp
              layout_type: strptime
              layout: "%Y-%m-%dT%H:%M:%S.%f%z"
            severity:
              parse_from: attributes.level
              overwrite_text: true
    exporters:
      debug:
      otlphttp:
        # Configure the US / EU endpoint for Logfire.
        # - US: https://logfire-us.pydantic.dev
        # - EU: https://logfire-eu.pydantic.dev
        endpoint: "https://logfire-eu.pydantic.dev"
        headers:
          Authorization: "Bearer ${env:LOGFIRE_TOKEN}"
    service:
      pipelines:
        logs:
          receivers: [filelog]
          exporters: [debug, otlphttp]
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: otel-collector
  template:
    metadata:
      labels:
        app: opentelemetry
        component: otel-collector
    spec:
      serviceAccountName: otel-collector
      terminationGracePeriodSeconds: 1
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.128.0
        env:
        - name: LOGFIRE_TOKEN
          valueFrom:
            secretKeyRef:
              name: logfire-token
              key: logfire-token
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - mountPath: /var/log
          name: varlog
          readOnly: true
        - mountPath: /var/lib/docker/containers
          name: varlibdockercontainers
          readOnly: true
        - mountPath: /etc/otelcol-contrib/config.yaml
          name: data
          subPath: config.yaml
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: data
        configMap:
          name: otel-collector-config
```

Apply this configuration via `kubectl apply -f otel-collector.yaml`.

You should now see logs from the `plain-app` and `json-app` in your Logfire dashboard!

## Add Kubernetes attributes to traces, logs and metrics

This example shows how to enrich your existing applications traces, logs and metrics with Kubernetes metadata, such as the deployment, node and namespace name.

It is supported by OpenTelemetry Collector in either daemonset or gateway deployment mode with no configuration changes, you can refer to the [OTel collector documentation](https://opentelemetry.io/docs/platforms/kubernetes/collector/components) for more information about the deployment patterns.

First, you need to setup RBAC for the OpenTelemetry Collector to access the metadata you'll need, for example:

rbac.yaml

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups:
      - ''
    resources:
      - 'pods'
      - 'namespaces'
    verbs:
      - 'get'
      - 'watch'
      - 'list'
  - apiGroups:
      - 'apps'
    resources:
      - 'replicasets'
      - 'deployments'
      - 'statefulsets'
      - 'daemonsets'
    verbs:
      - 'get'
      - 'list'
      - 'watch'
  - apiGroups:
      - 'extensions'
    resources:
      - 'replicasets'
    verbs:
      - 'get'
      - 'list'
      - 'watch'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
subjects:
  - kind: ServiceAccount
    name: otel-collector
    namespace: otel-collector
roleRef:
  kind: ClusterRole
  name: otel-collector
  apiGroup: rbac.authorization.k8s.io
```

If you want to limit the access to a single namespace you can use `Role` and `RoleBinding` with minimal changes documented below.

This is useful if you're running multiple instances of the Collector and you want to limit their access scope.

Now, you need to enable the `k8sattributes` processor in the collector config:

config.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  config.yaml: |-
    receivers:
      # an otlp receiver, you can configure your application to send data to it
      # for enrichment and processing before exporting to Logfire.
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    processors:
      # by default the connection IP is used to match data with k8s object
      # when using, for example, a daemonset to send logs to a gateway
      # you can use `pod_association` to configure which fields to use for matching.
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
      # If you're using a namespaced RBAC, you'll need to set this filter
      # filter:
      #   namespace: default
      extract:
        metadata:
          # the cluster's UID won't be set with the namespaced configuration
          - k8s.cluster.uid
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.name
          - container.image.tag
          - container.id
          - k8s.container.name
          - container.image.name
          - container.image.tag
          - container.id
    exporters:
      debug:
      otlphttp:
        # Configure the US / EU endpoint for Logfire.
        # - US: https://logfire-us.pydantic.dev
        # - EU: https://logfire-eu.pydantic.dev
        endpoint: "https://logfire-eu.pydantic.dev"
        headers:
          Authorization: "Bearer ${env:LOGFIRE_TOKEN}"
    service:
      # configure logs, metrics and traces with k8s attributes enrichment
      # before sending them to Logfire.
      pipelines:
        logs:
          receivers: [otlp]
          processors: [k8sattributes]
          exporters: [debug, otlphttp]
        metrics:
          receivers: [otlp]
          processors: [k8sattributes]
          exporters: [debug, otlphttp]
        traces:
          receivers: [otlp]
          processors: [k8sattributes]
          exporters: [debug, otlphttp]
```

After applying this configuration, you should be able to see, query and filter you traces, metrics and logs in Logfire with Kubernetes attributes!

For example:

```sql
SELECT exception_message
FROM records
WHERE is_exception = true
AND otel_resource_attributes->>'k8s.namespace.name' = 'default';
```

You can find more information about the `k8sattributes` processor in the [Kubernetes Attributes Processor for OpenTelemetry Collector documentation](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor).

# Advanced scrubbing with the OTel Collector

The Logfire SDK already comes with powerful, [built-in scrubbing](../../scrubbing/) to automatically protect sensitive data within your application. For most use cases, adding `extra_patterns` or using a `callback` is all you need.

However, as your system grows, you may need more powerful, centralized, and conditional scrubbing logic. This is where the [OpenTelemetry (OTel) Collector](../otel-collector-overview/) really stands out. By using the collector as a central hub, you can apply complex data transformation rules before data reaches our backend, without adding overhead to your applications.

This guide will walk you through advanced scrubbing techniques using OTel Collector processors.

Please take a look at the [OTel Collector overview](../otel-collector-overview/) first if you aren't already using it.

### Why Use the Collector for Scrubbing?

- **Performance**: Offload the processing work from your application to the collector, keeping your services lean and fast.
- **Advanced Logic:** Implement rules that are too complex for the SDK, such as scrubbing data conditionally based on other attributes (e.g., only scrub PII from failed requests).
- **Language Agnostic:** The same scrubbing rules apply whether your services are written in Python, Java, Go, or any other language.

______________________________________________________________________

**Note:** Make sure you set `logfire.configure(send_to_logfire=False)` where you want to apply data transformation, otherwise traces that reach logfire will not have the desired modifications. You can take a look at [Alternative Backends](../../alternative-backends/).

______________________________________________________________________

### Scenario 1: Scrubbing or Removing Attributes by Key

The [attributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md) works well for acting on known attribute keys.

For example, here's a config snippet showing how to:

- Replace any attribute with the *exact* keys `session_id` or `user_token` with 'SCRUBBED'
- Remove completely any key that *contains* `password`

```yaml
processors:
  attributes:
    actions:
      - key: session_id
        action: update
        value: "SCRUBBED"
      - key: user_token
        action: update
        value: "SCRUBBED"
      # Using `pattern` instead of `key` matches any key containing the pattern
      - pattern: "password"
      # Remove the key completely instead of replacing
        action: delete
```

______________________________________________________________________

### Scenario 2: Masking Sensitive Values with Regex

The [redaction processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/redactionprocessor/README.md) can mask or hash regex patterns *within* a value instead of scrubbing the whole thing. For example, here's how to mask email addresses:

Collector `config.yaml` snippet:

```yaml
processors:
  # The redaction processor is perfect for finding and masking patterns within values.
  redaction:
    # Flag to allow all span attribute keys. In this case, we want this set to true because we only want to block values.
    allow_all_keys: true
    # BlockedValues is a list of regular expressions for blocking span attribute values. Values that match are masked.
    blocked_values:
     - '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
    # You can also enable a hash function. By default, no hash function is used and masking with a fixed string is performed.
    # hash_function: md5
```

- **Before:** `user.comment` = "My email is `test@example.com`, please contact me."
- **After:** `user.comment` = "My email is `***`, please contact me."

______________________________________________________________________

### Scenario 3: Conditional Scrubbing with Logic

The [transform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md) has a powerful query language called OTTL which lets you apply conditional logic. For example, here's how to scrub the `credit_card_number` attribute, but **only** if the transaction failed, i.e. `http.status_code` is 500 or greater.

```yaml
processors:
  transform:
    trace_statements:
      - set(span.attributes["credit_card_number"], "[REDACTED]") where span.attributes["http.status_code"] >= 500
```

______________________________________________________________________

## Complete Example

To use these processors, you need to add them to a service pipeline in your collector configuration. The data will flow through them in the order you specify.

Here is a complete `config.yaml` showing how you might chain these processors together:

```yaml
# 1. RECEIVERS: How the collector ingests data
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

# 2. PROCESSORS: How we scrub and modify the data
processors:
  # First, do simple key-based scrubbing/removal.
  attributes:
    - key: session_id
      action: update
      value: "[Scrubbed due to session_id]"
    - key: user_token
      action: update
      value: "[Scrubbed due to user_token]"

  # Next, find and mask any PII values we missed.
  redaction:
    allow_all_keys: true
    blocked_values:
     - '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'

  # Finally, apply complex conditional rules.
  transform:
    trace_statements:
      - set(span.attributes["credit_card_number"], "[REDACTED]") where span.attributes["http.status_code"] >= 500

# 3. EXPORTERS: Where the scrubbed data is sent
exporters:
  debug:
  otlphttp:
    # Configure the US / EU endpoint for Logfire.
    # - US: https://logfire-us.pydantic.dev
    # - EU: https://logfire-eu.pydantic.dev
    endpoint: "https://logfire-eu.pydantic.dev"
    headers:
      Authorization: "Bearer ${env:LOGFIRE_TOKEN}"

# 4. SERVICE: The pipeline that connects everything
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [attributes, redaction, transform]
      exporters: [otlphttp, debug]
    logs:
      receivers: [otlp]
      processors: [attributes, redaction]
      exporters: [otlphttp, debug]
```

Now you should have a clearer sense of what's possible using the OpenTelemetry Collector processors for data scrubbing.

Remember, for this scrubbing to work, ensure all telemetry data is only routed through the OTel Collector by setting `logfire.configure(send_to_logfire=False)`
# Integrations

# Integrations

**Pydantic Logfire** supports first-class integration with many popular Python packages using a single `logfire.instrument_<package>()` function call. Each of these should be called exactly once after logfire.configure().

For example, to instrument FastAPI and HTTPX, you would do:

```python
import logfire

logfire.configure()
logfire.instrument_fastapi()
logfire.instrument_httpx()

# ... your application code here ...
```

If a package you are using is not listed in this documentation, please let us know on our [Slack](https://logfire.pydantic.dev/docs/join-slack/)!

## Documented Integrations

**Logfire** has documented integrations with many technologies, including:

- *LLM Clients and AI Frameworks*: Pydantic AI, OpenAI, Anthropic, LangChain, LlamaIndex, Mirascope, LiteLLM, Magentic
- *Web Frameworks*: FastAPI, Django, Flask, Starlette, AIOHTTP, ASGI, WSGI
- *Database Clients*: Psycopg, SQLAlchemy, Asyncpg, PyMongo, MySQL, SQLite3, Redis, BigQuery
- *HTTP Clients*: HTTPX, Requests, AIOHTTP
- *Task Queues and Schedulers*: Airflow, FastStream, Celery
- *Logging Libraries*: Standard Library Logging, Loguru, Structlog
- and more, such as Stripe, AWS Lambda, and system metrics.

The below table lists these integrations and any corresponding `logfire.instrument_<package>()` calls:

| Package                                 | Type                    | Logfire Instrument Call / Notes                                          |
| --------------------------------------- | ----------------------- | ------------------------------------------------------------------------ |
| [Pydantic Validation](pydantic/)        | Data Validation         | logfire.instrument_pydantic()                                            |
| [Pydantic AI](llms/pydanticai/)         | AI                      | logfire.instrument_pydantic_ai()                                         |
| [AIOHTTP](http-clients/aiohttp/)        | HTTP Client             | logfire.instrument_aiohttp_client(), logfire.instrument_aiohttp_server() |
| [Airflow](event-streams/airflow/)       | Task Scheduler          | N/A (built in, config needed)                                            |
| [Anthropic](llms/anthropic/)            | AI                      | logfire.instrument_anthropic()                                           |
| [ASGI](web-frameworks/asgi/)            | Web Framework Interface | logfire.instrument_asgi()                                                |
| [AWS Lambda](aws-lambda/)               | Cloud Function          | logfire.instrument_aws_lambda()                                          |
| [Asyncpg](databases/asyncpg/)           | Database                | logfire.instrument_asyncpg()                                             |
| [BigQuery](databases/bigquery/)         | Database                | N/A (built in, no config needed)                                         |
| [Celery](event-streams/celery/)         | Task Queue              | logfire.instrument_celery()                                              |
| [Django](web-frameworks/django/)        | Web Framework           | logfire.instrument_django()                                              |
| [FastAPI](web-frameworks/fastapi/)      | Web Framework           | logfire.instrument_fastapi()                                             |
| [FastStream](event-streams/faststream/) | Task Queue              | N/A (built in, config needed)                                            |
| [Flask](web-frameworks/flask/)          | Web Framework           | logfire.instrument_flask()                                               |
| [HTTPX](http-clients/httpx/)            | HTTP Client             | logfire.instrument_httpx()                                               |
| [LangChain](llms/langchain/)            | AI Framework            | N/A (built-in OpenTelemetry support)                                     |
| [LlamaIndex](llms/llamaindex/)          | AI Framework            | N/A (requires LlamaIndex OpenTelemetry package)                          |
| [LiteLLM](llms/litellm/)                | AI Gateway              | N/A (requires LiteLLM callback setup)                                    |
| [Loguru](loguru/)                       | Logging                 | See documentation                                                        |
| [Magentic](llms/magentic/)              | AI Framework            | N/A (built-in Logfire support)                                           |
| [Mirascope](llms/mirascope/)            | AI Framework            | N/A (use mirascope `@with_logfire` decorator)                            |
| [MySQL](databases/mysql/)               | Database                | logfire.instrument_mysql()                                               |
| [OpenAI](llms/openai/)                  | AI                      | logfire.instrument_openai()                                              |
| [Psycopg](databases/psycopg/)           | Database                | logfire.instrument_psycopg()                                             |
| [PyMongo](databases/pymongo/)           | Database                | logfire.instrument_pymongo()                                             |
| [Redis](databases/redis/)               | Database                | logfire.instrument_redis()                                               |
| [Requests](http-clients/requests/)      | HTTP Client             | logfire.instrument_requests()                                            |
| [SQLAlchemy](databases/sqlalchemy/)     | Database                | logfire.instrument_sqlalchemy()                                          |
| [SQLite3](databases/sqlite3/)           | Database                | logfire.instrument_sqlite3()                                             |
| [Standard Library Logging](logging/)    | Logging                 | See documentation                                                        |
| [Starlette](web-frameworks/starlette/)  | Web Framework           | logfire.instrument_starlette()                                           |
| [Stripe](stripe/)                       | Payment Gateway         | N/A (requires other instrumentations)                                    |
| [Structlog](structlog/)                 | Logging                 | See documentation                                                        |
| [System Metrics](system-metrics/)       | System Metrics          | logfire.instrument_system_metrics()                                      |
| [WSGI](web-frameworks/wsgi/)            | Web Framework Interface | logfire.instrument_wsgi()                                                |

If you are using Logfire with a web application, we also recommend reviewing our [Web Frameworks](web-frameworks/) documentation.

## OpenTelemetry Integrations

Since **Logfire** is [OpenTelemetry](https://opentelemetry.io/) compatible, it can be used with any OpenTelemetry instrumentation package. You can find the list of all OpenTelemetry instrumentation packages [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/).

Many of the integrations documented in the previous section are based upon the OpenTelemetry instrumentation packages with first-class support built into **Logfire**.

## Creating Custom Integrations

If you are a maintainer of a package and would like to create an integration for **Logfire**, you can do it!

We've created a shim package called `logfire-api`, which can be used to integrate your package with **Logfire**.

The idea of `logfire-api` is that it doesn't have any dependencies. It's a very small package that matches the API of **Logfire**. We created it so that you can create an integration for **Logfire** without having to install **Logfire** itself.

You can use `logfire-api` as a lightweight dependency of your own package. If `logfire` is installed, then `logfire-api` will use it. If not, it will use a no-op implementation. This way users of your package can decide whether or not they want to install `logfire`, and you don't need to check whether or not it's installed.

Here's how you can use `logfire-api`:

```python
import logfire_api as logfire

logfire.info("Hello, Logfire!")
```

Note

You generally *don't* want to call `logfire_api.configure()`, it's up to your users to call `logfire.configure()` if they want to use the integration.

All the **Logfire** API methods are available in `logfire-api`.

# AWS Lambda

The logfire.instrument_aws_lambda function can be used to instrument AWS Lambda functions to automatically send traces to **Logfire**.

## Installation

Install `logfire` with the `aws-lambda` extra:

```bash
pip install 'logfire[aws-lambda]'
```

```bash
uv add 'logfire[aws-lambda]'
```

```bash
poetry add 'logfire[aws-lambda]'
```

## Usage

To instrument an AWS Lambda function, call the `logfire.instrument_aws_lambda` function after defining the handler function:

```python
import logfire

logfire.configure()  # (1)!


def handler(event, context):
    return 'Hello from Lambda'

logfire.instrument_aws_lambda(handler)
```

1. Remember to set the `LOGFIRE_TOKEN` environment variable on your Lambda function configuration.

logfire.instrument_aws_lambda uses the **OpenTelemetry AWS Lambda Instrumentation** package, which you can find more information about [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/aws_lambda/aws_lambda.html).

# Standard Library Logging

**Logfire** can act as a sink for standard library logging by emitting a **Logfire** log for every standard library log record.

main.py

```py
from logging import basicConfig, getLogger

import logfire

logfire.configure()
basicConfig(handlers=[logfire.LogfireLoggingHandler()])

logger = getLogger(__name__)

logger.error("Hello %s!", "Fred")
# 10:05:06.855 Hello Fred!
```

main.py

```py
from logging.config import dictConfig

import logfire

logfire.configure()
dictConfig({
    'version': 1,
    'handlers': {
        'logfire': {
            'class': 'logfire.LogfireLoggingHandler',
        },
    },
    'root': {
        'handlers': ['logfire'],
    },
})

logger = getLogger(__name__)

logger.error("Hello %s!", "Fred")
# 10:05:06.855 Hello Fred!
```

The LogfireLoggingHandler will emit log records to the Logfire instance, unless [instrumentation is suppressed](../../how-to-guides/suppress/#suppress-instrumentation), in which case a fallback handler will be used (defaults to StreamHandler, writing to sys.stderr).

## Oh no! Too many logs from...

A common issue with logging is that it can be **too verbose**... Right?

Don't worry! We are here to help you.

In those cases, you can set the log level to a higher value to suppress logs that are less important. Let's see an example with the [`apscheduler`](https://apscheduler.readthedocs.io/en/stable/) logger:

main.py

```py
import logging

logger = logging.getLogger("apscheduler")
logger.setLevel(logging.WARNING)
```

In this example, we set the log level of the `apscheduler` logger to `WARNING`, which means that only logs with a level of `WARNING` or higher will be emitted.

## Disabling `urllib3` debug logs

As instrumentation is suppressed when sending log data to **Logfire**, unexpected DEBUG logs can appear in the console (through the use of the fallback handler), emitted when performing the API request to send logs.

To disable such logs, a [filter](https://docs.python.org/3/library/logging.html#filter-objects) can be used:

main.py

```py
import logging
from logging import DEBUG, basicConfig, getLogger

import logfire

logfire.configure()
logfire_handler = logfire.LogfireLoggingHandler()

urllib3_filter = logging.Filter('urllib3')
# Disable urllib3 debug logs on the fallback handler
# (by default, writing to `sys.stderr`):
logfire_handler.fallback.addFilter(lambda record: not urllib3_filter.filter(record))

basicConfig(handlers=[logfire_handler], level=DEBUG)
```

main.py

```py
import logging
from logging.config import dictConfig

import logfire

logfire.configure()

fallback = logging.StreamHandler()

urllib3_filter = logging.Filter('urllib3')
# Disable urllib3 debug logs on the fallback handler
# (by default, writing to `sys.stderr`):
fallback.addFilter(lambda record: not urllib3_filter.filter(record))

dictConfig({
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'logfire': {
            'class': 'logfire.LogfireLoggingHandler',
            'level': 'DEBUG',
            'fallback': fallback,
        },
    },
    'root': {
        'handlers': ['logfire'],
        'level': 'DEBUG',
    },
})
```

# Loguru

**Logfire** can act as a sink for [Loguru](https://github.com/Delgan/loguru) by emitting a **Logfire** log for every log record. For example:

main.py

```py
import logfire
from loguru import logger

logfire.configure()

logger.configure(handlers=[logfire.loguru_handler()])
logger.info('Hello, {name}!', name='World')
```

Note

Currently, **Logfire** will not scrub sensitive data from the message formatted by Loguru, e.g:

```python
logger.info('Foo: {bar}', bar='secret_value')
# > 14:58:26.085 Foo: secret_value
```

# Instrumenting `print()`

logfire.instrument_print() can be used to capture calls to `print()` and emit them as **Logfire** logs. For example:

main.py

```py
import logfire

logfire.configure()
logfire.instrument_print()

name = 'World'
print('Hello', name)
```

This will still print as usual, but will also emit a log with the message `Hello World` as expected.

If Logfire is configured with inspect_arguments=True, the names of the arguments passed to `print` will be included in the log attributes and will be used for scrubbing. In the example above, this means that the log will include `{'name': 'World'}` in the attributes. The first argument `'Hello'` is automatically excluded because it's a literal. If the variable name was `password`, then it would be scrubbed from both the message and the attributes.

# Pydantic Validation

Pydantic Logfire has a Pydantic Validation plugin to instrument [Pydantic Validation](https://docs.pydantic.dev/latest/) models. The plugin provides logs and metrics about model validation.

To enable the plugin, do one of the following:

- Set the `LOGFIRE_PYDANTIC_PLUGIN_RECORD` environment variable to `all`.
- Set `pydantic_plugin_record` in `pyproject.toml`, e.g:

```toml
[tool.logfire]
pydantic_plugin_record = "all"
```

- Call logfire.instrument_pydantic with the desired configuration, e.g:

```py
import logfire

logfire.instrument_pydantic()  # Defaults to record='all'
```

Note that if you only use the last option then only model classes defined and imported *after* calling `logfire.instrument_pydantic` will be instrumented.

Note

Remember to call logfire.configure() at some point, whether before or after calling `logfire.instrument_pydantic` and defining model classes. Model validations will only start being logged after calling `logfire.configure()`.

## Third party modules

By default, third party modules are not instrumented by the plugin to avoid noise. You can enable instrumentation for those using the include configuration.

```py
logfire.instrument_pydantic(include={'openai'})
```

You can also disable instrumentation for your own modules using the exclude configuration.

```py
logfire.instrument_pydantic(exclude={'app.api.v1'})
```

## Model configuration

If you want more granular control over the plugin, you can use the plugin_settings class parameter in your Pydantic models.

```py
from logfire.integrations.pydantic import PluginSettings
from pydantic import BaseModel


class Foo(BaseModel, plugin_settings=PluginSettings(logfire={'record': 'failure'})):
    ...
```

### Record

The record argument is used to configure what to record. It can be one of the following values:

- `all`: Send traces and metrics for all events. This is default value for `logfire.instrument_pydantic`.
- `failure`: Send metrics for all validations and traces only for validation failures.
- `metrics`: Send only metrics.
- `off`: Disable instrumentation.

### Tags

Tags are used to add additional information to the traces, and metrics. They can be included by adding the tags key in plugin_settings.

```py
from pydantic import BaseModel


class Foo(
  BaseModel,
  plugin_settings={'logfire': {'record': 'all', 'tags': ('tag1', 'tag2')}}
):
```

# Stripe

[Stripe](https://stripe.com) is a popular payment gateway that allows businesses to accept payments online.

The stripe Python client has both synchronous and asynchronous methods for making requests to the Stripe API.

By default, the stripe client uses the `requests` package for making synchronous requests and the `httpx` package for making asynchronous requests.

```py
from stripe import StripeClient

client = StripeClient(api_key='<your_secret_key>')

# Synchronous request
client.customers.list()  # uses `requests`

# Asynchronous request
async def main():
    await client.customers.list_async()  # uses `httpx`

if __name__ == '__main__':
    import asyncio

    asyncio.run(main())
```

You read more about this on the [Configuring an HTTP Client](https://github.com/stripe/stripe-python#configuring-an-http-client) section on the stripe repository.

## Synchronous Requests

As mentioned, by default, `stripe` uses the `requests` package for making HTTP requests.

In this case, you'll need to call [`logfire.instrument_requests()`](../http-clients/requests/).

```py
import os

import logfire
from stripe import StripeClient

logfire.configure()
logfire.instrument_requests()

client = StripeClient(api_key=os.getenv('STRIPE_SECRET_KEY'))

client.customers.list()
```

Note

If you use the `http_client` parameter to configure the stripe client to use a different HTTP client, you'll need to call the appropriate instrumentation method.

## Asynchronous Requests

As mentioned, by default, `stripe` uses the `httpx` package for making asynchronous HTTP requests.

In this case, you'll need to call [`logfire.instrument_httpx()`](../http-clients/httpx/).

```py
import asyncio
import os

import logfire
from stripe import StripeClient

logfire.configure()
logfire.instrument_httpx()     # for asynchronous requests

client = StripeClient(api_key=os.getenv('STRIPE_SECRET_KEY'))

async def main():
    with logfire.span('list async'):
        await client.customers.list_async()

if __name__ == '__main__':
    asyncio.run(main())
```

Note

If you use the `http_client` parameter to configure the stripe client to use a different HTTP client, you'll need to call the appropriate instrumentation method.

## Add logging instrumentation

Stripe also has a logger (`logger = getLogger('stripe')`) that [you can instrument with **Logfire**](../logging/).

```py
import os
from logging import basicConfig

import logfire
from stripe import StripeClient

logfire.configure()
basicConfig(handlers=[logfire.LogfireLoggingHandler()], level='INFO')

client = StripeClient(api_key=os.getenv('STRIPE_SECRET_KEY'))

client.customers.list()
```

You can change the `level=INFO` to `level=DEBUG` to see even more details, like the response body.

# Structlog

**Logfire** has a built-in [structlog](https://www.structlog.org/en/stable/) processor that can be used to emit Logfire logs for every structlog event.

main.py

```py
from dataclasses import dataclass

import structlog
import logfire

logfire.configure()

structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.TimeStamper(fmt='%Y-%m-%d %H:%M:%S', utc=False),
        logfire.StructlogProcessor(),
        structlog.dev.ConsoleRenderer(),
    ],
)
logger = structlog.get_logger()


@dataclass
class User:
    id: int
    name: str


logger.info('Login', user=User(id=42, name='Fred'))
#> 2024-03-22 12:57:33 [info     ] Login                          user=User(id=42, name='Fred')
```

The **Logfire** processor **MUST** come before the last processor that renders the logs in the structlog configuration.

By default, LogfireProcessor shown above disables console logging by logfire so you can use the existing logger you have configured for structlog, if you want to log with logfire, use LogfireProcessor(console_log=True).

Note

Positional arguments aren't collected as attributes by the processor, since they are already part of the event message when the processor is called.

If you have the following:

```py
logger.error('Hello %s!', 'Fred')
#> 2024-03-22 13:39:26 [error    ] Hello Fred!
```

The string `'Fred'` will not be collected by the processor as an attribute, just formatted with the message.

The logfire.instrument_system_metrics() method can be used to collect system metrics with **Logfire**, such as CPU and memory usage.

## Installation

Install `logfire` with the `system-metrics` extra:

```bash
pip install 'logfire[system-metrics]'
```

```bash
uv add 'logfire[system-metrics]'
```

```bash
poetry add 'logfire[system-metrics]'
```

## Usage

```py
import logfire

logfire.configure()

logfire.instrument_system_metrics()
```

Then in your project, click on 'Dashboards' in the top bar, click 'New Dashboard', and select 'Basic System Metrics (Logfire)' from the dropdown.

## Configuration

By default, `instrument_system_metrics` collects only the metrics it needs to display the 'Basic System Metrics (Logfire)' dashboard. You can choose exactly which metrics to collect and how much data to collect about each metric. The default is equivalent to this:

```py
logfire.instrument_system_metrics({
    'process.cpu.utilization': None,  # (1)!
    'system.cpu.simple_utilization': None,  # (2)!
    'system.memory.utilization': ['available'],  # (3)!
    'system.swap.utilization': ['used'],  # (4)!
})
```

1. The `None` value means that there are no fields to configure for this metric. The value of this metric is [`psutil.Process().cpu_percent()`](https://psutil.readthedocs.io/en/latest/#psutil.Process.cpu_percent)`/100`, i.e. the fraction of CPU time used by this process, where 1 means using 100% of a single CPU core. The value can be greater than 1 if the process uses multiple cores.
1. The `None` value means that there are no fields to configure for this metric. The value of this metric is [`psutil.cpu_percent()`](https://psutil.readthedocs.io/en/latest/#psutil.cpu_percent)`/100`, i.e. the fraction of CPU time used by the whole system, where 1 means using 100% of all CPU cores.
1. The value here is a list of 'modes' of memory. The full list can be seen in the [`psutil` documentation](https://psutil.readthedocs.io/en/latest/#psutil.virtual_memory). `available` is "the memory that can be given instantly to processes without the system going into swap. This is calculated by summing different memory metrics that vary depending on the platform. It is supposed to be used to monitor actual memory usage in a cross platform fashion." The value of the metric is a number between 0 and 1, and subtracting the value from 1 gives the fraction of memory used.
1. This is the fraction of available swap used. The value is a number between 0 and 1.

To collect lots of detailed data about all available metrics, use `logfire.instrument_system_metrics(base='full')`.

Warning

The amount of data collected by `base='full'` can be expensive, especially if you have many servers, and this is easy to forget about. If you enable this, be sure to monitor your usage and costs.

The most expensive metrics are `system.cpu.utilization/time` which collect data for each core and each mode, and `system.disk.*` which collect data for each disk device. The exact number depends on the machine hardware, but this can result in hundreds of data points per minute from each instrumented host.

`logfire.instrument_system_metrics(base='full')` is equivalent to:

```py
logfire.instrument_system_metrics({
    'system.cpu.simple_utilization': None,
    'system.cpu.time': ['idle', 'user', 'system', 'irq', 'softirq', 'nice', 'iowait', 'steal', 'interrupt', 'dpc'],
    'system.cpu.utilization': ['idle', 'user', 'system', 'irq', 'softirq', 'nice', 'iowait', 'steal', 'interrupt', 'dpc'],
    'system.memory.usage': ['available', 'used', 'free', 'active', 'inactive', 'buffers', 'cached', 'shared', 'wired', 'slab', 'total'],
    'system.memory.utilization': ['available', 'used', 'free', 'active', 'inactive', 'buffers', 'cached', 'shared', 'wired', 'slab'],
    'system.swap.usage': ['used', 'free'],
    'system.swap.utilization': ['used'],
    'system.disk.io': ['read', 'write'],
    'system.disk.operations': ['read', 'write'],
    'system.disk.time': ['read', 'write'],
    'system.network.dropped.packets': ['transmit', 'receive'],
    'system.network.packets': ['transmit', 'receive'],
    'system.network.errors': ['transmit', 'receive'],
    'system.network.io': ['transmit', 'receive'],
    'system.thread_count': None,
    'process.context_switches': ['involuntary', 'voluntary'],
    'process.runtime.gc_count': None,
    'process.open_file_descriptor.count': None,
    'process.cpu.time': ['user', 'system'],
    'process.cpu.utilization': None,
    'process.cpu.core_utilization': None,
    'process.memory.usage': None,
    'process.memory.virtual': None,
    'process.thread.count': None,
    'cpython.gc.collected_objects': None,
    'cpython.gc.collections': None,
    'cpython.gc.uncollectable_objects': None,
})
```

Each key here is a metric name. The values have different meanings for different metrics. For example, for `system.cpu.utilization`, the value is a list of CPU modes. So there will be a separate row for each CPU core saying what percentage of time it spent idle, another row for the time spent waiting for IO, etc. There are no fields to configure for `system.thread_count`, so the value is `None`.

For convenient customizability, the first dict argument is merged with the base. For example, if you want to collect disk read operations (but not writes) you can write:

- `logfire.instrument_system_metrics({'system.disk.operations': ['read']})` to collect that data in addition to the basic defaults.
- `logfire.instrument_system_metrics({'system.disk.operations': ['read']}, base='full')` to collect detailed data about all metrics, excluding disk write operations.
- `logfire.instrument_system_metrics({'system.disk.operations': ['read']}, base=None)` to collect only disk read operations and nothing else.

# asyncpg

The logfire.instrument_asyncpg() function can be used to instrument the [asyncpg](https://magicstack.github.io/asyncpg/) PostgreSQL driver with **Logfire**.

## Installation

Install `logfire` with the `asyncpg` extra:

```bash
pip install 'logfire[asyncpg]'
```

```bash
uv add 'logfire[asyncpg]'
```

```bash
poetry add 'logfire[asyncpg]'
```

## Usage

Let's setup a PostgreSQL database using Docker and run a Python script that connects to the database using asyncpg to demonstrate how to use **Logfire** with asyncpg.

### Setup a PostgreSQL Database Using Docker

First, we need to initialize a PostgreSQL database. This can be easily done using Docker with the following command:

```bash
docker run --name postgres \
    -e POSTGRES_USER=user \
    -e POSTGRES_PASSWORD=secret \
    -e POSTGRES_DB=database \
    -p 5432:5432 \
    -d postgres
```

This command will create a PostgreSQL database, that you can connect with `postgres://user:secret@0.0.0.0:5432/database`.

### Run the Python script

The following Python script connects to the PostgreSQL database and executes some SQL queries:

```py
import asyncio

import asyncpg
import logfire

logfire.configure()
logfire.instrument_asyncpg()


async def main():
    connection: asyncpg.Connection = await asyncpg.connect(
        user='user', password='secret', database='database', host='0.0.0.0', port=5432
    )

    with logfire.span('Create table and insert data'):
        await connection.execute('CREATE TABLE IF NOT EXISTS test (id serial PRIMARY KEY, num integer, data varchar);')

        # Insert some data
        await connection.execute('INSERT INTO test (num, data) VALUES ($1, $2)', 100, 'abc')
        await connection.execute('INSERT INTO test (num, data) VALUES ($1, $2)', 200, 'def')

        # Query the data
        for record in await connection.fetch('SELECT * FROM test'):
            logfire.info('Retrieved {record=}', record=record)


asyncio.run(main())
```

If you go to your project on the UI, you will see the span created by the script.

# BigQuery

The [Google Cloud BigQuery Python client library](https://pypi.org/project/google-cloud-bigquery/) is instrumented with OpenTelemetry out of the box, and all the extra dependencies are already included with **Logfire** by default, so you only need to call `logfire.configure()`.

What if I don't want to instrument BigQuery?

Since BigQuery automatically instruments itself, you need to opt-out of instrumentation if you don't want to use it.

To do it, you'll need to call logfire.suppress_scopes() with the scope `google.cloud.bigquery.opentelemetry_tracing`.

```python
import logfire

logfire.configure()
logfire.suppress_scopes("google.cloud.bigquery.opentelemetry_tracing")
```

Let's see an example:

```python
from google.cloud import bigquery

import logfire

logfire.configure()

client = bigquery.Client()
query = """
SELECT name
FROM `bigquery-public-data.usa_names.usa_1910_2013`
WHERE state = "TX"
LIMIT 100
"""
query_job = client.query(query)
print(list(query_job.result()))
```

You can find more information about the BigQuery Python client library in the [official documentation](https://cloud.google.com/python/docs/reference/bigquery/latest).

# MySQL

The logfire.instrument_mysql() method can be used to instrument the [MySQL Connector/Python](https://dev.mysql.com/doc/connector-python/en/) database driver with **Logfire**, creating a span for every query.

## Installation

Install `logfire` with the `mysql` extra:

```bash
pip install 'logfire[mysql]'
```

```bash
uv add 'logfire[mysql]'
```

```bash
poetry add 'logfire[mysql]'
```

## Usage

Let's setup a MySQL database using Docker and run a Python script that connects to the database using MySQL connector to demonstrate how to use **Logfire** with MySQL.

### Setup a MySQL Database Using Docker

First, we need to initialize a MySQL database. This can be easily done using Docker with the following command:

```bash
docker run --name mysql \
    -e MYSQL_ROOT_PASSWORD=secret \
    -e MYSQL_DATABASE=database \
    -e MYSQL_USER=user \
    -e MYSQL_PASSWORD=secret \
    -p 3306:3306 \
    -d mysql
```

The command above will create a MySQL database, that you can connect with `mysql://user:secret@0.0.0.0:3306/database`.

### Run the Python script

The following Python script connects to the MySQL database and executes some SQL queries:

```py
import logfire
import mysql.connector

logfire.configure()

# To instrument the whole module:
logfire.instrument_mysql()

connection = mysql.connector.connect(
    host="localhost",
    user="user",
    password="secret",
    database="database",
    port=3306,
    use_pure=True,
)

# Or instrument just the connection:
# connection = logfire.instrument_mysql(connection)

with logfire.span('Create table and insert data'), connection.cursor() as cursor:
    cursor.execute(
        'CREATE TABLE IF NOT EXISTS test (id INT AUTO_INCREMENT PRIMARY KEY, num integer, data varchar(255));'
    )

    # Insert some data
    cursor.execute('INSERT INTO test (num, data) VALUES (%s, %s)', (100, 'abc'))
    cursor.execute('INSERT INTO test (num, data) VALUES (%s, %s)', (200, 'def'))

    # Query the data
    cursor.execute('SELECT * FROM test')
    results = cursor.fetchall()  # Fetch all rows
    for row in results:
        print(row)  # Print each row
```

logfire.instrument_mysql() uses the **OpenTelemetry MySQL Instrumentation** package, which you can find more information about [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/mysql/mysql.html).

# Psycopg

The logfire.instrument_psycopg() function can be used to instrument the [Psycopg](https://www.psycopg.org/) PostgreSQL driver with **Logfire**. It works with both the `psycopg2` and `psycopg` (i.e. Psycopg 3) packages.

See the documentation for the [OpenTelemetry Psycopg Instrumentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/psycopg/psycopg.html) or the [OpenTelemetry Psycopg2 Instrumentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/psycopg2/psycopg2.html) package for more details.

## Installation

Install `logfire` with the `psycopg` extra:

```bash
pip install 'logfire[psycopg]'
```

```bash
uv add 'logfire[psycopg]'
```

```bash
poetry add 'logfire[psycopg]'
```

Or with the `psycopg2` extra:

```bash
pip install 'logfire[psycopg2]'
```

```bash
uv add 'logfire[psycopg2]'
```

```bash
poetry add 'logfire[psycopg2]'
```

## Usage

Let's setup a PostgreSQL database using Docker and run a Python script that connects to the database using Psycopg to demonstrate how to use **Logfire** with Psycopg.

### Setup a PostgreSQL Database Using Docker

First, we need to initialize a PostgreSQL database. This can be easily done using Docker with the following command:

```bash
docker run --rm --name postgres \
    -e POSTGRES_USER=user \
    -e POSTGRES_PASSWORD=secret \
    -e POSTGRES_DB=database \
    -p 5432:5432 \
    -d postgres
```

This command will create a PostgreSQL database, that you can connect with `postgres://user:secret@0.0.0.0:5432/database`.

### Run the Python script

The following Python script connects to the PostgreSQL database and executes some SQL queries:

```py
import logfire
import psycopg

logfire.configure()

# To instrument the whole module:
logfire.instrument_psycopg(psycopg)
# or
logfire.instrument_psycopg('psycopg')
# or just instrument whichever modules (psycopg and/or psycopg2) are installed:
logfire.instrument_psycopg()

connection = psycopg.connect(
    'dbname=database user=user password=secret host=0.0.0.0 port=5432'
)

# Or instrument just the connection:
logfire.instrument_psycopg(connection)

with logfire.span('Create table and insert data'), connection.cursor() as cursor:
    cursor.execute(
        'CREATE TABLE IF NOT EXISTS test (id serial PRIMARY KEY, num integer, data varchar);'
    )

    # Insert some data
    cursor.execute('INSERT INTO test (num, data) VALUES (%s, %s)', (100, 'abc'))
    cursor.execute('INSERT INTO test (num, data) VALUES (%s, %s)', (200, 'def'))

    # Query the data
    cursor.execute('SELECT * FROM test')
```

If you go to your project on the UI, you will see the span created by the script.

## SQL Commenter

To add SQL comments to the end of your queries to enrich your database logs with additional context, use the `enable_commenter` parameter:

```python
import logfire

logfire.configure()
logfire.instrument_psycopg(enable_commenter=True)
```

This can only be used when instrumenting the whole module, not individual connections.

By default the SQL comments will include values for the following keys:

- `db_driver`
- `dbapi_threadsafety`
- `dbapi_level`
- `libpq_version`
- `driver_paramstyle`
- `opentelemetry_values`

You can exclude any of these keys by passing a dictionary with those keys and the value `False` to `commenter_options`, e.g:

```python
import logfire

logfire.configure()
logfire.instrument_psycopg(enable_commenter=True, commenter_options={'db_driver': False, 'dbapi_threadsafety': False})
```

## API Reference

#### instrument_psycopg

```python
instrument_psycopg(
    conn_or_module: PsycopgConnection | Psycopg2Connection,
    **kwargs: Any,
) -> None
```

```python
instrument_psycopg(
    conn_or_module: (
        None | Literal["psycopg", "psycopg2"] | ModuleType
    ) = None,
    enable_commenter: bool = False,
    commenter_options: CommenterOptions | None = None,
    **kwargs: Any,
) -> None
```

```python
instrument_psycopg(
    conn_or_module: Any = None,
    enable_commenter: bool = False,
    commenter_options: CommenterOptions | None = None,
    **kwargs: Any,
) -> None
```

Instrument a `psycopg` connection or module so that spans are automatically created for each query.

Uses the OpenTelemetry instrumentation libraries for [`psycopg`](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/psycopg/psycopg.html) and [`psycopg2`](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/psycopg2/psycopg2.html).

**Parameters:**

| Name                | Type               | Description                                                                                                                                                                                                            | Default                                             |
| ------------------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| `conn_or_module`    | `Any`              | Can be: The psycopg (version 3) or psycopg2 module. The string 'psycopg' or 'psycopg2' to instrument the module. None (the default) to instrument whichever module(s) are installed. A psycopg or psycopg2 connection. | `None`                                              |
| `enable_commenter`  | `bool`             | Adds comments to SQL queries performed by Psycopg, so that database logs have additional context.                                                                                                                      | `False`                                             |
| `commenter_options` | \`CommenterOptions | None\`                                                                                                                                                                                                                 | Configure the tags to be added to the SQL comments. |
| `**kwargs`          | `Any`              | Additional keyword arguments to pass to the OpenTelemetry instrument methods, for future compatibility.                                                                                                                | `{}`                                                |

#### logfire.integrations.psycopg.CommenterOptions

Bases: `TypedDict`

The `commenter_options` parameter for `instrument_psycopg`.

##### db_driver

```python
db_driver: bool
```

Include the database driver name in the comment e.g. 'psycopg2'.

##### dbapi_threadsafety

```python
dbapi_threadsafety: bool
```

Include the DB-API threadsafety value in the comment.

##### dbapi_level

```python
dbapi_level: bool
```

Include the DB-API level in the comment.

##### libpq_version

```python
libpq_version: bool
```

Include the libpq version in the comment.

##### driver_paramstyle

```python
driver_paramstyle: bool
```

Include the driver paramstyle in the comment e.g. 'driver_paramstyle=pyformat'

##### opentelemetry_values

```python
opentelemetry_values: bool
```

Enabling this flag will add traceparent values to the comment.

The logfire.instrument_pymongo() method will create a span for every operation performed using your [PyMongo](https://pymongo.readthedocs.io/en/stable/) clients.

Also works with Motor... 🚗

This integration also works with [`motor`](https://motor.readthedocs.io/en/stable/), the asynchronous driver for MongoDB.

## Installation

Install `logfire` with the `pymongo` extra:

```bash
pip install 'logfire[pymongo]'
```

```bash
uv add 'logfire[pymongo]'
```

```bash
poetry add 'logfire[pymongo]'
```

## Usage

The following example demonstrates how to use **Logfire** with PyMongo.

### Run Mongo on Docker (Optional)

If you already have a MongoDB instance running, you can skip this step. Otherwise, you can start MongoDB using Docker with the following command:

```bash
docker run --name mongo -p 27017:27017 -d mongo:latest
```

### Run the Python script

The following script connects to a MongoDB database, inserts a document, and queries it:

```py
import logfire
from pymongo import MongoClient

logfire.configure()
logfire.instrument_pymongo()

client = MongoClient()
db = client["database"]
collection = db["collection"]
collection.insert_one({"name": "MongoDB"})
collection.find_one()
```

```py
import asyncio
import logfire
from motor.motor_asyncio import AsyncIOMotorClient

logfire.configure()
logfire.instrument_pymongo()

async def main():
    client = AsyncIOMotorClient()
    db = client["database"]
    collection = db["collection"]
    await collection.insert_one({"name": "MongoDB"})
    await collection.find_one()

asyncio.run(main())
```

Info

You can pass `capture_statement=True` to `logfire.instrument_pymongo()` to capture the queries.

By default, it is set to `False` to avoid capturing sensitive information.

The keyword arguments of `logfire.instrument_pymongo()` are passed to the `PymongoInstrumentor().instrument()` method of the OpenTelemetry pymongo Instrumentation package, read more about it [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/pymongo/pymongo.html).

## API Reference

#### instrument_pymongo

```python
instrument_pymongo(
    capture_statement: bool = False,
    request_hook: (
        Callable[[Span, CommandStartedEvent], None] | None
    ) = None,
    response_hook: (
        Callable[[Span, CommandSucceededEvent], None] | None
    ) = None,
    failed_hook: (
        Callable[[Span, CommandFailedEvent], None] | None
    ) = None,
    **kwargs: Any,
) -> None
```

Instrument the `pymongo` module so that spans are automatically created for each operation.

Uses the [OpenTelemetry pymongo Instrumentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/pymongo/pymongo.html) library, specifically `PymongoInstrumentor().instrument()`, to which it passes `**kwargs`.

**Parameters:**

| Name                | Type                                              | Description                                                                                            | Default                                                             |
| ------------------- | ------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------- |
| `capture_statement` | `bool`                                            | Set to True to capture the statement in the span attributes.                                           | `False`                                                             |
| `request_hook`      | \`Callable\[[Span, CommandStartedEvent], None\]   | None\`                                                                                                 | A function called when a command is sent to the server.             |
| `response_hook`     | \`Callable\[[Span, CommandSucceededEvent], None\] | None\`                                                                                                 | A function that is called when a command is successfully completed. |
| `failed_hook`       | \`Callable\[[Span, CommandFailedEvent], None\]    | None\`                                                                                                 | A function that is called when a command fails.                     |
| `**kwargs`          | `Any`                                             | Additional keyword arguments to pass to the OpenTelemetry instrument methods for future compatibility. | `{}`                                                                |

# Redis

The logfire.instrument_redis() method will create a span for every command executed by your [Redis](https://redis.readthedocs.io/en/stable/) clients.

## Installation

Install `logfire` with the `redis` extra:

```bash
pip install 'logfire[redis]'
```

```bash
uv add 'logfire[redis]'
```

```bash
poetry add 'logfire[redis]'
```

## Usage

Let's setup a container with Redis and run a Python script that connects to the Redis server to demonstrate how to use **Logfire** with Redis.

### Setup a Redis Server Using Docker

First, we need to initialize a Redis server. This can be easily done using Docker with the following command:

```bash
docker run --name redis -p 6379:6379 -d redis:latest
```

### Run the Python script

main.py

```py
import logfire
import redis


logfire.configure()
logfire.instrument_redis()

client = redis.StrictRedis(host="localhost", port=6379)
client.set("my-key", "my-value")

async def main():
    client = redis.asyncio.Redis(host="localhost", port=6379)
    await client.get("my-key")

if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

Info

You can pass `capture_statement=True` to `logfire.instrument_redis()` to capture the Redis command.

By default, it is set to `False` given that Redis commands can contain sensitive information.

The keyword arguments of `logfire.instrument_redis()` are passed to the `RedisInstrumentor().instrument()` method of the OpenTelemetry Redis Instrumentation package, read more about it [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/redis/redis.html).

## API Reference

#### instrument_redis

```python
instrument_redis(
    capture_statement: bool = False,
    request_hook: RequestHook | None = None,
    response_hook: ResponseHook | None = None,
    **kwargs: Any,
) -> None
```

Instrument the `redis` module so that spans are automatically created for each operation.

Uses the [OpenTelemetry Redis Instrumentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/redis/redis.html) library, specifically `RedisInstrumentor().instrument()`, to which it passes `**kwargs`.

**Parameters:**

| Name                | Type           | Description                                                                                            | Default                                                  |
| ------------------- | -------------- | ------------------------------------------------------------------------------------------------------ | -------------------------------------------------------- |
| `capture_statement` | `bool`         | Set to True to capture the statement in the span attributes.                                           | `False`                                                  |
| `request_hook`      | \`RequestHook  | None\`                                                                                                 | A function that is called before performing the request. |
| `response_hook`     | \`ResponseHook | None\`                                                                                                 | A function that is called after receiving the response.  |
| `**kwargs`          | `Any`          | Additional keyword arguments to pass to the OpenTelemetry instrument methods for future compatibility. | `{}`                                                     |

#### RequestHook

Bases: `Protocol`

A hook that is called before the request is sent.

##### __call__

```python
__call__(
    span: Span,
    instance: Connection,
    *args: Any,
    **kwargs: Any,
) -> None
```

Call the hook.

**Parameters:**

| Name       | Type         | Description                                           | Default    |
| ---------- | ------------ | ----------------------------------------------------- | ---------- |
| `span`     | `Span`       | The span that is being created.                       | *required* |
| `instance` | `Connection` | The connection instance.                              | *required* |
| `*args`    | `Any`        | The arguments that are passed to the command.         | `()`       |
| `**kwargs` | `Any`        | The keyword arguments that are passed to the command. | `{}`       |

#### ResponseHook

Bases: `Protocol`

A hook that is called after the response is received.

##### __call__

```python
__call__(
    span: Span, instance: Connection, response: Any
) -> None
```

Call the hook.

**Parameters:**

| Name       | Type         | Description                     | Default    |
| ---------- | ------------ | ------------------------------- | ---------- |
| `span`     | `Span`       | The span that is being created. | *required* |
| `instance` | `Connection` | The connection instance.        | *required* |
| `response` | `Any`        | The response that is received.  | *required* |

The logfire.instrument_sqlalchemy() method will create a span for every query executed by a [SQLAlchemy](https://www.sqlalchemy.org/) engine.

## Installation

Install `logfire` with the `sqlalchemy` extra:

```bash
pip install 'logfire[sqlalchemy]'
```

```bash
uv add 'logfire[sqlalchemy]'
```

```bash
poetry add 'logfire[sqlalchemy]'
```

## Usage

Let's see a minimal example below. You can run it with `python main.py`:

main.py

```py
import logfire
from sqlalchemy import create_engine

logfire.configure()

engine = create_engine("sqlite:///:memory:")
logfire.instrument_sqlalchemy(engine=engine)
```

The keyword arguments of `logfire.instrument_sqlalchemy()` are passed to the `SQLAlchemyInstrumentor().instrument()` method of the OpenTelemetry SQLAlchemy Instrumentation package, read more about it [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/sqlalchemy/sqlalchemy.html).

Tip

If you use [SQLModel](https://sqlmodel.tiangolo.com/), you can use the same `SQLAlchemyInstrumentor` to instrument it.

# SQLite3

The logfire.instrument_sqlite3() method can be used to instrument the [`sqlite3`](https://docs.python.org/3/library/sqlite3.html) standard library module. This will automatically create spans for each SQL query executed.

## Installation

Install `logfire` with the `sqlite3` extra:

```bash
pip install 'logfire[sqlite3]'
```

```bash
uv add 'logfire[sqlite3]'
```

```bash
poetry add 'logfire[sqlite3]'
```

## Usage

We can use the sqlite in-memory database to demonstrate the usage of the logfire.instrument_sqlite3() method.

You can either instrument the `sqlite3` module or instrument a specific connection.

### Instrument the module

Here's an example of instrumenting the [`sqlite3`](https://docs.python.org/3/library/sqlite3.html) module:

main.py

```py
import sqlite3

import logfire

logfire.configure()
logfire.instrument_sqlite3()

with sqlite3.connect(':memory:') as connection:
    cursor = connection.cursor()

    cursor.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)')
    cursor.execute("INSERT INTO users (name) VALUES ('Alice')")

    cursor.execute('SELECT * FROM users')
    print(cursor.fetchall())
    # > [(1, 'Alice')]
```

### Instrument a connection

As mentioned, you can also instrument a specific connection. Here's an example:

main.py

```py
import sqlite3

import logfire

logfire.configure()

with sqlite3.connect(':memory:') as connection:
    connection = logfire.instrument_sqlite3(connection)
    cursor = connection.cursor()

    cursor.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)')
    cursor.execute("INSERT INTO users (name) VALUES ('Alice')")

    cursor.execute('SELECT * FROM users')
    print(cursor.fetchall())
    # > [(1, 'Alice')]
```

Avoid using `execute` from `sqlite3.Connection`

The execute method from Connection is not instrumented!

You should use the execute method from the Cursor object instead.

See [opentelemetry-python-contrib#3082](https://github.com/open-telemetry/opentelemetry-python-contrib/issues/3082) for more information.

logfire.instrument_sqlite3() uses the **OpenTelemetry SQLite3 Instrumentation** package, which you can find more information about [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/sqlite3/sqlite3.html).

# Airflow

**Airflow** has a native OpenTelemetry integration for [traces](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/traces.html) and [metrics](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#setup-opentelemetry), which involves creating an exporter internally that sends data to the configured backend.

To configure **Airflow** to send data to **Logfire**, you'll need to:

- Set the `OTEL_EXPORTER_OTLP_HEADERS` environment variable.
- Configure the `otel_*` settings in the `airflow.cfg` file.

Warning

If your `apache-airflow` is older than 2.10.4, this section will not work for you.

In that case, go to the [Airflow with OpenTelemetry Collector] section.

```bash
export OTEL_EXPORTER_OTLP_HEADERS="Authorization=${LOGFIRE_TOKEN}"
```

Where `${LOGFIRE_TOKEN}` is your [**Logfire** write token](../../../how-to-guides/create-write-tokens/).

airflow.cfg

```ini
[metrics]
otel_on = True
otel_host = logfire-api.pydantic.dev
otel_port = 443
otel_prefix = airflow
otel_interval_milliseconds = 30000  # The interval between exports, defaults to 60000
otel_ssl_active = True

[traces]
otel_on = True
otel_host = logfire-api.pydantic.dev
otel_port = 443
otel_prefix = airflow
otel_ssl_active = True
otel_task_log_event = True
```

For more details, check airflow [traces](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/traces.html) and [metrics](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#setup-opentelemetry) documentation.

## Airflow with OpenTelemetry Collector

If your `apache-airflow` is older than 2.10.4, it means that you'll not be able to set the `OTEL_EXPORTER_OTLP_HEADERS` environment variable.

Why can't I set the `OTEL_EXPORTER_OTLP_HEADERS` environment variable?

This was a bug that was fixed in the 2.10.4 version of `apache-airflow`.

The **Logfire** team fixed it in [apache/airflow#44346](https://github.com/apache/airflow/pull/44346).

In that case, you'll need to set up an [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) to send data to **Logfire**.

### OpenTelemetry Collector

The OpenTelemetry Collector is a vendor-agnostic agent that can collect traces and metrics from your applications and send them to various backends.

In this case, we are interested in sending data to **Logfire**.

Note

Using a collector is an option when you are already sending data to a backend, but you want to migrate to **Logfire**.

Then you can configure the collector to send data to **Logfire**, as well as your current backend. This way you can compare the data and ensure that everything is working as expected. Cool, right?

You can check the [OpenTelemetry Collector installation](https://opentelemetry.io/docs/collector/installation/) guide to set it up, but I'll help you with the configuration.

otel-collector-config.yaml

```yaml
receivers:  # (1)!
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

exporters:  # (2)!
  debug:  # (3)!
  otlphttp:
    endpoint: https://logfire-api.pydantic.dev/
    compression: gzip
    headers:
      Authorization: "Bearer ${env:LOGFIRE_TOKEN}"  # (4)!

processors:
  batch:
    timeout: 1s
    send_batch_size: 32768

extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

service:  # (5)!
  extensions: [health_check]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [debug, otlphttp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [debug, otlphttp]
```

1. Define the receivers to collect data from your applications.

   See more about it on the [OpenTelemetry Collector Receiver](https://opentelemetry.io/docs/collector/configuration/#receivers) section.

1. Define the exporters to send data to **Logfire**.

   The `otlphttp` exporter is used to send data to **Logfire**.

1. The `debug` exporter is used to send data to the console, so you can see what's being sent.

   This is useful for debugging purposes, but it can be removed in production.

1. Set the `Authorization` header to send data to **Logfire**.

   The `{env:LOGFIRE_TOKEN}` will be replaced by the environment variable.

1. Define the service to configure the pipelines.

   The `traces` pipeline is used to send trace data, and the `metrics` pipeline is used to send metrics data.

### Airflow configuration

To configure Airflow to send data to the OpenTelemetry Collector, we'll need the following settings:

- [Metrics Configuration](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#setup-opentelemetry)
- [Traces Configuration](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/traces.html)

On your `airflow.cfg` file, add the following configuration:

airflow.cfg

```ini
[metrics]
otel_on = True
otel_host = localhost
otel_port = 4318
otel_prefix = airflow
otel_interval_milliseconds = 30000  # The interval between exports, defaults to 60000
otel_ssl_active = False

[traces]
otel_on = True
otel_host = localhost
otel_port = 4318
otel_prefix = airflow
otel_ssl_active = False
otel_task_log_event = True
```

# Celery

The logfire.instrument_celery() method will create a span for every task executed by your Celery workers.

The integration also supports the [Celery beat](https://docs.celeryq.dev/en/latest/userguide/periodic-tasks.html).

## Installation

Install `logfire` with the `celery` extra:

```bash
pip install 'logfire[celery]'
```

```bash
uv add 'logfire[celery]'
```

```bash
poetry add 'logfire[celery]'
```

## Celery Worker

Info

The broker you use doesn't matter for the Celery instrumentation.

Any [broker supported by Celery](https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html) will work.

For our example, we'll use [redis](https://redis.io/). You can run it with Docker:

```bash
docker run --rm -d -p 6379:6379 redis
```

Below we have a minimal example using Celery. You can run it with `celery -A tasks worker --loglevel=info`:

tasks.py

```py
import logfire
from celery import Celery
from celery.signals import worker_init


@worker_init.connect()  # (1)!
def init_worker(*args, **kwargs):
    logfire.configure(service_name="worker")  # (2)!
    logfire.instrument_celery()

app = Celery("tasks", broker="redis://localhost:6379/0")  # (3)!

@app.task
def add(x: int, y: int):
    return x + y

add.delay(42, 50)  # (4)!
```

1. Celery implements different signals that you can use to run code at specific points in the application lifecycle. You can see more about the Celery signals [here](https://docs.celeryq.dev/en/latest/userguide/signals.html).
1. Use a `service_name` to identify the service that is sending the spans.
1. Install `redis` with `pip install redis`.
1. Trigger the task synchronously. On your application, you probably want to use `app.send_task("tasks.add", args=[42, 50])`. Which will send the task to the broker and return immediately.

## Celery Beat

As said before, it's also possible that you have periodic tasks scheduled with **Celery beat**.

Let's add the beat to the previous example:

tasks.py

```py
import logfire
from celery import Celery
from celery.signals import worker_init, beat_init


@worker_init.connect()
def init_worker(*args, **kwargs):
    logfire.configure(service_name="worker")
    logfire.instrument_celery()

@beat_init.connect()  # (1)!
def init_beat(*args, **kwargs):
    logfire.configure(service_name="beat")  # (2)!
    logfire.instrument_celery()

app = Celery("tasks", broker="redis://localhost:6379/0")
app.conf.beat_schedule = {  # (3)!
    "add-every-30-seconds": {
        "task": "tasks.add",
        "schedule": 30.0,
        "args": (16, 16),
    },
}

@app.task
def add(x: int, y: int):
    return x + y
```

1. The `beat_init` signal is emitted when the beat process starts.
1. Use a different `service_name` to identify the beat process.
1. Add a task to the beat schedule. See more about the beat schedule [here](https://docs.celeryq.dev/en/latest/userguide/periodic-tasks.html#entries).

The code above will schedule the `add` task to run every 30 seconds with the arguments `16` and `16`.

To run the beat, you can use the following command:

```bash
celery -A tasks beat --loglevel=info
```

The keyword arguments of logfire.instrument_celery() are passed to the CeleryInstrumentor().instrument() method.

# FastStream

To instrument [FastStream](https://faststream.airt.ai/latest/) with OpenTelemetry, you need to:

1. Call `logfire.configure()`.
1. Add the needed middleware according to your broker.

Let's see an example:

main.py

```python
from faststream import FastStream
from faststream.redis import RedisBroker
from faststream.redis.opentelemetry import RedisTelemetryMiddleware

import logfire

logfire.configure()

broker = RedisBroker(middlewares=(RedisTelemetryMiddleware(),))

app = FastStream(broker)


@broker.subscriber("test-channel")
@broker.publisher("another-channel")
async def handle():
    return "Hi!"


@broker.subscriber("another-channel")
async def handle_next(msg: str):
    assert msg == "Hi!"


@app.after_startup
async def test():
    await broker.publish("", channel="test-channel")
```

Since we are using Redis, we added the RedisTelemetryMiddleware to the broker. In case you use a different broker, you need to add the corresponding middleware.

See more about FastStream OpenTelemetry integration in [their documentation](https://faststream.airt.ai/latest/getting-started/opentelemetry/#faststream-tracing).

# AIOHTTP Client

[AIOHTTP](https://docs.aiohttp.org/en/stable/) is an asynchronous HTTP client/server framework for asyncio and Python.

The logfire.instrument_aiohttp_client() method will create a span for every request made by your AIOHTTP clients.

For AIOHTTP server instrumentation, see [here](../../web-frameworks/aiohttp/).

## Installation

Install `logfire` with the `aiohttp-client` extra:

```bash
pip install 'logfire[aiohttp-client]'
```

```bash
uv add 'logfire[aiohttp-client]'
```

```bash
poetry add 'logfire[aiohttp-client]'
```

## Usage

Let's see a minimal example below. You can run it with `python main.py`:

main.py

```py
import logfire
import aiohttp


logfire.configure()
logfire.instrument_aiohttp_client()


async def main():
    async with aiohttp.ClientSession() as session:
        await session.get("https://httpbin.org/get")


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

The keyword arguments of `logfire.instrument_aiohttp_client()` are passed to the `AioHttpClientInstrumentor().instrument()` method of the OpenTelemetry aiohttp client Instrumentation package, read more about it [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/aiohttp_client/aiohttp_client.html).

## Hiding sensitive URL parameters

The `url_filter` keyword argument can be used to modify the URL that's recorded in spans. Here's an example of how to use this to redact query parameters:

```python
from yarl import URL

def mask_url(url: URL) -> str:
    sensitive_keys = {"username", "password", "token", "api_key", "api_secret", "apikey"}
    masked_query = {key: "*****" if key in sensitive_keys else value for key, value in url.query.items()}
    return str(url.with_query(masked_query))

logfire.instrument_aiohttp_client(url_filter=mask_url)
```

# HTTPX

The logfire.instrument_httpx() method can be used to instrument [HTTPX](https://www.python-httpx.org/) with **Logfire**.

## Installation

Install `logfire` with the `httpx` extra:

```bash
pip install 'logfire[httpx]'
```

```bash
uv add 'logfire[httpx]'
```

```bash
poetry add 'logfire[httpx]'
```

## Usage

Let's see a minimal example below. You can run it with `python main.py`:

main.py

```py
import asyncio

import httpx
import logfire

logfire.configure()
logfire.instrument_httpx()

url = "https://httpbin.org/get"

with httpx.Client() as client:
    client.get(url)


async def main():
    async with httpx.AsyncClient() as client:
        await client.get(url)


asyncio.run(main())
```

main.py

```py
import asyncio

import httpx
import logfire

logfire.configure()

url = 'https://httpbin.org/get'

with httpx.Client() as client:
    logfire.instrument_httpx(client)
    client.get(url)


async def main():
    async with httpx.AsyncClient() as client:
        logfire.instrument_httpx(client)
        await client.get(url)


asyncio.run(main())
```

logfire.instrument_httpx() uses the **OpenTelemetry HTTPX Instrumentation** package, which you can find more information about [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/httpx/httpx.html).

## Configuration

The `logfire.instrument_httpx()` method accepts various parameters to configure the instrumentation.

### Capture Everything

You can capture all information (headers and bodies) by setting the `capture_all` parameter to `True`.

```py
import httpx
import logfire

logfire.configure()
logfire.instrument_httpx(capture_all=True)
client.post("https://httpbin.org/post", json={"key": "value"})
```

### Capture HTTP Headers

By default, **Logfire** doesn't capture HTTP headers. You can enable it by setting the `capture_headers` parameter to `True`.

```py
import httpx
import logfire

logfire.configure()
logfire.instrument_httpx(capture_headers=True)

client = httpx.Client()
client.get("https://httpbin.org/get")
```

#### Capture Only Request Headers

Instead of capturing both request and response headers, you can create a request hook to capture only the request headers:

```py
import httpx
import logfire
from logfire.integrations.httpx import RequestInfo
from opentelemetry.trace import Span


def capture_request_headers(span: Span, request: RequestInfo):
    headers = request.headers
    span.set_attributes(
        {
            f'http.request.header.{header_name}': headers.get_list(header_name)
            for header_name in headers.keys()
        }
    )


logfire.configure()
logfire.instrument_httpx(request_hook=capture_request_headers)

client = httpx.Client()
client.get("https://httpbin.org/get")
```

#### Capture Only Response Headers

Similarly, you can create a response hook to capture only the response headers:

```py
import httpx
import logfire
from opentelemetry.trace import Span
from logfire.integrations.httpx import RequestInfo, ResponseInfo


def capture_response_headers(span: Span, request: RequestInfo, response: ResponseInfo):
    headers = response.headers
    span.set_attributes(
        {f'http.response.header.{header_name}': headers.get_list(header_name)
        for header_name in headers.keys()}
    )


logfire.configure()
logfire.instrument_httpx(response_hook=capture_response_headers)

client = httpx.Client()
client.get('https://httpbin.org/get')
```

You can also use the hooks to filter headers or modify them before capturing them.

### Capture HTTP Bodies

By default, **Logfire** doesn't capture HTTP bodies.

To capture bodies, you can set the `capture_request_body` and `capture_response_body` parameters to `True`.

```py
import httpx
import logfire

logfire.configure()
logfire.instrument_httpx(
    capture_request_body=True,
    capture_response_body=True,
)

client = httpx.Client()
client.post("https://httpbin.org/post", data="Hello, World!")
```

# Requests

The logfire.instrument_requests() method can be used to instrument [`requests`](https://docs.python-requests.org/en/master/) with **Logfire**.

## Installation

Install `logfire` with the `requests` extra:

```bash
pip install 'logfire[requests]'
```

```bash
uv add 'logfire[requests]'
```

```bash
poetry add 'logfire[requests]'
```

## Usage

main.py

```py
import logfire
import requests

logfire.configure()
logfire.instrument_requests()

requests.get("https://httpbin.org/get")
```

logfire.instrument_requests() uses the **OpenTelemetry requests Instrumentation** package, which you can find more information about [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/requests/requests.html).

# JavaScript

Logfire offers first-class integration for the most popular JavaScript frameworks and runtimes. Where appropriate (like Deno or Next.js), integration happens through the framework/runtime's built-in OTel mechanism.

In addition to the instrumentation itself, we ship an `@pydantic/logfire-api` package that mirrors the Python `logfire` package API for creating spans and reporting exceptions.

## Browser

The `@pydantic/logfire-browser` package wraps [the OpenTelemetry browser tracing](https://opentelemetry.io/docs/languages/js/getting-started/browser/) with some sensible defaults and provides a simple API for creating spans and reporting exceptions.

Refer to the [browser documentation section](browser/) for more details.

## Next.js

Next.js is a popular React framework for building server-rendered applications. It offers a first-party OTel integration through `@vercel/otel`, which is fully compatible with Logfire. In addition to that, the client-side can be instrumented with the `@pydantic/logfire-browser` package.

Refer to the [Next.js documentation section](nextjs/) for more details.

## Cloudflare

Instrumenting Cloudflare Workers is straightforward with Logfire. You can use the `@pydantic/logfire-cf-workers` package to instrument your worker handlers, and the `@pydantic/logfire-api` package to send logs and spans.

Refer to the [Cloudflare Workers documentation section](cloudflare/) for more details.

## Express

To instrument an Express app, use the `logfire` package, optionally using `dotenv` for reading environment variables from a file. Refer to the [Express documentation section](express/) for more details.

## Node.js

Generic Node.js scripts can be instrumented using the `logfire` package. Refer to the [Node.js documentation section](node/) for more details.

## Deno

Deno has built-in support for OpenTelemetry. You can configure OTel export to Logfire using environment variables. Refer to the [Deno documentation section](deno/) for more details.

# Browser

The `@pydantic/logfire-browser` NPM package wraps [OpenTelemetry browser tracing](https://opentelemetry.io/docs/languages/js/getting-started/browser/) with sensible defaults and provides a simple API for creating spans and reporting exceptions.

Info

Logfire does not directly expose an endpoint suitable for sending traces from the browser, as this would make your write token publicly accessible. To send traces from the browser, you must create a proxy in your app that **forwards requests from your browser instrumentation to Logfire** while adding the `Authorization` header. Check the [Next.js middleware example implementation](https://github.com/pydantic/logfire-js/blob/main/examples/nextjs-client-side-instrumentation/middleware.ts#L8) for more details.

## Simple Usage

```ts
import { getWebAutoInstrumentations } from "@opentelemetry/auto-instrumentations-web";
import * as logfire from '@pydantic/logfire-browser';

// Set the path to your traces proxy endpoint - assuming it's hosted at `/client-traces`, same domain.
const url = new URL(window.location.href);
url.pathname = "/client-traces";

logfire.configure({
  traceUrl: url.toString(),
  serviceName: 'my-service',
  serviceVersion: '0.1.0',
  // The instrumentations to use
  // https://www.npmjs.com/package/@opentelemetry/auto-instrumentations-web - for more options and configuration
  instrumentations: [
    getWebAutoInstrumentations()
  ],
  // This outputs details about the generated spans in the browser console, use only in development and for troubleshooting.
  diagLogLevel: logfire.DiagLogLevel.ALL
})
```

Info

the `@pydantic/logfire-browser` package is bundled as an ESM module, that's supported by all modern frameworks and browsers.

Note that if you're using an SSR/SSG framework, you should ensure that the code above runs only in the browser runtime. [A dedicated example for Next.js](https://github.com/pydantic/logfire-js/tree/main/examples/nextjs-client-side-instrumentation) is available.

# Cloudflare

To instrument your Cloudflare Workers and send spans to Logfire, install the `@pydantic/logfire-cf-workers` and `@pydantic/logfire-api` NPM packages:

```sh
npm install @pydantic/logfire-cf-workers @pydantic/logfire-api
```

Next, add the Node.js compatibility flag to your Wrangler configuration:

- For `wrangler.toml`: `compatibility_flags = [ "nodejs_compat" ]`
- For `wrangler.jsonc`: `"compatibility_flags": ["nodejs_compat"]`

Add your [Logfire write token](https://logfire.pydantic.dev/docs/how-to-guides/create-write-tokens/) to your `.dev.vars` file:

```sh
LOGFIRE_TOKEN=your-write-token
LOGFIRE_ENVIRONMENT=development
```

The `LOGFIRE_ENVIRONMENT` variable is optional and specifies the environment name for your service.

For production deployment, refer to the [Cloudflare documentation on managing and deploying secrets](https://developers.cloudflare.com/workers/configuration/secrets/). You can set secrets using the Wrangler CLI:

```sh
npx wrangler secret put LOGFIRE_TOKEN
```

Finally, wrap your handler with the instrumentation. The `instrument` function will automatically configure Logfire using your environment variables:

```ts
import * as logfire from "@pydantic/logfire-api";
import { instrument } from "@pydantic/logfire-cf-workers";

const handler = {
  async fetch(): Promise<Response> {
    logfire.info("info span from inside the worker body");
    return new Response("hello world!");
  },
} satisfies ExportedHandler;

export default instrument(handler, {
    service: {
        name: 'my-cloudflare-worker',
        namespace: '',
        version: '1.0.0',
    },
});
```

A complete working example is available in the [examples/cf-worker](https://github.com/pydantic/logfire-js/tree/main/examples/cf-worker) directory.

Info

If you're testing your Worker with Vitest, add the following configuration to your `vitest.config.mts` to ensure proper module loading:

```ts
export default defineWorkersConfig({
  test: {
    deps: {
      optimizer: {
        ssr: {
          enabled: true,
          include: ['@pydantic/logfire-cf-workers'],
        },
      },
    },
    poolOptions: {
      workers: {
        // ...
      },
    },
  },
});
```

Third-party integrations

Third-party integrations are not officially supported by **Logfire**.

They are maintained by the community and may not be as reliable as the integrations developed by **Logfire**.

# Deno

Since v2.2, Deno has [built-in support for OpenTelemetry](https://docs.deno.com/runtime/fundamentals/open_telemetry/). The [logfire-js examples directory includes a `Hello World` example](https://github.com/pydantic/logfire-js/tree/main/examples/deno-project) that shows how to configure Deno to export OpenTelemetry data to Logfire through environment variables.

You can also use the Logfire API package to create manual spans. Install the `@pydantic/logfire-api` NPM package and call the appropriate methods in your code.

# Express

Instrumenting an Express application with Logfire is straightforward. You can use the `logfire` package to set up logging and monitoring for your Express routes.

Info

The [@opentelemetry/instrumentation-express](https://www.npmjs.com/package/@opentelemetry/instrumentation-express) NPM package lists the following Express version requirements:

> `express` version >=4.0.0 \<5

app.ts

```ts
import express, type { Express } from 'express';

const PORT: number = parseInt(process.env.PORT || '8080');
const app: Express = express();

function getRandomNumber(min: number, max: number) {
  return Math.floor(Math.random() * (max - min + 1) + min);
}

app.get('/rolldice', (req, res) => {
  res.send(getRandomNumber(1, 6).toString());
});

app.listen(PORT, () => {
  console.log(`Listening for requests on http://localhost:${PORT}`);
});
```

To get started, install the `logfire` and `dotenv` NPM packages. This will allow you to keep your Logfire write token in a `.env` file:

```sh
npm install logfire dotenv
```

Add your token to the `.env` file:

.env

```sh
LOGFIRE_TOKEN=your-write-token
```

Then, create an `instrumentation.ts` file to set up the instrumentation. The `logfire` package includes a `configure` function that simplifies the setup:

instrumentation.ts

```ts
import * as logfire from "logfire";
import "dotenv/config";

logfire.configure();
```

The `logfire.configure` call should happen before importing the actual Express module, so your NPM start script should look like this in `package.json`. Note that we use `npx ts-node` to run the TypeScript code directly:

package.json

```json
"scripts": {
  "start": "npx ts-node --require ./instrumentation.ts app.ts"
```

A working example can be found in the [examples/express](https://github.com/pydantic/logfire-js/tree/main/examples/express) directory of the `pydantic/logfire-js` repository.

# Next.js

## Server-side Instrumentation

Vercel provides a comprehensive OpenTelemetry integration through the `@vercel/otel` package. After following [Vercel's integration instructions](https://vercel.com/docs/otel), add the following environment variables to your project:

```sh
OTEL_EXPORTER_OTLP_ENDPOINT=https://logfire-api.pydantic.dev
OTEL_EXPORTER_OTLP_HEADERS='Authorization=your-write-token'
```

This configuration directs the OpenTelemetry instrumentation to send data to Logfire.

Note

Vercel production deployments use a caching mechanism that may prevent configuration changes from taking effect immediately or prevent spans from being reported. If you don't see spans appearing in Logfire, you can [clear the data cache for your project](https://vercel.com/docs/data-cache/manage-data-cache).

Optionally, you can use the Logfire API package to create manual spans. Install the `@pydantic/logfire-api` NPM package and call the appropriate methods from your server-side code:

```tsx
import * as logfire from "@pydantic/logfire-api";

export default async function Home() {
  return logfire.span("A warning span", {}, {
    level: logfire.Level.Warning,
  }, async (span) => {
    logfire.info("Nested info span");
      // Call span.end() to ensure the span is properly reported
    span.end();
    return <div>Hello</div>;
  });
}
```

A working example can be found in the [examples/nextjs](https://github.com/pydantic/logfire-js/tree/main/examples/nextjs) directory.

## Client-side Instrumentation

Client-side instrumentation can be implemented using the `@pydantic/logfire-browser` package. To set it up, you need to complete the following steps:

- Add a [proxy to the Logfire traces endpoint in `middleware.ts`](https://github.com/pydantic/logfire-js/blob/main/examples/nextjs-client-side-instrumentation/middleware.ts) to prevent exposing your Logfire write token.
- Wrap the browser instrumentation in [a client-only React component](https://github.com/pydantic/logfire-js/blob/main/examples/nextjs-client-side-instrumentation/app/components/ClientInstrumentationProvider.tsx). Use `next/dynamic` to ensure the component renders only in the browser ([see example](https://github.com/pydantic/logfire-js/blob/main/examples/nextjs-client-side-instrumentation/app/page.tsx#L5-L8)).

A complete working example can be found in the [examples/nextjs-client-side-instrumentation](https://github.com/pydantic/logfire-js/tree/main/examples/nextjs-client-side-instrumentation) directory.

# Node.js

Using Logfire in your Node.js script is straightforward. You need to [get a write token](https://logfire.pydantic.dev/docs/how-to-guides/create-write-tokens/), install the package, configure it, and use the provided API.

Let's create an empty project:

```sh
mkdir test-logfire-js
cd test-logfire-js
npm init -y es6 # This creates a package.json with `type: module` for ES6 support
npm install logfire
```

Then, create the following `hello.js` script in the directory:

```js
import * as logfire from "logfire";

logfire.configure({
  token: "your-write-token",
  serviceName: "example-node-script",
  serviceVersion: "1.0.0",
});

logfire.info("Hello from Node.js", {
  "attribute-key": "attribute-value",
}, {
  tags: ["example", "example2"],
});
```

Run the script with `node hello.js`, and you should see the span appear in the live view of your Logfire project.

A working example can be found in the [examples/node](https://github.com/pydantic/logfire-js/tree/main/examples/node) directory of the logfire-js repository.

# Anthropic

**Logfire** supports instrumenting calls to [Anthropic](https://github.com/anthropics/anthropic-sdk-python) with the logfire.instrument_anthropic() method, for example:

```python
import anthropic
import logfire

client = anthropic.Anthropic()

logfire.configure()
logfire.instrument_anthropic()  # instrument all Anthropic clients globally
# or logfire.instrument_anthropic(client) to instrument a specific client instance

response = client.messages.create(
    max_tokens=1000,
    model='claude-3-haiku-20240307',
    system='You are a helpful assistant.',
    messages=[{'role': 'user', 'content': 'Please write me a limerick about Python logging.'}],
)
print(response.content[0].text)
```

With that you get:

- a span around the call to Anthropic which records duration and captures any exceptions that might occur
- Human-readable display of the conversation with the agent
- details of the response, including the number of tokens used

Anthropic span and conversation

Span arguments including response details

## Methods covered

The following Anthropic methods are covered:

- [`client.messages.create`](https://docs.anthropic.com/en/api/messages)
- [`client.messages.stream`](https://docs.anthropic.com/en/api/messages-streaming)
- [`client.beta.tools.messages.create`](https://docs.anthropic.com/en/docs/tool-use)

All methods are covered with both `anthropic.Anthropic` and `anthropic.AsyncAnthropic`.

## Streaming Responses

When instrumenting streaming responses, Logfire creates two spans — one around the initial request and one around the streamed response.

Here we also use Rich's Live and Markdown types to render the response in the terminal in real-time.

```python
import anthropic
import logfire
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown

client = anthropic.AsyncAnthropic()
logfire.configure()
logfire.instrument_anthropic(client)


async def main():
    console = Console()
    with logfire.span('Asking Anthropic to write some code'):
        response = client.messages.stream(
            max_tokens=1000,
            model='claude-3-haiku-20240307',
            system='Reply in markdown one.',
            messages=[{'role': 'user', 'content': 'Write Python to show a tree of files 🤞.'}],
        )
        content = ''
        with Live('', refresh_per_second=15, console=console) as live:
            async with response as stream:
                async for chunk in stream:
                    if chunk.type == 'content_block_delta':
                        content += chunk.delta.text
                        live.update(Markdown(content))


if __name__ == '__main__':
    import asyncio

    asyncio.run(main())
```

Shows up like this in Logfire:

Anthropic streaming response

## Amazon Bedrock

You can also log Anthropic LLM calls to Amazon Bedrock using the `AmazonBedrock` and `AsyncAmazonBedrock` clients.

```python
import anthropic
import logfire

client = anthropic.AnthropicBedrock(
    aws_region='us-east-1',
    aws_access_key='access-key',
    aws_secret_key='secret-key',
)

logfire.configure()
logfire.instrument_anthropic(client)
```

# LangChain

[LangChain](https://www.langchain.com/) (and thus [LangGraph](https://www.langchain.com/langgraph)) has [built-in OpenTelemetry tracing via Langsmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel) which you can use with **Logfire**. It's enabled by these two environment variables:

```text
LANGSMITH_OTEL_ENABLED=true
LANGSMITH_TRACING=true
```

Here's a complete example using LangGraph:

```python
import os

import logfire

# These environment variables need to be set before importing langchain or langgraph
os.environ['LANGSMITH_OTEL_ENABLED'] = 'true'
os.environ['LANGSMITH_TRACING'] = 'true'

from langgraph.prebuilt import create_react_agent

logfire.configure()


def add(a: float, b: float) -> float:
    """Add two numbers."""
    return a + b


math_agent = create_react_agent('openai:gpt-4o', tools=[add], name='math_agent')

result = math_agent.invoke({'messages': [{'role': 'user', 'content': "what's 123 + 456?"}]})
print(result['messages'][-1].content)
```

The resulting trace looks like this in Logfire:

Third-party integrations

Third-party integrations are not officially supported by **Logfire**.

They are maintained by the community and may not be as reliable as the integrations developed by **Logfire**.

LiteLLM allows you to call over 100 Large Language Models (LLMs) using the same input/output format. It also supports Logfire for logging and monitoring.

To integrate Logfire with LiteLLM:

1. Set the `LOGFIRE_TOKEN` environment variable.
1. Add `logfire` to the callbacks of LiteLLM.

For more details, [check the official LiteLLM documentation.](https://docs.litellm.ai/docs/observability/logfire_integration)

The way we recommend instrumenting **LlamaIndex** is to use the OpenTelemetry specific instrumentation provided by [OpenLLMetry](https://www.traceloop.com/openllmetry): [`opentelemetry-instrumentation-llamaindex`](https://github.com/traceloop/openllmetry/tree/main/packages/opentelemetry-instrumentation-llamaindex).

## Installation

Install the [`opentelemetry-instrumentation-llamaindex`](https://github.com/traceloop/openllmetry/tree/main/packages/opentelemetry-instrumentation-llamaindex) package:

```bash
pip install opentelemetry-instrumentation-llamaindex
```

## Usage

Let's use LlamaIndex with OpenAI as an example.

You only need to include the `LlamaIndexInstrumentor` and call its `instrument` method to enable the instrumentation.

````python
import logfire
from llama_index.core import VectorStoreIndex
from llama_index.llms.openai import OpenAI
from llama_index.readers.web import SimpleWebPageReader
from opentelemetry.instrumentation.llamaindex import LlamaIndexInstrumentor

logfire.configure()
LlamaIndexInstrumentor().instrument()

# URL for Pydantic's main concepts page
url = 'https://docs.pydantic.dev/latest/concepts/models/'

# Load the webpage
documents = SimpleWebPageReader(html_to_text=True).load_data([url])

# Create index from documents
index = VectorStoreIndex.from_documents(documents)

# Initialize the LLM
query_engine = index.as_query_engine(llm=OpenAI())

# Get response
response = query_engine.query('Can I use RootModels without subclassing them? Show me an example.')
print(str(response))
"""
Yes, you can use RootModels without subclassing them. Here is an example:

```python
from pydantic import RootModel

Pets = RootModel[list[str]]

my_pets = Pets.model_validate(['dog', 'cat'])

print(my_pets[0])
#> dog
print([pet for pet in my_pets])
#> ['dog', 'cat']
"""
````

## Instrument the underlying LLM

The `LlamaIndexInstrumentor` will specifically instrument the LlamaIndex library, not the LLM itself. If you want to instrument the LLM, you'll need to instrument it separately:

- For **OpenAI**, you can use the OpenAI, you can check the [OpenAI documentation](../openai/).
- For **Anthropic**, you can check the [Anthropic documentation](../anthropic/).

If you are using a different LLM, and you can't find a way to instrument it, or you need any help, feel free to [reach out to us](../../../help/)!

Third-party integrations

Third-party integrations are not officially supported by **Logfire**.

They are maintained by the community and may not be as reliable as the integrations developed by **Logfire**.

[Magentic](https://github.com/jackmpcollins/magentic) is a lightweight library for working with structured output from LLMs, built around standard python type annotations and **Pydantic**. It integrates with **Logfire** to provide observability into prompt-templating, retries, tool/function call execution, and [other features](https://magentic.dev/#features).

Magentic instrumentation requires no additional setup beyond configuring **Logfire** itself. You might also want to enable the [OpenAI](../openai/) and/or [Anthropic](../anthropic/) integrations.

```python
from typing import Annotated

import logfire
from magentic import chatprompt, OpenaiChatModel, SystemMessage, UserMessage
from pydantic import BaseModel, Field
from pydantic.functional_validators import AfterValidator

logfire.configure()
logfire.instrument_openai()


def assert_upper(value: str) -> str:
    if not value.isupper():
        raise ValueError('Value must be upper case')
    return value


class Superhero(BaseModel):
    name: Annotated[str, AfterValidator(assert_upper)]
    powers: list[str]
    city: Annotated[str, Field(examples=["New York, NY"])]


@chatprompt(
    SystemMessage('You are professor A, in charge of the A-people.'),
    UserMessage('Create a new superhero named {name}.'),
    model=OpenaiChatModel("gpt-4o"),
    max_retries=3,
)
def make_superhero(name: str) -> Superhero: ...


hero = make_superhero("The Bark Night")
print(hero)
```

This creates the following in **Logfire**:

- A span for the call to `make_superhero` showing the input arguments
- A span showing that retries have been enabled for this query
- A warning for each retry that was needed in order to generate a valid output
- The chat messages to/from the LLM, including tool calls and invalid outputs that required retrying

Magentic chatprompt-function call span and conversation

To learn more about Magentic, check out [magentic.dev](https://magentic.dev).

Third-party integrations

Third-party integrations are not officially supported by **Logfire**.

They are maintained by the community and may not be as reliable as the integrations developed by **Logfire**.

[Mirascope](https://github.com/Mirascope/mirascope) is a developer tool for building with LLMs. Their library focuses on abstractions that aren't obstructions and integrates with Logfire to make observability and monitoring for LLMs easy and seamless.

You can enable it using their [`@with_logfire`](https://mirascope.io/docs/latest/integrations/logfire/) decorator, which will work with all of the [model providers that they support](https://mirascope.io/docs/latest/learn/calls/#supported-providers) (e.g. OpenAI, Anthropic, Gemini, Mistral, Groq, and more).

```py
import logfire
from mirascope.core import anthropic, prompt_template
from mirascope.integrations.logfire import with_logfire

logfire.configure()


@with_logfire()
@anthropic.call("claude-3-5-sonnet-20240620")
@prompt_template("Please recommend some {genre} books")
def recommend_books(genre: str): ...


response = recommend_books("fantasy")  # this will automatically get logged with logfire
print(response.content)
# > Certainly! Here are some popular and well-regarded fantasy books and series: ...
```

This will give you:

- A span around the `recommend_books` that captures items like the prompt template, templating properties and fields, and input/output attributes
- Human-readable display of the conversation with the agent
- Details of the response, including the number of tokens used

Mirascope Anthropic call span and Anthropic span and conversation

Since Mirascope is built on top of [Pydantic](https://docs.pydantic.dev/latest/), you can use the [Pydantic plugin](../../pydantic/) to track additional logs and metrics about model validation.

This can be particularly useful when [extracting structured information](https://mirascope.io/docs/latest/learn/response_models/) using LLMs:

```py
from typing import Literal, Type

import logfire
from mirascope.core import openai, prompt_template
from mirascope.integrations.logfire import with_logfire
from pydantic import BaseModel

logfire.configure()
logfire.instrument_pydantic()


class TaskDetails(BaseModel):
    description: str
    due_date: str
    priority: Literal["low", "normal", "high"]


@with_logfire()
@openai.call("gpt-4o-mini", response_model=TaskDetails)
@prompt_template("Extract the details from the following task: {task}")
def extract_task_details(task: str): ...


task = "Submit quarterly report by next Friday. Task is high priority."
task_details = extract_task_details(task)  # this will be logged automatically with logfire
assert isinstance(task_details, TaskDetails)
print(task_details)
# > description='Submit quarterly report' due_date='next Friday' priority='high'
```

This will give you:

- Tracking for validation of Pydantic models
- A span around the `extract_task_details` that captures items like the prompt template, templating properties and fields, and input/output attributes
- Human-readable display of the conversation with the agent including the function call
- Details of the response, including the number of tokens used

Mirascope OpenAI Extractor span and OpenAI span and function call

For more information on Mirascope and what you can do with it, check out their [documentation](https://mirascope.io/docs).

We support instrumenting both the [standard OpenAI SDK](https://github.com/openai/openai-python) package and [OpenAI "agents"](https://github.com/openai/openai-agents-python) framework.

## OpenAI SDK

**Logfire** supports instrumenting calls to OpenAI with the logfire.instrument_openai() method, for example:

```python
import openai
import logfire

client = openai.Client()

logfire.configure()
logfire.instrument_openai()  # instrument all OpenAI clients globally
# or logfire.instrument_openai(client) to instrument a specific client instance

response = client.chat.completions.create(
    model='gpt-4',
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': 'Please write me a limerick about Python logging.'},
    ],
)
print(response.choices[0].message)
```

With that you get:

- a span around the call to OpenAI which records duration and captures any exceptions that might occur
- Human-readable display of the conversation with the agent
- details of the response, including the number of tokens used

OpenAI span and conversation

Span arguments including response details

### Methods covered

The following OpenAI methods are covered:

- [`client.chat.completions.create`](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) — with and without `stream=True`
- [`client.completions.create`](https://platform.openai.com/docs/guides/text-generation/completions-api) — with and without `stream=True`
- [`client.embeddings.create`](https://platform.openai.com/docs/guides/embeddings/how-to-get-embeddings)
- [`client.images.generate`](https://platform.openai.com/docs/guides/images/generations)
- [`client.responses.create`](https://platform.openai.com/docs/api-reference/responses)

All methods are covered with both `openai.Client` and `openai.AsyncClient`.

For example, here's instrumentation of an image generation call:

```python
import openai
import logfire

async def main():
    client = openai.AsyncClient()
    logfire.configure()
    logfire.instrument_openai(client)

    response = await client.images.generate(
        prompt='Image of R2D2 running through a desert in the style of cyberpunk.',
        model='dall-e-3',
    )
    url = response.data[0].url
    import webbrowser
    webbrowser.open(url)

if __name__ == '__main__':
    import asyncio
    asyncio.run(main())
```

Gives:

OpenAI image generation span

### Streaming Responses

When instrumenting streaming responses, Logfire creates two spans — one around the initial request and one around the streamed response.

Here we also use Rich's Live and Markdown types to render the response in the terminal in real-time.

```python
import openai
import logfire
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown

client = openai.AsyncClient()
logfire.configure()
logfire.instrument_openai(client)

async def main():
    console = Console()
    with logfire.span('Asking OpenAI to write some code'):
        response = await client.chat.completions.create(
            model='gpt-4',
            messages=[
                {'role': 'system', 'content': 'Reply in markdown one.'},
                {'role': 'user', 'content': 'Write Python to show a tree of files 🤞.'},
            ],
            stream=True
        )
        content = ''
        with Live('', refresh_per_second=15, console=console) as live:
            async for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    content += chunk.choices[0].delta.content
                    live.update(Markdown(content))

if __name__ == '__main__':
    import asyncio
    asyncio.run(main())
```

Shows up like this in Logfire:

OpenAI streaming response

## OpenAI Agents

We also support instrumenting the [OpenAI "agents"](https://github.com/openai/openai-agents-python) framework.

```python
import logfire
from agents import Agent, Runner

logfire.configure()
logfire.instrument_openai_agents()

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)
```

*For more information, see the instrument_openai_agents() API reference.*

Which shows up like this in Logfire:

OpenAI Agents

In this example we add a function tool to the agents:

```python
from typing_extensions import TypedDict

import logfire
from httpx import AsyncClient
from agents import RunContextWrapper, Agent, function_tool, Runner

logfire.configure()
logfire.instrument_openai_agents()


class Location(TypedDict):
    lat: float
    long: float


@function_tool
async def fetch_weather(ctx: RunContextWrapper[AsyncClient], location: Location) -> str:
    """Fetch the weather for a given location.

    Args:
        ctx: Run context object.
        location: The location to fetch the weather for.
    """
    r = await ctx.context.get('https://httpbin.org/get', params=location)
    return 'sunny' if r.status_code == 200 else 'rainy'


agent = Agent(name='weather agent', tools=[fetch_weather])


async def main():
    async with AsyncClient() as client:
        logfire.instrument_httpx(client)
        result = await Runner.run(agent, 'Get the weather at lat=51 lng=0.2', context=client)
    print(result.final_output)


if __name__ == '__main__':
    import asyncio
    asyncio.run(main())
```

We see spans from within the function call nested within the agent spans:

OpenAI Agents

**Pydantic Logfire** supports instrumenting [Pydantic AI](https://ai.pydantic.dev/) with the logfire.instrument_pydantic_ai() method:

```python
import logfire
from pydantic_ai import Agent, RunContext

logfire.configure()
logfire.instrument_pydantic_ai()

roulette_agent = Agent(
    'openai:gpt-4o',
    deps_type=int,
    result_type=bool,
    system_prompt=(
        'Use the `roulette_wheel` function to see if the '
        'customer has won based on the number they provide.'
    ),
)


@roulette_agent.tool
async def roulette_wheel(ctx: RunContext[int], square: int) -> str:
    """check if the square is a winner"""
    return 'winner' if square == ctx.deps else 'loser'


# Run the agent
success_number = 18
result = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)
print(result.data)
#> True

result = roulette_agent.run_sync('I bet five is the winner', deps=success_number)
print(result.data)
#> False
```

The above example displays like this in **Logfire**:

You can use Pydantic AI with a [large variety of LLMs](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.KnownModelName), the example just happens to show `gpt-4o`.

You can also instrument a specific agent with `logfire.instrument_pydantic_ai(agent)`.

For more information, see the logfire.instrument_pydantic_ai() reference or the [Pydantic AI docs on instrumenting](https://ai.pydantic.dev/logfire/) with **Logfire**.

# Web Frameworks

Here are some tips for instrumenting your web applications.

## Integrations

If you're using one of the following libraries, check out the integration docs:

- [FastAPI](fastapi/)
- [Starlette](starlette/)
- [Django](django/)
- [Flask](flask/)
- [AIOHTTP](aiohttp/)

Otherwise, check if your server uses [WSGI](wsgi/) or [ASGI](asgi/) and check the corresponding integration.

You can also use [Gunicorn](gunicorn/) with Logfire, which is a Python WSGI HTTP server for UNIX.

## Capturing HTTP server request and response headers

Some methods (e.g. `logfire.instrument_fastapi()`) allow you to pass `capture_headers=True` to record all request and response headers in the spans, and that's all you usually need.

If you want more control, there are three environment variables to tell the OpenTelemetry instrumentation libraries to capture request and response headers:

- `OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST`
- `OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE`
- `OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SANITIZE_FIELDS`

Each accepts a comma-separated list of regexes which are checked case-insensitively against header names. The first two determine which request/response headers are captured and added to span attributes. The third determines which headers will have their values redacted.

For example, to capture *all* headers, set the following:

```text
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST=".*"
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE=".*"
```

(this is what `capture_headers=True` does)

To specifically capture the `content-type` request header and request headers starting with `X-`:

```text
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST="content-type,X-.*"
```

To replace the `Authorization` header value with `[REDACTED]` to avoid leaking user credentials:

```text
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SANITIZE_FIELDS="Authorization"
```

(although usually it's better to rely on **Logfire**'s [scrubbing](../../how-to-guides/scrubbing/) feature)

## Query HTTP requests duration per percentile

It's usually interesting to visualize HTTP requests duration per percentile. Instead of having an average, which may be influenced by extreme values, percentiles allow us know the maximum duration for 50%, 90%, 95% or 99% of the requests.

Here is a sample query to compute those percentiles for HTTP requests duration:

```sql
WITH dataset AS (
  SELECT
    time_bucket('%time_bucket_duration%', start_timestamp) AS x,
    duration * 1000 as duration_ms
  FROM records
  WHERE attributes ? 'http.method'
)
SELECT
  x,
  approx_percentile_cont(0.50) WITHIN GROUP (ORDER BY duration_ms) as percentile_50,
  approx_percentile_cont(0.90) WITHIN GROUP (ORDER BY duration_ms) as percentile_90,
  approx_percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms) as percentile_95,
  approx_percentile_cont(0.99) WITHIN GROUP (ORDER BY duration_ms) as percentile_99
FROM dataset
GROUP BY x
ORDER BY x
```

Notice how we filtered on records that have the `http.method` attributes set. It's a good starting point to retrieve traces that are relevant for HTTP requests, but depending on your setup, you might need to add more filters.

You can use this query in a Time Series chart in a dashboard:

See the [DataFusion documentation](https://datafusion.apache.org/user-guide/sql/aggregate_functions.html#approx-percentile-cont) for more information on the `approx_percentile_cont` function.

## Excluding URLs from instrumentation

If you want to exclude certain URLs from tracing, you can either use Logfire's instrumentation methods or OpenTelemetry configuration. You can specify said URLs using a string of comma-separated regexes which will be matched against the full request URL.

### Using Logfire

Some methods (e.g. `logfire.instrument_fastapi()`) allow you to pass the argument `excluded_urls` as a string of comma-separated regexes.

### Using OpenTelemetry

You can set one of two environment variables to exclude URLs from tracing:

- `OTEL_PYTHON_EXCLUDED_URLS`, which will also apply to all instrumentations for which excluded URLs apply).
- `OTEL_PYTHON_FASTAPI_EXCLUDED_URLS`, for example, which will only apply to FastAPI instrumentation. You can replace `FASTAPI` with the name of the framework you're using.

If you'd like to trace all URLs except the base `/` URL, you can use the following regex for `excluded_urls`: `^https?://[^/]+/$`

Breaking it down:

- `^` matches the start of the string
- `https?` matches `http` or `https`
- `://` matches `://`
- `[^/]+` matches one or more characters that are not `/` (this will be the host part of the URL)
- `/` matches `/`
- `$` matches the end of the string

So this regex will only match routes that have no path after the host.

This instrumentation might look like:

```py
from fastapi import FastAPI

import logfire

app = FastAPI()

logfire.configure()
logfire.instrument_fastapi(app, excluded_urls='^https?://[^/]+/$')

if __name__ == '__main__':
    import uvicorn

    uvicorn.run(app)
```

If you visit <http://127.0.0.1:8000/>, that matches the above regex, so no spans will be sent to Logfire. If you visit <http://127.0.0.1:8000/hello/> (or any other endpoint that's not `/`, for that matter), a trace will be started and sent to Logfire.

Note

Under the hood, the `opentelemetry` library is using `re.search` (not `re.match` or `re.fullmatch`) to check for a match between the route and the `excluded_urls` regex, which is why we need to include the `^` at the start and `$` at the end of the regex.

Note

Specifying excluded URLs for a given instrumentation only prevents that specific instrumentation from creating spans/metrics, it doesn't suppress other instrumentation within the excluded endpoints.

# AIOHTTP Server

[AIOHTTP](https://docs.aiohttp.org/en/stable/) is an asynchronous HTTP client/server framework for asyncio and Python.

The logfire.instrument_aiohttp_server() method will create a span for every request made to your AIOHTTP server.

For AIOHTTP client instrumentation, see [here](../../http-clients/aiohttp/).

## Installation

Install `logfire` with the `aiohttp-server` extra:

```bash
pip install 'logfire[aiohttp-server]'
```

```bash
uv add 'logfire[aiohttp-server]'
```

```bash
poetry add 'logfire[aiohttp-server]'
```

## Usage

Here's a minimal server example:

server.py

```py
import logfire
from aiohttp import web


logfire.configure()
logfire.instrument_aiohttp_server()


async def hello(request):
    return web.Response(text="Hello, World!")


async def user_handler(request):
    user_id = request.match_info['user_id']
    return web.json_response({"user_id": user_id, "message": "User profile"})


app = web.Application()
app.router.add_get('/', hello)
app.router.add_get('/users/{user_id}', user_handler)


if __name__ == "__main__":
    web.run_app(app, host='localhost', port=8080)
```

You can run this server with `python server.py` and then make requests to `http://localhost:8080/` or `http://localhost:8080/users/123` to see the spans created for each request.

The keyword arguments of logfire.instrument_aiohttp_server() are passed to the `AioHttpServerInstrumentor().instrument()` method of the [OpenTelemetry AIOHTTP server instrumentation package](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/aiohttp_server/aiohttp_server.html).

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/aiohttp_server/aiohttp_server.html#exclude-lists)

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/aiohttp_server/aiohttp_server.html#capture-http-request-and-response-headers)

# ASGI

If the [ASGI](https://asgi.readthedocs.io/en/latest/) web framework you're using doesn't have a dedicated integration, you can use the logfire.instrument_asgi() method to instrument it.

## Installation

Install `logfire` with the `asgi` extra:

```bash
pip install 'logfire[asgi]'
```

```bash
uv add 'logfire[asgi]'
```

```bash
poetry add 'logfire[asgi]'
```

## Usage

Below we have a minimal example using [Uvicorn](https://www.uvicorn.org/). You can run it with `python main.py`:

main.py

```py
import logfire


logfire.configure()


async def app(scope, receive, send):
    assert scope["type"] == "http"
    await send(
        {
            "type": "http.response.start",
            "status": 200,
            "headers": [(b"content-type", b"text/plain"), (b"content-length", b"13")],
        }
    )
    await send({"type": "http.response.body", "body": b"Hello, world!"})

app = logfire.instrument_asgi(app)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app)
```

The keyword arguments of logfire.instrument_asgi() are passed to the OpenTelemetryMiddleware class of the OpenTelemetry ASGI Instrumentation package.

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)

Note

`instrument_asgi` does accept an `excluded_urls` parameter, but does not support specifying said URLs via an environment variable, unlike other instrumentations.

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/asgi/asgi.html#capture-http-request-and-response-headers)

# Django

The logfire.instrument_django() method can be used to instrument the [Django](https://www.djangoproject.com/) web framework with **Logfire**.

## Installation

Install `logfire` with the `django` extra:

```bash
pip install 'logfire[django]'
```

```bash
uv add 'logfire[django]'
```

```bash
poetry add 'logfire[django]'
```

## Usage

In your [Django settings file](https://docs.djangoproject.com/en/stable/topics/settings/), add the following lines:

```py
import logfire

# ...All the other settings...

LOGGING = {  # (1)!
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'logfire': {
            'class': 'logfire.LogfireLoggingHandler',
        },
    },
    'root': {
        'handlers': ['logfire'],
    },
}

# Add the following lines at the end of the file
logfire.configure()
logfire.instrument_django()
```

1. Django uses the standard library logging module, and can be configured using the [dictConfig format](https://docs.djangoproject.com/en/stable/topics/logging/#configuring-logging). As per our dedicated [logging section](../../logging/), you can make use of the LogfireLoggingHandler to redirect logs to Logfire.

logfire.instrument_django() uses the **OpenTelemetry Django Instrumentation** package, which you can find more information about [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/django/django.html).

Note

The above lines must be the last thing to execute in your settings. On a regular Django project, this means at the end of your settings file. If you use an exotic configuration setup with several settings files (e.g. divided into `local/prod/dev`), make sure you put those lines where they will be imported and executed last. Otherwise, the instrumentation might not work as expected.

In case you are using Gunicorn to run your Django application, you can [configure Logfire in Gunicorn](../gunicorn/) as well.

## Instrumenting Django ORM Queries

To instrument Django ORM queries, you need to install the associated DB instrumentation tool, then add the corresponding instrumentation command to your ‍settings file.

By default, the Django configuration [uses SQLite as the database engine](https://docs.djangoproject.com/en/stable/ref/databases/). To instrument it, you need to call logfire.instrument_sqlite3().

If you are using a different database, check the available instrumentation methods in our [Integrations section](../../).

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/django/django.html#exclude-lists)

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/django/django.html#capture-http-request-and-response-headers)

# FastAPI

**Logfire** combines custom and third-party instrumentation for [FastAPI](https://fastapi.tiangolo.com/) with the logfire.instrument_fastapi() method.

## Installation

Install `logfire` with the `fastapi` extra:

```bash
pip install 'logfire[fastapi]'
```

```bash
uv add 'logfire[fastapi]'
```

```bash
poetry add 'logfire[fastapi]'
```

## Usage

We have a minimal example below. Please install [Uvicorn](https://www.uvicorn.org/) to run it:

```bash
pip install uvicorn
```

You can run it with `python main.py`:

main.py

```py
import logfire
from fastapi import FastAPI

app = FastAPI()

logfire.configure()
logfire.instrument_fastapi(app)


@app.get("/hello")
async def hello(name: str):
    return {"message": f"hello {name}"}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app)
```

Then visit <http://localhost:8000/hello?name=world> and check the logs.

logfire.instrument_fastapi() accepts arbitrary additional keyword arguments and passes them to the OpenTelemetry `FastAPIInstrumentor.instrument_app()` method. See [their documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/fastapi/fastapi.html) for more details.

## Endpoint arguments and validation errors

logfire.instrument_fastapi() adds the following attributes to the request spans:

- `fastapi.arguments.values`: A dictionary mapping argument names of the endpoint function to parsed and validated values.
- `fastapi.arguments.errors`: A list of validation errors for any invalid inputs.

You can customize these attributes by passing a `request_attributes_mapper` function to `instrument_fastapi`. This function will be called with the `Request` or `WebSocket` object and a dictionary containing keys `values` and `errors` corresponding to the attributes above. It should return a new dictionary of attributes. For example:

```py
import logfire

app = ...


def request_attributes_mapper(request, attributes):
    if attributes["errors"]:
        # Only log validation errors, not valid arguments
        return {
            # This will become the `fastapi.arguments.errors` attribute
            "errors": attributes["errors"],
            # Arbitrary custom attributes can also be added here
            "my_custom_attribute": ...,
        }
    else:
        # Don't log anything for valid requests
        return {}


logfire.configure()
logfire.instrument_fastapi(app, request_attributes_mapper=request_attributes_mapper)
```

Note

The request_attributes_mapper function mustn't mutate the contents of `values` or `errors`, but it can safely replace them with new values.

## Timestamps of argument parsing and endpoint execution

logfire.instrument_fastapi() also adds the following attributes to the request spans:

- The times when parsing arguments and resolving dependencies started and ended:
  - `fastapi.arguments.start_timestamp`
  - `fastapi.arguments.end_timestamp`
- The times when the actual endpoint function started and ended executing, leaving out the time spent on dependencies and middleware:
  - `fastapi.endpoint_function.start_timestamp`
  - `fastapi.endpoint_function.end_timestamp`

## Spans for argument parsing and endpoint execution

You can also enable spans for argument parsing and endpoint execution with `logfire.instrument_fastapi(app, extra_spans=True)`. The main request span will still have the attributes described above, but it will also have two extra child spans. This is mostly redundant now and is mainly provided for backwards compatibility. It can also be useful for grouping together child logs and spans produced by the request.

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/fastapi/fastapi.html#exclude-lists)

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/fastapi/fastapi.html#capture-http-request-and-response-headers)

# Flask

The logfire.instrument_flask() method will create a span for every request to your [Flask](https://flask.palletsprojects.com/en/2.0.x/) application.

## Install

Install `logfire` with the `flask` extra:

```bash
pip install 'logfire[flask]'
```

```bash
uv add 'logfire[flask]'
```

```bash
poetry add 'logfire[flask]'
```

## Usage

Let's see a minimal example below. You can run it with `python main.py`:

main.py

```py
import logfire
from flask import Flask


logfire.configure()

app = Flask(__name__)
logfire.instrument_flask(app)


@app.route("/")
def hello():
    return "Hello!"


if __name__ == "__main__":
    app.run(debug=True)
```

The keyword arguments of `logfire.instrument_flask()` are passed to the `FlaskInstrumentor().instrument_app()` method of the OpenTelemetry Flask Instrumentation package, read more about it [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/flask/flask.html).

In case you are using Gunicorn to run your Flask application, you can [configure Logfire in Gunicorn](../gunicorn/) as well.

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/flask/flask.html#exclude-lists)

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/flask/flask.html#capture-http-request-and-response-headers)

# Gunicorn

[Gunicorn](https://docs.gunicorn.org/en/latest/index.html) is a Python WSGI HTTP server for UNIX. It is a pre-fork worker model, which means it forks multiple worker processes to handle requests concurrently.

To configure Logfire with Gunicorn, you can use the `logfire.configure()` function to set up Logfire in the [`post_fork` hook](https://docs.gunicorn.org/en/latest/settings.html#post-fork) in Gunicorn's configuration file:

```py
import logfire

def post_fork(server, worker):
    logfire.configure()
```

Then start Gunicorn with the configuration file:

```bash
gunicorn myapp:app --config gunicorn_config.py
```

Where `myapp:app` is your WSGI application and `gunicorn_config.py` is the configuration file where you defined the `post_fork` function.

## Instrumenting a Flask application

This section shows how to instrument a Flask application running under Gunicorn with Logfire.

Here is the `Flask` application code (`myapp.py`):

myapp.py

```py
from flask import Flask

app = Flask(__name__)

@app.route("/")
def index():
    return "Hello from Flask + Gunicorn!"
```

To instrument this Flask application with Logfire, you can modify the `post_fork` function in your Gunicorn configuration file to import and instrument the Flask app (`gunicorn_config.py`):

gunicorn_config.py

```py
import logfire

from myapp import app

def post_fork(server, worker):
    logfire.configure()
    logfire.instrument_flask(app)
```

Then, you can start Gunicorn with the following command:

```bash
gunicorn myapp:app --config gunicorn_config.py
```

This will start Gunicorn with the Flask application, and Logfire will automatically instrument the HTTP requests handled by the Flask app.

# Starlette

The logfire.instrument_starlette() method will create a span for every request to your [Starlette](https://www.starlette.io/) application.

## Installation

Install `logfire` with the `starlette` extra:

```bash
pip install 'logfire[starlette]'
```

```bash
uv add 'logfire[starlette]'
```

```bash
poetry add 'logfire[starlette]'
```

## Usage

We have a minimal example below. Please install [Uvicorn](https://www.uvicorn.org/) to run it:

```bash
pip install uvicorn
```

You can run it with `python main.py`:

main.py

```py
import logfire
from starlette.applications import Starlette
from starlette.responses import PlainTextResponse
from starlette.requests import Request
from starlette.routing import Route

logfire.configure()


async def home(request: Request) -> PlainTextResponse:
    return PlainTextResponse("Hello, world!")


app = Starlette(routes=[Route("/", home)])
logfire.instrument_starlette(app)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app)
```

The keyword arguments of `logfire.instrument_starlette()` are passed to the `StarletteInstrumentor.instrument_app()` method of the OpenTelemetry Starlette Instrumentation package, read more about it [here](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/starlette/starlette.html).

What about the OpenTelemetry ASGI middleware?

If you are a more experienced user, you might be wondering why we are not using the [OpenTelemetry ASGI middleware](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/asgi/asgi.html). The reason is that the `StarletteInstrumentor` actually wraps the ASGI middleware and adds some additional information related to the routes.

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/starlette/starlette.html#exclude-lists)

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/starlette/starlette.html#capture-http-request-and-response-headers)

# WSGI

If the [WSGI](https://wsgi.readthedocs.io/en/latest/) web framework you're using doesn't have a dedicated integration, you can use the logfire.instrument_wsgi() method to instrument it.

## Installation

Install `logfire` with the `wsgi` extra:

```bash
pip install 'logfire[wsgi]'
```

```bash
uv add 'logfire[wsgi]'
```

```bash
poetry add 'logfire[wsgi]'
```

## Usage

Below we have a minimal example using the standard library [`wsgiref`](https://docs.python.org/3/library/wsgiref.html). You can run it with `python main.py`:

main.py

```py
from wsgiref.simple_server import make_server

import logfire


logfire.configure()

def app(env, start_response):
    start_response('200 OK', [('Content-Type','text/html')])
    return [b"Hello World"]

app = logfire.instrument_wsgi(app)

with make_server("", 8000, app) as httpd:
    print("Serving on port 8000...")

    # Serve until process is killed
    httpd.serve_forever()
```

The keyword arguments of logfire.instrument_wsgi() are passed to the OpenTelemetryMiddleware class of the OpenTelemetry WSGI Instrumentation package.

## Excluding URLs from instrumentation

- [Quick guide](../#excluding-urls-from-instrumentation)

## Capturing request and response headers

- [Quick guide](../#capturing-http-server-request-and-response-headers)
- [OpenTelemetry Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/wsgi/wsgi.html#capture-http-request-and-response-headers)
# Optional

This [Github issue](https://github.com/pydantic/logfire/issues/1004) is the best place to check for updates to the Pydantic Logfire roadmap.

If you have any questions, or a feature request, **please join our [Slack](https://logfire.pydantic.dev/docs/join-slack/)**.

# Release Notes

## [v4.7.0](https://github.com/pydantic/logfire/compare/v4.6.0...v4.7.0) (2025-09-12)

- Update to OpenTelemetry SDK 1.37.0, drop support for \<1.35.0 by @alexmojaki in [#1398](https://github.com/pydantic/logfire/pull/1398)

## [v4.6.0](https://github.com/pydantic/logfire/compare/v4.5.0...v4.6.0) (2025-09-10)

- Fix: check `LOGFIRE_IGNORE_NO_CONFIG` from environment when needed by @Lftobs in [#1379](https://github.com/pydantic/logfire/pull/1379)
- Expand scrubbing whitelist, especially for `gen_ai` messages by @alexmojaki in [#1392](https://github.com/pydantic/logfire/pull/1392)
- Print project link eagerly from credentials file if possible by @alexmojaki in [#1393](https://github.com/pydantic/logfire/pull/1393)
- Fix: Only list orgs in CLI where projects can be created by @alexmojaki in [#1391](https://github.com/pydantic/logfire/pull/1391)

## [v4.5.0](https://github.com/pydantic/logfire/compare/v4.4.0...v4.5.0) (2025-09-09)

- Export `attach_context` and `get_context` directly from `logfire` module, not just `logfire.propagate` by @dhruv-ahuja in [#1385](https://github.com/pydantic/logfire/pull/1385)
- Only list writeable projects from CLI by @alexmojaki in [#1386](https://github.com/pydantic/logfire/pull/1386)
- Add `ensure_ascii=False` to `json.dumps` calls by @qiangxinglin in [#1381](https://github.com/pydantic/logfire/pull/1381)
- Remove unused `stack_offset` from `with_settings()` by @LucasSantos27 in [#1380](https://github.com/pydantic/logfire/pull/1380)

## [v4.4.0](https://github.com/pydantic/logfire/compare/v4.3.6...v4.4.0) (2025-09-05)

- Add `logfire.instrument_print()` by @alexmojaki in [#1368](https://github.com/pydantic/logfire/pull/1368)
- Record response on 'MCP server handle request' span by @alexmojaki in [#1362](https://github.com/pydantic/logfire/pull/1362)
- Handle MCP request metadata being a dict by @alexmojaki in [#1360](https://github.com/pydantic/logfire/pull/1360)
- fix: handle optional args in `logfire.instrument` by @stevenh in [#1337](https://github.com/pydantic/logfire/pull/1337)
- Add `logfire_token` to scrubbing patterns by @alexmojaki in [#1367](https://github.com/pydantic/logfire/pull/1367)

## [v4.3.6](https://github.com/pydantic/logfire/compare/v4.3.5...v4.3.6) (2025-08-26)

- Add specific code agent options for `logfire prompt` by @Kludex in [#1350](https://github.com/pydantic/logfire/pull/1350)

## [v4.3.5](https://github.com/pydantic/logfire/compare/v4.3.4...v4.3.5) (2025-08-22)

- Add `--base-url` to CLI by @Kludex in [#1345](https://github.com/pydantic/logfire/pull/1345)
- Don't require stream kwarg in OpenAI methods by @alexmojaki in [#1348](https://github.com/pydantic/logfire/pull/1348)

## [v4.3.4](https://github.com/pydantic/logfire/compare/v4.3.3...v4.3.4) (2025-08-21)

- `logfire.instrument_google_genai()` fixes for `response_schema` and elided content by @alexmojaki in [#1342](https://github.com/pydantic/logfire/pull/1342)
- Fix `enable_commenter` and `commenter_options` args of `instrument_sqlalchemy` by @alexmojaki in [#1335](https://github.com/pydantic/logfire/pull/1335)
- Fix LangChain instrumentation by @alexmojaki in [#1319](https://github.com/pydantic/logfire/pull/1319)
- Suppress `urllib3` logs while checking token by @alexmojaki in [#1341](https://github.com/pydantic/logfire/pull/1341)

## [v4.3.3](https://github.com/pydantic/logfire/compare/v4.3.2...v4.3.3) (2025-08-13)

- Ensure `logfire_api.LogfireSpan.context` and other attrs are None by @alexmojaki in [#1317](https://github.com/pydantic/logfire/pull/1317)

## [v4.3.2](https://github.com/pydantic/logfire/compare/v4.3.1...v4.3.2) (2025-08-13)

- Able to see output from token selection by @Kludex in [#1312](https://github.com/pydantic/logfire/pull/1312)
- Remove trailing `/` from prompts call by @Kludex in [#1314](https://github.com/pydantic/logfire/pull/1314)
- Change get read token info endpoint by @hramezani in [#1309](https://github.com/pydantic/logfire/pull/1309)

## [v4.3.1](https://github.com/pydantic/logfire/compare/v4.3.0...v4.3.1) (2025-08-12)

- Fix `logfire_api` requiring OpenTelemetry SDK by @alexmojaki in [#1310](https://github.com/pydantic/logfire/pull/1310)

## [v4.3.0](https://github.com/pydantic/logfire/compare/v4.2.0...v4.3.0) (2025-08-12)

- Closes logfire_api.LogfireSpan should support ReadableSpan attributes #989 by @RichardMarto in [#1288](https://github.com/pydantic/logfire/pull/1288)
- Fix `test_instrument_google_genai` for specific versions by @alexmojaki in [#1303](https://github.com/pydantic/logfire/pull/1303)
- Create read tokens via CLI by @Kludex in [#1306](https://github.com/pydantic/logfire/pull/1306)
- Retrieve the prompt to fix issues via CLI by @Kludex in [#1307](https://github.com/pydantic/logfire/pull/1307)

## [v4.2.0](https://github.com/pydantic/logfire/compare/v4.1.0...v4.2.0) (2025-08-06)

- Handle wrong JSON schema in console exporter by @alexmojaki in [#1294](https://github.com/pydantic/logfire/pull/1294)
- Move creation of span down on cli by @Kludex in [#1298](https://github.com/pydantic/logfire/pull/1298)
- Allow to set `instrument_httpx(capture_all=True)` via env var by @Kludex in [#1295](https://github.com/pydantic/logfire/pull/1295)

## [v4.1.0](https://github.com/pydantic/logfire/compare/v4.0.1...v4.1.0) (2025-08-04)

- Add `min_level` argument to `logfire.configure` by @alexmojaki in [#1265](https://github.com/pydantic/logfire/pull/1265)

## [v4.0.1](https://github.com/pydantic/logfire/compare/v4.0.0...v4.0.1) (2025-07-31)

- Handle cyclic references in exceptions by @alexmojaki in [#1284](https://github.com/pydantic/logfire/pull/1284)
- Support OpenTelemetry 1.36.0 by @alexmojaki in [#1285](https://github.com/pydantic/logfire/pull/1285)

## [v4.0.0](https://github.com/pydantic/logfire/compare/v3.25.0...v4.0.0) (2025-07-22)

**BREAKING CHANGES**:

- Remove extra FastAPI spans by default by @alexmojaki in [#1268](https://github.com/pydantic/logfire/pull/1268)
- Stop implicitly emitting deprecated process runtime metrics by @alexmojaki in [#932](https://github.com/pydantic/logfire/pull/932)

## [v3.25.0](https://github.com/pydantic/logfire/compare/v3.24.2...v3.25.0) (2025-07-18)

- Use local timezone in console logging by @fswair in [#1255](https://github.com/pydantic/logfire/pull/1255)
- Add `logfire run` command by @Kludex in [#1139](https://github.com/pydantic/logfire/pull/1139)
- Allow removing extra FastAPI spans by @alexmojaki in [#1258](https://github.com/pydantic/logfire/pull/1258)
- Fix `litellm` instrumentation by @alexmojaki in [#1249](https://github.com/pydantic/logfire/pull/1249)
- Add `logfire.exception.fingerprint` attribute to spans with exceptions by @alexmojaki in [#1253](https://github.com/pydantic/logfire/pull/1253)

## [v3.24.2](https://github.com/pydantic/logfire/compare/v3.24.1...v3.24.2) (2025-07-14)

- Fix auto-tracing Python 3.12 ParamSpec syntax by @alexmojaki in [#1247](https://github.com/pydantic/logfire/pull/1247)

## [v3.24.1](https://github.com/pydantic/logfire/compare/v3.24.0...v3.24.1) (2025-07-14)

- Support OpenTelemetry 1.35.0 by @alexmojaki in [#1242](https://github.com/pydantic/logfire/pull/1242)

## [v3.24.0](https://github.com/pydantic/logfire/compare/v3.23.0...v3.24.0) (2025-07-10)

- Add experimental `logfire.instrument_litellm()` by @alexmojaki in [#1237](https://github.com/pydantic/logfire/pull/1237)

## [v3.23.0](https://github.com/pydantic/logfire/compare/v3.22.1...v3.23.0) (2025-07-08)

- Add info method to logfire query clients by @hramezani in [#1204](https://github.com/pydantic/logfire/pull/1204)
- Make query client not experimental by @alexmojaki in [#1234](https://github.com/pydantic/logfire/pull/1234)

## [v3.22.1](https://github.com/pydantic/logfire/compare/v3.22.0...v3.22.1) (2025-07-07)

- Fix OpenAI streaming reasoning by @alexmojaki in [#1232](https://github.com/pydantic/logfire/pull/1232)
- Handle bytes in google genai messages by @alexmojaki in [#1231](https://github.com/pydantic/logfire/pull/1231)

## [v3.22.0](https://github.com/pydantic/logfire/compare/v3.21.2...v3.22.0) (2025-07-02)

- Add `instrument_google_genai` by @alexmojaki in [#1217](https://github.com/pydantic/logfire/pull/1217)
- Refactor user tokens, introduce Logfire client by @Viicos in [#981](https://github.com/pydantic/logfire/pull/981)
- Use new endpoint for project creation by @hramezani in [#1202](https://github.com/pydantic/logfire/pull/1202)

## [v3.21.2](https://github.com/pydantic/logfire/compare/v3.21.1...v3.21.2) (2025-06-30)

- Fix importlib resources with auto tracing by @alexmojaki in [#1212](https://github.com/pydantic/logfire/pull/1212)

## [v3.21.1](https://github.com/pydantic/logfire/compare/v3.21.0...v3.21.1) (2025-06-18)

- Fix for new OpenAI Agents SDK by @alexmojaki in [#1152](https://github.com/pydantic/logfire/pull/1152)

## [v3.21.0](https://github.com/pydantic/logfire/compare/v3.20.0...v3.21.0) (2025-06-17)

- Add up counter/histogram metrics in spans by @alexmojaki in [#1099](https://github.com/pydantic/logfire/pull/1099)
- Fix kwargs in proxy metric instruments, preventing warning about duplicate histograms by @alexmojaki in [#1149](https://github.com/pydantic/logfire/pull/1149)

## [v3.20.0](https://github.com/pydantic/logfire/compare/v3.19.0...v3.20.0) (2025-06-16)

- Add baggage utilities by @dmontagu in [#1128](https://github.com/pydantic/logfire/pull/1128)
- Handle NonRecordingSpans correctly in OpenAI instrumentation by @alexmojaki in [#1145](https://github.com/pydantic/logfire/pull/1145)

## [v3.19.0](https://github.com/pydantic/logfire/compare/v3.18.0...v3.19.0) (2025-06-12)

- `logfire.instrument_aiohttp_server()` by @adtyavrdhn in [#1131](https://github.com/pydantic/logfire/pull/1131)
- Fix handling of `pydantic_core.Url` and `pydantic.AnyUrl` by @dmontagu in [#1130](https://github.com/pydantic/logfire/pull/1130)
- Correct context for MCP logs by @alexmojaki in [#1138](https://github.com/pydantic/logfire/pull/1138)
- Check empty tokens before making connection attempt by @nagarajRPoojari in [#1110](https://github.com/pydantic/logfire/pull/1110)
- Pass meter provider to Pydantic AI by @alexmojaki in [#1136](https://github.com/pydantic/logfire/pull/1136)

## [v3.18.0](https://github.com/pydantic/logfire/compare/v3.17.0...v3.18.0) (2025-06-05)

- Upgrade to OpenTelemetry SDK 1.34.0 by @alexmojaki in [#1120](https://github.com/pydantic/logfire/pull/1120)
- Drop Python 3.8 support by @alexmojaki in [#1122](https://github.com/pydantic/logfire/pull/1122)

## [v3.17.0](https://github.com/pydantic/logfire/compare/v3.16.2...v3.17.0) (2025-06-03)

- LangChain instrumentation via LangSmith by @alexmojaki in [#1084](https://github.com/pydantic/logfire/pull/1084)
- MCP OTel context propagation by @samuelcolvin in [#1103](https://github.com/pydantic/logfire/pull/1103)
- Add `capfire.get_collected_metrics()` by @alexmojaki in [#1116](https://github.com/pydantic/logfire/pull/1116)

## [v3.16.2](https://github.com/pydantic/logfire/compare/v3.16.1...v3.16.2) (2025-06-03)

- Fixes for OpenAI Responses API and Agents SDK by @alexmojaki in [#1092](https://github.com/pydantic/logfire/pull/1092), [#1093](https://github.com/pydantic/logfire/pull/1093), [#1094](https://github.com/pydantic/logfire/pull/1094), and [#1095](https://github.com/pydantic/logfire/pull/1095)
- Fix verbose console formatting for enum, dates, and decimals by @sbhrwlr in [#1096](https://github.com/pydantic/logfire/pull/1096)
- Allow setting `logfire.msg` in structlog integration by @alexmojaki in [#1113](https://github.com/pydantic/logfire/pull/1113)
- Add ASGI instrumentation package to `django` extra by @alexmojaki in [#1097](https://github.com/pydantic/logfire/pull/1097)

## [v3.16.1](https://github.com/pydantic/logfire/compare/v3.16.0...v3.16.1) (2025-05-26)

- Infer base URL from read token in query client by @Viicos in [#1088](https://github.com/pydantic/logfire/pull/1088)
- Add `include_binary_content` ([#1090](https://github.com/pydantic/logfire/pull/1090)) and `**kwargs` ([#1078](https://github.com/pydantic/logfire/pull/1078)) to `instrument_pydantic_ai` by @alexmojaki

## [v3.16.0](https://github.com/pydantic/logfire/compare/v3.15.1...v3.16.0) (2025-05-14)

- Make OpenAI spans show token usage in logfire UI by @alexmojaki in [#1076](https://github.com/pydantic/logfire/pull/1076)
- Fixes for verbose console logging by @alexmojaki in [#1071](https://github.com/pydantic/logfire/pull/1071) and [#1072](https://github.com/pydantic/logfire/pull/1072)
- Export first batch of spans more quickly by @alexmojaki in [#1066](https://github.com/pydantic/logfire/pull/1066)
- Tighten scrubbing patterns to reduce accidental matches by @alexmojaki in [#1074](https://github.com/pydantic/logfire/pull/1074)
- Add `do_not_scrub` and `binary_content` as safe keys for scrubber by @alexmojaki in [#1075](https://github.com/pydantic/logfire/pull/1075)

## [v3.15.1](https://github.com/pydantic/logfire/compare/v3.15.0...v3.15.1) (2025-05-12)

- Support OpenTelemetry SDK 1.33.0 by @alexmojaki in [#1067](https://github.com/pydantic/logfire/pull/1067)

## [v3.15.0](https://github.com/pydantic/logfire/compare/v3.14.1...v3.15.0) (2025-05-08)

- Remove attributes from `http.server.active_requests` metric to prevent emitting too many by @alexmojaki in [#1060](https://github.com/pydantic/logfire/pull/1060)
- This is technically a breaking change as it means less data is sent to Logfire, but most users don't use it and some will save a significant amount of money.

## [v3.14.1](https://github.com/pydantic/logfire/compare/v3.14.0...v3.14.1) (2025-04-24)

- Handle changes in `openai` and `anthropic` by @alexmojaki in [#1030](https://github.com/pydantic/logfire/pull/1030)
- Fix exporting of very large spans and payloads by @alexmojaki in [#1027](https://github.com/pydantic/logfire/pull/1027)
- Prevent infinite loop in `get_user_frame_and_stacklevel` by @alexmojaki in [#1031](https://github.com/pydantic/logfire/pull/1031)

## [v3.14.0](https://github.com/pydantic/logfire/compare/v3.13.1...v3.14.0) (2025-04-11)

- Experimental functions for recording feedback annotations

## [v3.13.1](https://github.com/pydantic/logfire/compare/v3.13.0...v3.13.1) (2025-04-10)

- Upgrade to OpenTelemetry SDK 1.32.0 by @alexmojaki in [#991](https://github.com/pydantic/logfire/pull/991)

## [v3.13.0](https://github.com/pydantic/logfire/compare/v3.12.0...v3.13.0) (2025-04-10)

- Emit logs sent from MCP server to client by @alexmojaki in [#974](https://github.com/pydantic/logfire/pull/974)
- Return `None` from `logfire_api.LogfireSpan.context` when `logfire` could not be imported by @DouweM in [#983](https://github.com/pydantic/logfire/pull/983)

## [v3.12.0](https://github.com/pydantic/logfire/compare/v3.11.0...v3.12.0) (2025-03-31)

- Add `logfire.instrument_mcp()` method by @alexmojaki in [#966](https://github.com/pydantic/logfire/pull/966)
- Merge headers if passed via `client_kwargs` in query client by @Kludex in [#958](https://github.com/pydantic/logfire/pull/958)
- Warn user if f-string expression contains `await` by @Sbargaoui in [#944](https://github.com/pydantic/logfire/pull/944)
- Handle new MCP span in OpenAI Agents SDK by @alexmojaki in [#963](https://github.com/pydantic/logfire/pull/963)

## [v3.11.0](https://github.com/pydantic/logfire/compare/v3.10.0...v3.11.0) (2025-03-26)

- Add `record_return` flag to `@logfire.instrument` by @alexmojaki in [#955](https://github.com/pydantic/logfire/pull/955)

## [v3.10.0](https://github.com/pydantic/logfire/compare/v3.9.1...v3.10.0) (2025-03-25)

- Account for new EU region by @Viicos in [#901](https://github.com/pydantic/logfire/pull/901)

## [v3.9.1](https://github.com/pydantic/logfire/compare/v3.9.0...v3.9.1) (2025-03-25)

- Handle Anthropic thinking blocks by @alexmojaki in [#952](https://github.com/pydantic/logfire/pull/952)
- Handle new voice span types from OpenAI Agents SDK by @alexmojaki in [#943](https://github.com/pydantic/logfire/pull/943)

## [v3.9.0](https://github.com/pydantic/logfire/compare/v3.8.1...v3.9.0) (2025-03-18)

- Add `logfire.instrument_pydantic_ai()` by @alexmojaki in [#926](https://github.com/pydantic/logfire/pull/926)

## [v3.8.1](https://github.com/pydantic/logfire/compare/v3.8.0...v3.8.1) (2025-03-13)

- Upgrade to OpenTelemetry 1.31.0 by @alexmojaki in [#927](https://github.com/pydantic/logfire/pull/927)
- Record exception with traceback for non-fatal function tool errors in OpenAI agents SDK by @alexmojaki in [#924](https://github.com/pydantic/logfire/pull/924)

## [v3.8.0](https://github.com/pydantic/logfire/compare/v3.7.1...v3.8.0) (2025-03-11)

- OpenAI Agents Framework instrumentation by @alexmojaki in [#917](https://github.com/pydantic/logfire/pull/917)
- OTel log scrubbing by @alexmojaki in [#903](https://github.com/pydantic/logfire/pull/903)

## [v3.7.1](https://github.com/pydantic/logfire/compare/v3.7.0...v3.7.1) (2025-03-05)

- Handle errors in OpenAI response by @alexmojaki in [#910](https://github.com/pydantic/logfire/pull/910)
- Include domain in message for outgoing HTTP requests: fix for old semconv by @alexmojaki in [#909](https://github.com/pydantic/logfire/pull/909)

## [v3.7.0](https://github.com/pydantic/logfire/compare/v3.6.4...v3.7.0) (2025-03-04)

- Include domain in message for outgoing requests by @alexmojaki in [#892](https://github.com/pydantic/logfire/pull/892)
- Console logging for OTel logs by @alexmojaki in [#882](https://github.com/pydantic/logfire/pull/882)
- Fix auto-tracing with `python -m` by @alexmojaki in [#905](https://github.com/pydantic/logfire/pull/905)

## [v3.6.4](https://github.com/pydantic/logfire/compare/v3.6.3...v3.6.4) (2025-02-25)

- Handle mocks by calling `to_dict` on type by @alexmojaki in [#897](https://github.com/pydantic/logfire/pull/897)

## [v3.6.3](https://github.com/pydantic/logfire/compare/v3.6.2...v3.6.3) (2025-02-25)

- Handle missing `shutdown` and `force_flush` on `NoOpLoggerProvider` better by @alexmojaki in [#895](https://github.com/pydantic/logfire/pull/895)
- Handle missing events SDK by @alexmojaki in [#893](https://github.com/pydantic/logfire/pull/893)

## [v3.6.2](https://github.com/pydantic/logfire/compare/v3.6.1...v3.6.2) (2025-02-22)

- Fix typing errors involving `handle_internal_errors` by @alexmojaki in [#885](https://github.com/pydantic/logfire/pull/885)
- Avoid double shutdown of logger provider by @alexmojaki in [#878](https://github.com/pydantic/logfire/pull/878)

## [v3.6.1](https://github.com/pydantic/logfire/compare/v3.6.0...v3.6.1) (2025-02-19)

- avoid `BatchLogRecordProcessor` use on pyodide/emscripten by @samuelcolvin in [#873](https://github.com/pydantic/logfire/pull/873)

## [v3.6.0](https://github.com/pydantic/logfire/compare/v3.5.3...v3.6.0) (2025-02-18)

- Set log level to warning instead of error for 4xx HTTPExceptions from FastAPI/Starlette by @alexmojaki in [#858](https://github.com/pydantic/logfire/pull/858)
- Add option to disable printing tags to console by @dmontagu in [#860](https://github.com/pydantic/logfire/pull/860)
- Experimental support for OTel logs by @alexmojaki in [#863](https://github.com/pydantic/logfire/pull/863), [#870](https://github.com/pydantic/logfire/pull/870), and [#871](https://github.com/pydantic/logfire/pull/871)
- Fix `excluded_urls` typo in instrument_flask by @alexmojaki in [#852](https://github.com/pydantic/logfire/pull/852)
- Catch more errors when checking for sqlalchemy objects by @alexmojaki in [#854](https://github.com/pydantic/logfire/pull/854)
- Don't scrub exception message by @alexmojaki in [#865](https://github.com/pydantic/logfire/pull/865)
- Only skip logging to console after updating span stack and indentation by @alexmojaki in [#844](https://github.com/pydantic/logfire/pull/844)

## [v3.5.3](https://github.com/pydantic/logfire/compare/v3.5.2...v3.5.3) (2025-02-05)

- Fixes for capturing httpx bodies by @alexmojaki in [#842](https://github.com/pydantic/logfire/pull/842)

## [v3.5.2](https://github.com/pydantic/logfire/compare/v3.5.1...v3.5.2) (2025-02-05)

- Support OpenTelemetry 1.30.0 by @alexmojaki in [#839](https://github.com/pydantic/logfire/pull/839)

## [v3.5.1](https://github.com/pydantic/logfire/compare/v3.5.0...v3.5.1) (2025-02-04)

- Prevent side effects when importing logfire by @alexmojaki in [#835](https://github.com/pydantic/logfire/pull/835)

## [v3.5.0](https://github.com/pydantic/logfire/compare/v3.4.0...v3.5.0) (2025-02-03)

- Add `logfire.logfire_info()` by @samuelcolvin in [#826](https://github.com/pydantic/logfire/pull/826)
- Add `logfire.add_non_user_code_prefix` function for library developers by @dmontagu in [#829](https://github.com/pydantic/logfire/pull/829)
- Skip export retry in pyodide by @samuelcolvin in [#823](https://github.com/pydantic/logfire/pull/823)
- More resilient console logging by @samuelcolvin in [#831](https://github.com/pydantic/logfire/pull/831)

## [v3.4.0](https://github.com/pydantic/logfire/compare/v3.3.0...v3.4.0) (2025-01-27)

- Support Pyodide by @samuelcolvin in [#818](https://github.com/pydantic/logfire/pull/818)

## [v3.3.0](https://github.com/pydantic/logfire/compare/v3.2.0...v3.3.0) (2025-01-22)

- Add process runtime information by @Kludex in [#811](https://github.com/pydantic/logfire/pull/811)

## [v3.2.0](https://github.com/pydantic/logfire/compare/v3.1.1...v3.2.0) (2025-01-17)

- Fix conflict with `ddtrace` futures patching by renaming `fn` parameter by @alexmojaki in [#802](https://github.com/pydantic/logfire/pull/802)
- Add `logfire.warning` to mirror `logging.warning` by @JacobHayes in [#800](https://github.com/pydantic/logfire/pull/800)
- Try `to_dict` method when encoding JSON by @alexmojaki in [#799](https://github.com/pydantic/logfire/pull/799)
- Don't truncate numpy array dimensions below max by @alexmojaki in [#792](https://github.com/pydantic/logfire/pull/792)

## [v3.1.1](https://github.com/pydantic/logfire/compare/v3.1.0...v3.1.1) (2025-01-14)

- Prevent OTel from logging noisy traceback for handled requests exceptions by @alexmojaki in [#796](https://github.com/pydantic/logfire/pull/796)

## [v3.1.0](https://github.com/pydantic/logfire/compare/v3.0.0...v3.1.0) (2025-01-09)

- Add `capture_all` to `instrument_httpx` by @Kludex in [#780](https://github.com/pydantic/logfire/pull/780)
- Ensure cleanup when forked process ends by @alexmojaki in [#785](https://github.com/pydantic/logfire/pull/785)
- Generate trace IDs as ULIDs by default by @adriangb in [#783](https://github.com/pydantic/logfire/pull/783)

## [v3.0.0](https://github.com/pydantic/logfire/compare/v2.11.1...v3.0.0) (2025-01-07)

- **BREAKING CHANGE**: Removed `capture_request_json_body`, `capture_request_text_body`, `capture_request_form_data`, and `capture_response_json_body` parameters from `logfire.instrument_httpx()`, replaced with `capture_request_body` `capture_response_body` by @Kludex in [#769](https://github.com/pydantic/logfire/pull/769)

Other changes:

- Add `distributed_tracing` argument to `logfire.configure()` and warn by default when trace context is extracted by @alexmojaki in [#773](https://github.com/pydantic/logfire/pull/773)
- Don't show `urllib3` when `requests` is installed on `logfire inspect` by @Kludex in [#744](https://github.com/pydantic/logfire/pull/744)
- Add `--ignore` to `logfire inspect` by @Kludex in [#748](https://github.com/pydantic/logfire/pull/748)
- Access `model_fields` on the model class by @Viicos in [#761](https://github.com/pydantic/logfire/pull/761)
- Remove double record exception by @dmontagu in [#712](https://github.com/pydantic/logfire/pull/712)

## [v2.11.1](https://github.com/pydantic/logfire/compare/v2.11.0...v2.11.1) (2024-12-30)

- Handle errors from `sqlalchemy.inspect` by @alexmojaki in [#733](https://github.com/pydantic/logfire/pull/733)

## [v2.11.0](https://github.com/pydantic/logfire/compare/v2.10.0...v2.11.0) (2024-12-23)

- Add `capture_request_text_body` param to `instrument_httpx` by @alexmojaki in [#722](https://github.com/pydantic/logfire/pull/722)
- Support for `AnthropicBedrock` client by @stephenhibbert in [#701](https://github.com/pydantic/logfire/pull/701)

## [v2.10.0](https://github.com/pydantic/logfire/compare/v2.9.0...v2.10.0) (2024-12-23)

- Add `capture_request_form_data` param to `instrument_httpx` by @alexmojaki in [#711](https://github.com/pydantic/logfire/pull/711)
- Replace `capture_(request|response)_headers` with just `capture_headers` in `instrument_httpx` by @Kludex in [#719](https://github.com/pydantic/logfire/pull/719)
- Support SQLAlchemy `AsyncEngine` by @Kludex in [#717](https://github.com/pydantic/logfire/pull/717)

## [v2.9.0](https://github.com/pydantic/logfire/compare/v2.8.0...v2.9.0) (2024-12-20)

- Capture httpx response JSON bodies by @alexmojaki in [#700](https://github.com/pydantic/logfire/pull/700)
- Use end-at-shutdown and custom `record_exception` logic for all spans by @dmontagu in [#696](https://github.com/pydantic/logfire/pull/696)

## [v2.8.0](https://github.com/pydantic/logfire/compare/v2.7.1...v2.8.0) (2024-12-18)

- Add `capture_(request|response)_headers` ([#671](https://github.com/pydantic/logfire/pull/671)) and `capture_request_json_body` ([#682](https://github.com/pydantic/logfire/pull/682)) to `instrument_httpx` by @Kludex
- Fix patching of ProcessPoolExecutor by @alexmojaki in [#690](https://github.com/pydantic/logfire/pull/690)
- Rearrange span processors to avoid repeating scrubbing and other tweaking by @alexmojaki in [#658](https://github.com/pydantic/logfire/pull/658)
- Remove end-on-exit stuff by @dmontagu in [#676](https://github.com/pydantic/logfire/pull/676)

## [v2.7.1](https://github.com/pydantic/logfire/compare/v2.7.0...v2.7.1) (2024-12-13)

- Fix erroneous `<circular reference>` when object is repeated in list by @alexmojaki in [#664](https://github.com/pydantic/logfire/pull/664)

## [v2.7.0](https://github.com/pydantic/logfire/compare/v2.6.2...v2.7.0) (2024-12-11)

- Add `logfire.instrument_aws_lambda` by @Kludex in [#657](https://github.com/pydantic/logfire/pull/657)

## [v2.6.2](https://github.com/pydantic/logfire/compare/v2.6.1...v2.6.2) (2024-12-05)

- Update the `process.pid` resource attribute after `os.fork()` by @alexmojaki in [#647](https://github.com/pydantic/logfire/pull/647)
- Check for `os.register_at_fork` before calling by @alexmojaki in [#648](https://github.com/pydantic/logfire/pull/648)

## [v2.6.1](https://github.com/pydantic/logfire/compare/v2.6.0...v2.6.1) (2024-12-05)

- Use `exc_info` in structlog processor by @alexmojaki in [#641](https://github.com/pydantic/logfire/pull/641)
- Re-seed random ID generator after `os.fork()` by @alexmojaki in [#644](https://github.com/pydantic/logfire/pull/644)

## [v2.6.0](https://github.com/pydantic/logfire/compare/v2.5.0...v2.6.0) (2024-12-02)

- Add `instrument_sqlite3` by @Kludex in [#634](https://github.com/pydantic/logfire/pull/634)

## [v2.5.0](https://github.com/pydantic/logfire/compare/v2.4.1...v2.5.0) (2024-11-27)

- Add `logfire.suppress_scopes` method by @alexmojaki in [#628](https://github.com/pydantic/logfire/pull/628)
- Replace `ModuleNotFoundError` by `ImportError` by @Kludex in [#622](https://github.com/pydantic/logfire/pull/622)

## [v2.4.1](https://github.com/pydantic/logfire/compare/v2.4.0...v2.4.1) (2024-11-21)

- Allow new context argument of metric instrument methods to be passed positionally by @alexmojaki in [#616](https://github.com/pydantic/logfire/pull/616)

## [v2.4.0](https://github.com/pydantic/logfire/compare/v2.3.0...v2.4.0) (2024-11-20)

- Support `logfire.instrument` without arguments by @Kludex in [#607](https://github.com/pydantic/logfire/pull/607)
- Handle internal errors in `create_json_schema` by @alexmojaki in [#613](https://github.com/pydantic/logfire/pull/613)
- Handle errors in auto-tracing better by @alexmojaki in [#610](https://github.com/pydantic/logfire/pull/610)

## [v2.3.0](https://github.com/pydantic/logfire/compare/v2.2.1...v2.3.0) (2024-11-14)

- Respect repr on fields when logging a dataclass by @dmontagu in [#592](https://github.com/pydantic/logfire/pull/592)
- Allow `extract_args` to be an iterable of argument names by @alexmojaki in [#570](https://github.com/pydantic/logfire/pull/570)
- Make metric instrument methods compatible with older OTel versions by @alexmojaki in [#600](https://github.com/pydantic/logfire/pull/600)
- Add span links by @Kludex in [#587](https://github.com/pydantic/logfire/pull/587)

## [v2.2.1](https://github.com/pydantic/logfire/compare/v2.2.0...v2.2.1) (2024-11-13)

- Ignore trivial/empty functions in auto-tracing by @alexmojaki in [#596](https://github.com/pydantic/logfire/pull/596)
- Handle missing attributes in `_custom_object_schema` by @alexmojaki in [#597](https://github.com/pydantic/logfire/pull/597)
- Let user know what they should install for integrations by @Kludex in [#593](https://github.com/pydantic/logfire/pull/593)

## [v2.2.0](https://github.com/pydantic/logfire/compare/v2.1.2...v2.2.0) (2024-11-13)

- Allow instrumenting a single httpx client by @alexmojaki in [#575](https://github.com/pydantic/logfire/pull/575)
- Log LLM tool call for streamed response by @jackmpcollins in [#545](https://github.com/pydantic/logfire/pull/545)

## [v2.1.2](https://github.com/pydantic/logfire/compare/v2.1.1...v2.1.2) (2024-11-04)

- Check `.logfire` for creds to respect `'if-token-present'` setting by @sydney-runkle in [#561](https://github.com/pydantic/logfire/pull/561)

## [v2.1.1](https://github.com/pydantic/logfire/compare/v2.1.0...v2.1.1) (2024-10-31)

- Use `functools.wraps` in `@logfire.instrument` by @alexmojaki in [#562](https://github.com/pydantic/logfire/pull/562)
- Set `logfire.code.work_dir` resource attribute whenever other code source attributes are present by @alexmojaki in [#563](https://github.com/pydantic/logfire/pull/563)
- Don't scrub `logfire.logger_name` by @alexmojaki in [#564](https://github.com/pydantic/logfire/pull/564)

## [v2.1.0](https://github.com/pydantic/logfire/compare/v2.0.0...v2.1.0) (2024-10-30)

- Add ASGI & WSGI instrument methods by @Kludex in [#324](https://github.com/pydantic/logfire/pull/324)
- Add `logfire.work_dir` resource attribute by @Kludex in [#532](https://github.com/pydantic/logfire/pull/532)
- Add `logfire.configure(environment=...)` by @Kludex in [#557](https://github.com/pydantic/logfire/pull/557)
- Show message from API backend when checking token fails by @alexmojaki in [#559](https://github.com/pydantic/logfire/pull/559)

## [v2.0.0](https://github.com/pydantic/logfire/compare/v1.3.2...v2.0.0) (2024-10-30)

- `@logfire.instrument()` no longer needs source code by @alexmojaki in [#543](https://github.com/pydantic/logfire/pull/543). **BREAKING CHANGES** caused by this:
- Functions decorated with `@logfire.instrument()` and functions nested within them can now be auto-traced unlike before. Use `@logfire.no_auto_trace` anywhere on functions you want to exclude, especially the instrumented function.
- Decorated async generator functions won't support the `.asend` method properly - the generator will only receive `None`. But `instrument` shouldn't be used on generators anyway unless the generator is being used as a context manager, so new warnings about this have been added. See https://logfire.pydantic.dev/docs/guides/advanced/generators/#using-logfireinstrument

## [v1.3.2](https://github.com/pydantic/logfire/compare/v1.3.1...v1.3.2) (2024-10-29)

- Handle NonRecordingSpans for fastapi arguments by @alexmojaki in [#551](https://github.com/pydantic/logfire/pull/551)
- Preserve docstrings in auto-tracing by @alexmojaki in [#550](https://github.com/pydantic/logfire/pull/550)

## [v1.3.1](https://github.com/pydantic/logfire/compare/v1.3.0...v1.3.1) (2024-10-28)

- Handle null fastapi route.name and route.operation_id by @alexmojaki in [#547](https://github.com/pydantic/logfire/pull/547)

## [v1.3.0](https://github.com/pydantic/logfire/compare/v1.2.0...v1.3.0) (2024-10-24)

- Add Code Source links by @Kludex in [#451](https://github.com/pydantic/logfire/pull/451) and [#505](https://github.com/pydantic/logfire/pull/505)
- Add fastapi arguments attributes directly on the root OTel span, remove `use_opentelemetry_instrumentation` kwarg by @alexmojaki in [#509](https://github.com/pydantic/logfire/pull/509)
- Allow setting tags on logfire spans by @AdolfoVillalobos in [#497](https://github.com/pydantic/logfire/pull/497)
- Add logger name to `LogfireLoggingHandler` spans by @samuelcolvin in [#534](https://github.com/pydantic/logfire/pull/534)
- Format `None` as `None` instead of `null` in messages by @alexmojaki in [#525](https://github.com/pydantic/logfire/pull/525)
- Use `PYTEST_VERSION` instead of `PYTEST_CURRENT_TEST` to detect `logfire.configure()` being called within a pytest run but outside any test by @Kludex in [#531](https://github.com/pydantic/logfire/pull/531)

## [v1.2.0](https://github.com/pydantic/logfire/compare/v1.1.0...v1.2.0) (2024-10-17)

- Add `local` parameter to `logfire.configure()` by @alexmojaki in [#508](https://github.com/pydantic/logfire/pull/508)

## [v1.1.0](https://github.com/pydantic/logfire/compare/v1.0.1...v1.1.0) (2024-10-14)

- Fix error in checking for generators in auto-tracing by @alexmojaki in https://github.com/pydantic/logfire/pull/498
- Support `'if-token-present'` for env var `'LOGFIRE_SEND_TO_LOGFIRE'` by @sydney-runkle in https://github.com/pydantic/logfire/pull/488
- Use `Compression.Gzip` by @Kludex in https://github.com/pydantic/logfire/pull/491

## [v1.0.1](https://github.com/pydantic/logfire/compare/v1.0.0...v1.0.1) (2024-10-02)

- Fix warning about unregistered MetricReaders by @alexmojaki in https://github.com/pydantic/logfire/pull/465

## [v1.0.0](https://github.com/pydantic/logfire/compare/v0.55.0...v1.0.0) (2024-09-30)

- Upgrade `DeprecationWarning`s to `UserWarning`s by @alexmojaki in https://github.com/pydantic/logfire/pull/458
- Update query client APIs by @dmontagu in https://github.com/pydantic/logfire/pull/454

## [v0.55.0](https://github.com/pydantic/logfire/compare/v0.54.0...v0.55.0) (2024-09-27)

- Replace `pydantic_plugin` in `logfire.configure()` with `logfire.instrument_pydantic()` by @alexmojaki in https://github.com/pydantic/logfire/pull/453
- Keep `METRICS_PREFERRED_TEMPORALITY` private by @alexmojaki in https://github.com/pydantic/logfire/pull/456
- Use `SeededRandomIdGenerator` by default to prevent interference from `random.seed` by @alexmojaki in https://github.com/pydantic/logfire/pull/457

## [v0.54.0](https://github.com/pydantic/logfire/compare/v0.53.0...v0.54.0) (2024-09-26)

- **Changes in `logfire.configure()`:**
- Remove `show_summary` and `fast_shutdown` by @alexmojaki in https://github.com/pydantic/logfire/pull/431
- Move `base_url`, `id_generator`, and `ns_timestamp_generator` parameters into `advanced: AdvancedOptions` by @alexmojaki in https://github.com/pydantic/logfire/pull/432
- Add `metrics` parameter by @alexmojaki in https://github.com/pydantic/logfire/pull/444
- Remove default `min_duration` for `install_auto_tracing` by @alexmojaki in https://github.com/pydantic/logfire/pull/446

## [v0.53.0](https://github.com/pydantic/logfire/compare/v0.52.0...v0.53.0) (2024-09-17)

- Tail sampling by @alexmojaki in https://github.com/pydantic/logfire/pull/407
- Use OTEL scopes better, especially instead of tags by @alexmojaki in https://github.com/pydantic/logfire/pull/420
- Deprecate `project_name` in `logfire.configure()`, remove old kwargs from signature by @alexmojaki in https://github.com/pydantic/logfire/pull/428
- Fix websocket span messages by @alexmojaki in https://github.com/pydantic/logfire/pull/426
- Remove warning about attribute/variable name conflicts in f-string magic by @alexmojaki in https://github.com/pydantic/logfire/pull/418

## [v0.52.0](https://github.com/pydantic/logfire/compare/v0.51.0...v0.52.0) (2024-09-05)

- Handle FastAPI update with SolvedDependencies by @alexmojaki in https://github.com/pydantic/logfire/pull/415
- Add experimental client for the Logfire Query API by @dmontagu in https://github.com/pydantic/logfire/pull/405
- Remove `default_span_processor` parameter from `configure` by @alexmojaki in https://github.com/pydantic/logfire/pull/400
- Remove `custom_scope_suffix` parameter of `Logfire.log` by @alexmojaki in https://github.com/pydantic/logfire/pull/399
- Add missing `service_version` field to `_LogfireConfigData` so that it gets copied into subprocesses by @alexmojaki in https://github.com/pydantic/logfire/pull/401

## [v0.51.0](https://github.com/pydantic/logfire/compare/v0.50.1...v0.51.0) (2024-08-22)

### BREAKING CHANGES

- **System metrics are no longer collected by default** when the correct dependency is installed. Use [`logfire.instrument_system_metrics()`](https://logfire.pydantic.dev/docs/integrations/system-metrics/) to enable system metrics collection. **If you are simply using the old 'Basic System Metrics' dashboard, then no further code changes are required, but that dashboard will no longer work properly and you should create a new dashboard from the template named 'Basic System Metrics (Logfire)'**. If you were using other collected metrics, see the documentation for how to collect those. By @alexmojaki in https://github.com/pydantic/logfire/pull/373
- Stop collecting package versions by @alexmojaki in https://github.com/pydantic/logfire/pull/387
- Don't auto-trace generators by @alexmojaki in https://github.com/pydantic/logfire/pull/386
- Disable ASGI send/receive spans by default by @alexmojaki in https://github.com/pydantic/logfire/pull/371

### Other fixes

- Add py.typed file to logfire-api by @jackmpcollins in https://github.com/pydantic/logfire/pull/379
- Check `LambdaRuntimeClient` before logging tracebacks in `_ensure_flush_after_aws_lambda` by @alexmojaki in https://github.com/pydantic/logfire/pull/388

## [v0.50.1](https://github.com/pydantic/logfire/compare/v0.50.0...v0.50.1) (2024-08-06)

(Previously released as `v0.50.0`, then yanked due to https://github.com/pydantic/logfire/issues/367)

- **BREAKING CHANGES:** Separate sending to Logfire from using standard OTEL environment variables by @alexmojaki in https://github.com/pydantic/logfire/pull/351. See https://logfire.pydantic.dev/docs/guides/advanced/alternative_backends/ for details. Highlights:
- `OTEL_EXPORTER_OTLP_ENDPOINT` is no longer just an alternative to `LOGFIRE_BASE_URL`. Setting `OTEL_EXPORTER_OTLP_ENDPOINT`, `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`, and/or `OTEL_EXPORTER_OTLP_METRICS_ENDPOINT` will set up appropriate exporters *in addition* to sending to Logfire, which must be turned off separately if desired. These are basic exporters relying on OTEL defaults. In particular they don't use our custom retrying logic.
- `LOGFIRE_BASE_URL` / `logfire.configure(base_url=...)` is now only intended for actual alternative Logfire backends, which are currently only available to Logfire developers, and unlike `OTEL_EXPORTER_OTLP_ENDPOINT` requires authenticating with Logfire.
- Pending spans are only sent to logfire-specific exporters.
- Add `capture_statement` to Redis instrumentation by @Kludex in https://github.com/pydantic/logfire/pull/355

## [v0.49.1](https://github.com/pydantic/logfire/compare/v0.49.0...v0.49.1) (2024-08-05)

- Add missing return on instrument methods by @Kludex in https://github.com/pydantic/logfire/pull/360
- Add `logfire.exception()` to `logfire-api` by @Kludex in https://github.com/pydantic/logfire/pull/358
- Remove `TypeAlias` from code source by @Kludex in https://github.com/pydantic/logfire/pull/359
- Turn `ParamSpec` non-private by @Kludex in https://github.com/pydantic/logfire/pull/361

## [v0.49.0](https://github.com/pydantic/logfire/compare/v0.48.1...v0.49.0) (2024-08-05)

- Add `logfire.instrument_mysql()` by @aditkumar72 in https://github.com/pydantic/logfire/pull/341
- Set OTEL status description when logging exceptions by @alexmojaki in https://github.com/pydantic/logfire/pull/348
- Switch UpDownCounters to cumulative aggregation temporality by @alexmojaki in https://github.com/pydantic/logfire/pull/347
- Log more info about internal errors by @alexmojaki in https://github.com/pydantic/logfire/pull/346

## [v0.48.1](https://github.com/pydantic/logfire/compare/v0.48.0...v0.48.1) (2024-07-29)

- Handle newer opentelemetry versions by @alexmojaki in https://github.com/pydantic/logfire/pull/337
- More lenient handling of loguru message mismatch and better warnings by @alexmojaki in https://github.com/pydantic/logfire/pull/338
- Add better type hints for HTTPX and AsyncPG by @Kludex in https://github.com/pydantic/logfire/pull/342
- Handle `setuptools` changing `sys.path` for importing `packaging.version` by @alexmojaki in https://github.com/pydantic/logfire/pull/344

## [v0.48.0](https://github.com/pydantic/logfire/compare/v0.47.0...v0.48.0) (2024-07-24)

- Add `instrument_celery` method by @Kludex in https://github.com/pydantic/logfire/pull/322
- `capture_headers` by @alexmojaki in https://github.com/pydantic/logfire/pull/318
- Handle message formatting errors by @alexmojaki in https://github.com/pydantic/logfire/pull/329
- Handle logging `None` with `loguru` by @alexmojaki in https://github.com/pydantic/logfire/pull/331

## [v0.47.0](https://github.com/pydantic/logfire/compare/v0.46.1...v0.47.0) (2024-07-20)

- Fix recursive logging from OTEL's `BatchSpanProcessor` by @alexmojaki in https://github.com/pydantic/logfire/pull/306
- Set sqlalchemy 'connect' spans to debug level by @alexmojaki in https://github.com/pydantic/logfire/pull/307
- Add type hints to instrument methods by @Kludex in https://github.com/pydantic/logfire/pull/320
- Handle older versions of anthropic by @alexmojaki in https://github.com/pydantic/logfire/pull/316
- Update dependencies, handle change in importlib by @alexmojaki in https://github.com/pydantic/logfire/pull/323
- Summarize db.statement in message by @alexmojaki in https://github.com/pydantic/logfire/pull/308
- Handle and test other OpenAI/Anthropic client methods by @alexmojaki in https://github.com/pydantic/logfire/pull/312

## [v0.46.1](https://github.com/pydantic/logfire/compare/v0.46.0...v0.46.1) (2024-07-05)

- Fix release process for `logfire-api` by @Kludex in https://github.com/pydantic/logfire/pull/303

## [v0.46.0](https://github.com/pydantic/logfire/compare/v0.45.1...v0.46.0) (2024-07-05)

- Add `logfire-api` by @Kludex in https://github.com/pydantic/logfire/pull/268
- Use exponential histogram buckets by @alexmojaki in https://github.com/pydantic/logfire/pull/282
- Add attribute noting details of scrubbed values by @alexmojaki in https://github.com/pydantic/logfire/pull/278
- Ensure `force_flush` at end of AWS Lambda invocation by @alexmojaki in https://github.com/pydantic/logfire/pull/296

## [v0.45.1](https://github.com/pydantic/logfire/compare/v0.45.0...v0.45.1) (2024-07-01)

- Fix ignore no config warning message by @ba1mn in https://github.com/pydantic/logfire/pull/292
- Ensure `StaticFiles` doesn't break `instrument_fastapi` by @alexmojaki in https://github.com/pydantic/logfire/pull/294

## [v0.45.0](https://github.com/pydantic/logfire/compare/v0.44.0...v0.45.0) (2024-06-29)

- Add `scrubbing: ScrubbingOptions | False` parameter to `logfire.configure`, replacing `scrubbing_patterns` and `scrubbing_callback` by @alexmojaki in https://github.com/pydantic/logfire/pull/283
- Fix and test unmapped SQLModels by @alexmojaki in https://github.com/pydantic/logfire/pull/286
- Optimize `collect_package_info` by @alexmojaki in https://github.com/pydantic/logfire/pull/285

## [v0.44.0](https://github.com/pydantic/logfire/compare/v0.43.0...v0.44.0) (2024-06-26)

- Prevent 'dictionary changed size during iteration' error in `install_auto_tracing` by @alexmojaki in https://github.com/pydantic/logfire/pull/277
- `suppress_instrumentation` when retrying exports by @alexmojaki in https://github.com/pydantic/logfire/pull/279
- Log async stack in `log_slow_async_callbacks` by @alexmojaki in https://github.com/pydantic/logfire/pull/280

## [v0.43.0](https://github.com/pydantic/logfire/compare/v0.42.0...v0.43.0) (2024-06-24)

- **BREAKING CHANGE**: Remove default for `modules` parameter of `install_auto_tracing` by @alexmojaki in https://github.com/pydantic/logfire/pull/261
- **BREAKING CHANGE**: Check if logfire token is valid in separate thread, so `logfire.configure` won't block startup and will no longer raise an exception for an invalid token, by @alexmojaki in https://github.com/pydantic/logfire/pull/274
- Remove `logfire_api_session` parameter from `logfire.configure` by @alexmojaki in https://github.com/pydantic/logfire/pull/272
- Default the log level to error if the status code is error, and vice versa by @alexmojaki in https://github.com/pydantic/logfire/pull/269
- Avoid importing `gitpython` by @alexmojaki in https://github.com/pydantic/logfire/pull/260
- Only delete files on `logfire clean` by @Kludex in https://github.com/pydantic/logfire/pull/267
- Bug fix: Logging arguments of a request to a FastAPI sub app by @sneakyPad in https://github.com/pydantic/logfire/pull/259
- Fix query params not being in message by @alexmojaki in https://github.com/pydantic/logfire/pull/271
- Replace 'Redacted' with 'Scrubbed' in 'Redacted due to...' by @alexmojaki in https://github.com/pydantic/logfire/pull/273

## [v0.42.0](https://github.com/pydantic/logfire/compare/v0.41.0...v0.42.0) (2024-06-11)

- Improved handling of request errors when exporting by @alexmojaki in https://github.com/pydantic/logfire/pull/252
- `ignore_no_config` setting added to `pyproject.toml` by @deepakdinesh1123 in https://github.com/pydantic/logfire/pull/254
- Make `logfire whoami` respect the `LOGFIRE_TOKEN` env var by @alexmojaki in https://github.com/pydantic/logfire/pull/256

## [v0.41.0](https://github.com/pydantic/logfire/compare/v0.40.0...v0.41.0) (2024-06-06)

- Fix backfill command by @alexmojaki in https://github.com/pydantic/logfire/pull/243
- Update Anthropic to use tools that are no longer in beta by @willbakst in https://github.com/pydantic/logfire/pull/249
  - Anthropic instrumentation now requires `anthropic>=0.27.0`

## [v0.40.0](https://github.com/pydantic/logfire/compare/v0.39.0...v0.40.0) (2024-06-04)

- **BREAKING CHANGE:** The `processors` parameter of `logfire.configure()` has been replaced by `additional_span_processors`. Passing `processors` will raise an error. Unlike `processors`, setting `additional_span_processors` to an empty sequence will not disable the default span processor which exports to Logfire. To do that, pass `send_to_logfire=False`. Similarly `metric_readers` has been replaced by `additional_metric_reader`. By @alexmojaki in https://github.com/pydantic/logfire/pull/233
- Improve error raised when opentelemetry.instrumentation.django is not installed by @deepakdinesh1123 in https://github.com/pydantic/logfire/pull/231
- Handle internal errors by @alexmojaki in https://github.com/pydantic/logfire/pull/232

## [v0.39.0](https://github.com/pydantic/logfire/compare/v0.38.0...v0.39.0) (2024-06-03)

Add new methods for easier integration in https://github.com/pydantic/logfire/pull/207:

- `instrument_flask`
- `instrument_starlette`
- `instrument_aiohttp_client`
- `instrument_sqlalchemy`
- `instrument_pymongo`
- `instrument_redis`

## [v0.38.0](https://github.com/pydantic/logfire/compare/v0.37.0...v0.38.0) (2024-05-31)

**BREAKING CHANGE**: Calling `logfire.info`, `logfire.error`, `logfire.span` etc. will no longer automatically configure logfire if it hasn't been configured already. Instead it will emit a warning and not log anything. Users must call `logfire.configure()` before they want logging to actually start, even if they don't pass any arguments to `configure` and all configuration is done by environment variables. Using integrations like `logfire.instrument_fastapi()` before calling `configure` will also emit a warning but it will still set up the instrumentation, although it will not log anything until `configure` is called.

## [v0.37.0](https://github.com/pydantic/logfire/compare/v0.36.1...v0.37.0) (2024-05-29)

- Add `logfire.suppress_instrumentation` context manager, silence `urllib3` debug logs from exporting by @jlondonobo in https://github.com/pydantic/logfire/pull/197

## [v0.36.1](https://github.com/pydantic/logfire/compare/v0.36.0...v0.36.1) (2024-05-27)

- Fix structlog import by @alexmojaki in https://github.com/pydantic/logfire/pull/217

## [v0.36.0](https://github.com/pydantic/logfire/compare/v0.35.0...v0.36.0) (2024-05-27)

- Allow passing OTEL kwargs through instrument_fastapi by @alexmojaki in https://github.com/pydantic/logfire/pull/205
- Retry connection errors by @alexmojaki in https://github.com/pydantic/logfire/pull/214

## [v0.35.0](https://github.com/pydantic/logfire/compare/v0.34.0...v0.35.0) (2024-05-21)

- Add `logfire.instrument_requests()` by @tlpinney in https://github.com/pydantic/logfire/pull/196
- Add `logfire.instrument_httpx()` by @tlpinney in https://github.com/pydantic/logfire/pull/198
- Add `logfire.instrument_django()` by @inspirsmith in https://github.com/pydantic/logfire/pull/200

## [v0.34.0](https://github.com/pydantic/logfire/compare/v0.33.0...v0.34.0) (2024-05-21)

- Allow instrumenting OpenAI/Anthropic client classes or modules by @alexmojaki in https://github.com/pydantic/logfire/pull/191

## [v0.33.0](https://github.com/pydantic/logfire/compare/v0.32.1...v0.33.0) (2024-05-18)

- Fix logging integrations with non-string messages by @alexmojaki in https://github.com/pydantic/logfire/pull/179
- Anthropic instrumentation by @willbakst in https://github.com/pydantic/logfire/pull/181

## [v0.32.1](https://github.com/pydantic/logfire/compare/v0.32.0...v0.32.1) (2024-05-15)

- Add 'executing' version to 'logfire info' output by @alexmojaki in https://github.com/pydantic/logfire/pull/180
- Don't use `include_url` with Pydantic's V1 `ValidationError` by @Kludex in https://github.com/pydantic/logfire/pull/184

## [v0.32.0](https://github.com/pydantic/logfire/compare/v0.31.0...v0.32.0) (2024-05-14)

- Don't scrub spans from OpenAI integration by @alexmojaki in https://github.com/pydantic/logfire/pull/173
- Convert FastAPI arguments log to span, don't set to debug by default by @alexmojaki in https://github.com/pydantic/logfire/pull/164
- Raise an exception when Pydantic plugin is enabled on Pydantic \<2.5.0 by @bossenti in https://github.com/pydantic/logfire/pull/160
- Do not require project name on `logfire projects use` command by @Kludex in https://github.com/pydantic/logfire/pull/177

## [v0.31.0](https://github.com/pydantic/logfire/compare/v0.30.0...v0.31.0) (2024-05-13)

- Improve error when `opentelemetry-instrumentation-fastapi` is missing by @Kludex in https://github.com/pydantic/logfire/pull/143
- Set `send_to_logfire` to `False` when running under Pytest by @Kludex in https://github.com/pydantic/logfire/pull/154
- Add `logfire.metric_gauge()` by @Kludex in https://github.com/pydantic/logfire/pull/153
- Use `stack_info` instead of `stack_offset` by @Kludex in https://github.com/pydantic/logfire/pull/137
- Fix JSON encoding/schema of pydantic v1 models by @alexmojaki in https://github.com/pydantic/logfire/pull/163
- Handle intermediate logging/loguru levels by @alexmojaki in https://github.com/pydantic/logfire/pull/162
- Add exception on console by @Kludex in https://github.com/pydantic/logfire/pull/168
- f-string magic by @alexmojaki in https://github.com/pydantic/logfire/pull/151

## [v0.30.0](https://github.com/pydantic/logfire/compare/v0.29.0...v0.30.0) (2024-05-06)

- Close spans when process shuts down before the exporter shuts down and drops them by @alexmojaki in https://github.com/pydantic/logfire/pull/108
- add `psycopg` in OTEL_PACKAGES and optional-dependencies by @Elkiwa in https://github.com/pydantic/logfire/pull/115
- [PYD-877] Log OpenAI streaming response at the end instead of opening a span and attaching context in a generator that may not finish by @alexmojaki in https://github.com/pydantic/logfire/pull/107
- Increase minimum typing-extensions version by @Kludex in https://github.com/pydantic/logfire/pull/129
- Add note about creating write tokens when user is not authenticated by @Kludex in https://github.com/pydantic/logfire/pull/127
- Make pip install command printed by 'logfire inspect' easy to copy by @alexmojaki in https://github.com/pydantic/logfire/pull/130

## [v0.29.0](https://github.com/pydantic/logfire/compare/v0.28.3...v0.29.0) (2024-05-03)

- Add log level on `on_start` for ASGI send and receive messages by @Kludex in https://github.com/pydantic/logfire/pull/94
- Support a dataclass type as an argument by @dmontagu in https://github.com/pydantic/logfire/pull/100
- Add min_log_level to console options by @Kludex in https://github.com/pydantic/logfire/pull/95
- Improve the OpenAI integration by @Kludex in https://github.com/pydantic/logfire/pull/104

## [v0.28.3](https://github.com/pydantic/logfire/compare/v0.28.2...v0.28.3) (2024-05-02)

- Fix pydantic plugin for cloudpickle by @alexmojaki in https://github.com/pydantic/logfire/pull/86
- Handle unloaded SQLAlchemy fields in JSON schema by @alexmojaki in https://github.com/pydantic/logfire/pull/92

## [v0.28.2](https://github.com/pydantic/logfire/compare/v0.28.1...v0.28.2) (2024-05-02)

- Fix OpenAI streaming empty chunk error by @hramezani in https://github.com/pydantic/logfire/pull/69
- Update pyproject.toml to include logfire in sdist build target by @syniex in https://github.com/pydantic/logfire/pull/51
- Recommend `opentelemetry-instrumentation-sklearn` on `scikit-learn` instead of `sklearn` by @Kludex in https://github.com/pydantic/logfire/pull/75

## [v0.28.1](https://github.com/pydantic/logfire/compare/v0.28.0...v0.28.1) (2024-05-01)

- Don't scrub 'author' by @alexmojaki in https://github.com/pydantic/logfire/pull/55
- Check if object is SQLAlchemy before dataclass by @Kludex in https://github.com/pydantic/logfire/pull/67

## [v0.28.0](https://github.com/pydantic/logfire/compare/v0.27.0...v0.28.0) (2024-04-30)

- Add `logfire.instrument_asyncpg()` by @alexmojaki in https://github.com/pydantic/logfire/pull/44

## [v0.27.0](https://github.com/pydantic/logfire/commits/v0.27.0) (2024-04-30)

First release from new repo!

- Update README by @Kludex in https://github.com/pydantic/logfire/pull/2
- Add linter & pipeline by @Kludex in https://github.com/pydantic/logfire/pull/1
- Use Python 3.8 for pipeline by @Kludex in https://github.com/pydantic/logfire/pull/3
- Use Rye instead of Poetry by @Kludex in https://github.com/pydantic/logfire/pull/5
- Add test suite by @Kludex in https://github.com/pydantic/logfire/pull/6
- Add custom PyPI by @Kludex in https://github.com/pydantic/logfire/pull/7
- Add release job by @Kludex in https://github.com/pydantic/logfire/pull/8
- Install rye on release job by @Kludex in https://github.com/pydantic/logfire/pull/9
- Add latest monorepo changes by @Kludex in https://github.com/pydantic/logfire/pull/10
- Add Python 3.12 to CI by @hramezani in https://github.com/pydantic/logfire/pull/11
- improve readme and related faff by @samuelcolvin in https://github.com/pydantic/logfire/pull/12
- CF pages docs build by @samuelcolvin in https://github.com/pydantic/logfire/pull/14
- Create alert docs by @Kludex in https://github.com/pydantic/logfire/pull/15
- improve the readme and contributing guide by @samuelcolvin in https://github.com/pydantic/logfire/pull/16
- Add classifiers by @hramezani in https://github.com/pydantic/logfire/pull/17
- tell rye to use uv by @samuelcolvin in https://github.com/pydantic/logfire/pull/19
- Adding docs for `instrument_openai` by @samuelcolvin in https://github.com/pydantic/logfire/pull/18
- Live view docs by @samuelcolvin in https://github.com/pydantic/logfire/pull/20
- Add `logfire info` and issue templates by @samuelcolvin in https://github.com/pydantic/logfire/pull/22
- Add GitHub discussions to help page, remove "login", show source link by @samuelcolvin in https://github.com/pydantic/logfire/pull/23
- setup coverage by @samuelcolvin in https://github.com/pydantic/logfire/pull/24
- improve coverage by @samuelcolvin in https://github.com/pydantic/logfire/pull/25
- Write token docs (#2244) by @Kludex in https://github.com/pydantic/logfire/pull/27
- add direct connect docs by @davidhewitt in https://github.com/pydantic/logfire/pull/26
- Add dashboard docs by @Kludex in https://github.com/pydantic/logfire/pull/28
- Add SQL Explore docs by @Kludex in https://github.com/pydantic/logfire/pull/29
- Make the `psycopg2` docs runnable as is by @Kludex in https://github.com/pydantic/logfire/pull/31
- Add Rye to installation by @Kludex in https://github.com/pydantic/logfire/pull/33
- Apply Logfire brand colors by @Kludex in https://github.com/pydantic/logfire/pull/32
- `logfire.instrument_psycopg()` function by @alexmojaki in https://github.com/pydantic/logfire/pull/30
- Handle recursive logging from OTEL by @alexmojaki in https://github.com/pydantic/logfire/pull/35
- Improve MongoDB docs by @Kludex in https://github.com/pydantic/logfire/pull/34
- Improve colors by @dmontagu in https://github.com/pydantic/logfire/pull/38
- Rename files to not have numeric prefixes by @dmontagu in https://github.com/pydantic/logfire/pull/39
- Handle cyclic references in JSON encoding and schema by @alexmojaki in https://github.com/pydantic/logfire/pull/37
- Ensure `logfire.testing` doesn't depend on pydantic and eval_type_backport by @alexmojaki in https://github.com/pydantic/logfire/pull/40
- Allow using pydantic plugin with models defined before calling logfire.configure by @alexmojaki in https://github.com/pydantic/logfire/pull/36
